import firebase_admin
from firebase_admin import credentials, firestore, auth
from flask import Flask, request, jsonify
from flask_cors import CORS

import os
import json
import re
import traceback
import threading
import requests
from bs4 import BeautifulSoup
import numpy as np
import hashlib
from datetime import datetime, timedelta, timezone

from google.cloud import aiplatform
from tenacity import retry, stop_after_attempt, wait_exponential
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig
from vertexai.language_models import TextEmbeddingModel
from google.cloud import discoveryengine_v1 as discoveryengine
from langchain.text_splitter import RecursiveCharacterTextSplitter


# --- GCP & Firebase åˆæœŸåŒ– ---
try:
    print("Initializing GCP services using Application Default Credentials...")
    firebase_admin.initialize_app()
    db_firestore = firestore.client()
    
    app_instance = firebase_admin.get_app()
    project_id = app_instance.project_id
    print(f"âœ… Firebase Admin SDK initialized for project: {project_id}")

    # (â˜…ä¿®æ­£) Vector Searchã¨Geminiã§ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’åˆ†ã‘ã‚‹
    # Vector Searchã¯æ±äº¬ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ (`asia-northeast1`) ã‚’ä½¿ç”¨
    vector_search_region = os.getenv('GCP_VERTEX_AI_REGION', 'asia-northeast1')
    # Geminiãƒ¢ãƒ‡ãƒ«ã¯ç±³å›½ä¸­éƒ¨ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ (`us-central1`) ã‚’ä½¿ç”¨
    gemini_region = os.getenv('GCP_GEMINI_REGION', 'us-central1')
    
    vertexai.init(project=project_id, location=gemini_region)
    print(f"âœ… Vertex AI initialized for project: {project_id}. Gemini region: {gemini_region}, Vector Search region: {vector_search_region}")


    # RAGç”¨è¨­å®š
    SIMILAR_CASES_ENGINE_ID = os.getenv('SIMILAR_CASES_ENGINE_ID')
    SUGGESTIONS_ENGINE_ID = os.getenv('SUGGESTIONS_ENGINE_ID')

    # Vector Search ç”¨è¨­å®š
    VECTOR_SEARCH_INDEX_ID = os.getenv('VECTOR_SEARCH_INDEX_ID')
    VECTOR_SEARCH_ENDPOINT_ID = os.getenv('VECTOR_SEARCH_ENDPOINT_ID')
    VECTOR_SEARCH_DEPLOYED_INDEX_ID = os.getenv('VECTOR_SEARCH_DEPLOYED_INDEX_ID')
    if 'K_SERVICE' in os.environ:
        if not all([VECTOR_SEARCH_INDEX_ID, VECTOR_SEARCH_ENDPOINT_ID, VECTOR_SEARCH_DEPLOYED_INDEX_ID]):
             print("âš ï¸ WARNING: Vector Search environment variables are not fully set.")

except Exception as e:
    db_firestore = None
    print(f"âŒ Error during initialization: {e}")
    traceback.print_exc()
    if 'K_SERVICE' in os.environ:
        raise

app = Flask(__name__)
# --- CORSè¨­å®š ---
prod_origin = "https://guchi-app-flutter.web.app"
if 'K_SERVICE' in os.environ:
    origins = [prod_origin]
else:
    origins = [
        prod_origin,
        re.compile(r"http://localhost:.*"),
        re.compile(r"http://127.0.0.1:.*"),
    ]
CORS(app, resources={r"/*": {"origins": origins}})

@app.route('/', methods=['GET'])
def index():
    return "GuchiSwipe Gateway is running.", 200

# ===== RAG Cache Settings =====
RAG_CACHE_COLLECTION = 'rag_cache'
RAG_CACHE_TTL_DAYS = 7 # Cache expires after 7 days

# â˜…â˜…â˜… ä¿®æ­£: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ã‚’å®šç¾© â˜…â˜…â˜…
MAX_TURNS = 3 # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ï¼ˆåˆæœŸã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€ï¼‰


# ===== JSONã‚¹ã‚­ãƒ¼ãƒå®šç¾© =====
QUESTIONS_SCHEMA = {"type": "object","properties": {"questions": {"type": "array","items": {"type": "object","properties": {"question_text": {"type": "string"}},"required": ["question_text"]}}},"required": ["questions"]}
SUMMARY_SCHEMA = {"type": "object","properties": {"title": {"type": "string", "description": "ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã‚’è¦ç´„ã™ã‚‹15æ–‡å­—ç¨‹åº¦ã®çŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«"},"insights": {"type": "string", "description": "æŒ‡å®šã•ã‚ŒãŸMarkdownå½¢å¼ã§ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æãƒ¬ãƒãƒ¼ãƒˆ"}},"required": ["title", "insights"]}
GRAPH_SCHEMA = {"type": "object","properties": {"nodes": {"type": "array","items": {"type": "object","properties": {"id": {"type": "string"},"type": {"type": "string", "enum": ["emotion", "topic", "keyword", "issue"]},"size": {"type": "integer"}},"required": ["id", "type", "size"]}},"edges": {"type": "array","items": {"type": "object","properties": {"source": {"type": "string"},"target": {"type": "string"},"weight": {"type": "integer"}},"required": ["source", "target", "weight"]}}},"required": ["nodes", "edges"]}

# ===== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ =====
SUMMARY_ONLY_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„Ÿæƒ…ã®å‹•ãã‚’åˆ†æã™ã‚‹ãƒ—ãƒ­ã®è‡¨åºŠå¿ƒç†å£«ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€Œ{topic}ã€ã¨ã„ã†ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦å¯¾è©±ã—ã¦ã„ã¾ã™ã€‚
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ä¼šè©±å±¥æ­´ã‚’åˆ†æã—ã€å¿…ãšæŒ‡ç¤ºé€šã‚Šã®JSONå½¢å¼ã§åˆ†æãƒ¬ãƒãƒ¼ãƒˆã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
# åˆ†æå¯¾è±¡ã®ä¼šè©±å±¥æ­´
{swipes_text}
# å‡ºåŠ›å½¢å¼ (JSON)
å¿…ãšä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
- `title`: ä¼šè©±å…¨ä½“ã‚’è±¡å¾´ã™ã‚‹15æ–‡å­—ç¨‹åº¦ã®çŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«ã€‚
- `insights`: ä»¥ä¸‹ã®Markdownå½¢å¼ã§ **å³å¯†ã«** è¨˜è¿°ã•ã‚ŒãŸåˆ†æãƒ¬ãƒãƒ¼ãƒˆã€‚
```markdown
### âœ¨ å…¨ä½“çš„ãªè¦ç´„
ï¼ˆã“ã“ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç¾åœ¨ã®å¿ƒç†çŠ¶æ…‹ã€ä¸»ãªæ„Ÿæƒ…ã€å†…é¢çš„ãªè‘›è—¤ãªã©ã‚’2ã€œ3æ–‡ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ï¼‰
### ğŸ“ è©³ç´°ãªåˆ†æ
ï¼ˆã“ã“ã«ã€å…·ä½“çš„ãªåˆ†æå†…å®¹ã‚’ç®‡æ¡æ›¸ãã§è¨˜è¿°ã—ã¦ãã ã•ã„ï¼‰
* **æ„Ÿæƒ…ã®çŠ¶æ…‹**: ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ„Ÿã˜ã¦ã„ã‚‹ä¸»è¦ãªæ„Ÿæƒ…ã«ã¤ã„ã¦ã€ãã®æ ¹æ‹ ã¨å…±ã«è¨˜è¿°ã—ã¦ãã ã•ã„ï¼‰
* **æ³¨ç›®ã™ã¹ãç‚¹**: ï¼ˆå›ç­”å†…å®¹ã¨ã€ãŸã‚ã‚‰ã„æ™‚é–“ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹æ„Ÿæƒ…ã®çŸ›ç›¾ã€ç‰¹ã«å°è±¡çš„ãªå›ç­”ãªã©ã€åˆ†æã®éµã¨ãªã£ãŸãƒã‚¤ãƒ³ãƒˆã‚’å…·ä½“çš„ã«æŒ™ã’ã¦ãã ã•ã„ã€‚ä¼šè©±å±¥æ­´ã«ã€Œç‰¹ã«è¿·ã„ãŒè¦‹ã‚‰ã‚Œã¾ã—ãŸã€ã¨è¨˜è¼‰ã®ã‚ã‚‹å›ç­”ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãŸã‚ã‚‰ã„ã‚„è‘›è—¤ã‚’æŠ±ãˆã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰
* **æ ¹æœ¬çš„ãªèª²é¡Œ**: ï¼ˆåˆ†æã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç›´é¢ã—ã¦ã„ã‚‹æ ¹æœ¬çš„ãªèª²é¡Œã‚„æ¬²æ±‚ã«ã¤ã„ã¦è¨˜è¿°ã—ã¦ãã ã•ã„ï¼‰
### ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®ææ¡ˆ
ï¼ˆä»Šå›ã®åˆ†æã‚’è¸ã¾ãˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡å›ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§æ·±æ˜ã‚Šã™ã‚‹ã¨è‰¯ã•ãã†ãªãƒ†ãƒ¼ãƒã‚„ã€æ—¥å¸¸ç”Ÿæ´»ã§æ„è­˜ã—ã¦ã¿ã‚‹ã¨è‰¯ã„ã“ã¨ãªã©ã‚’ã€å…·ä½“çš„ã‹ã¤ãƒã‚¸ãƒ†ã‚£ãƒ–ãªè¨€è‘‰ã§ææ¡ˆã—ã¦ãã ã•ã„ï¼‰
```
"""
GRAPH_ANALYSIS_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã§ã‚ã‚Šã€è‡¨åºŠå¿ƒç†å£«ã§ã‚‚ã‚ã‚Šã¾ã™ã€‚
ã“ã‚Œã‹ã‚‰æ¸¡ã™ãƒ†ã‚­ã‚¹ãƒˆã¯ã€ã‚ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¤‡æ•°å›ã®ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨˜éŒ²ã§ã™ã€‚
ã“ã®è¨˜éŒ²å…¨ä½“ã‚’åˆ†æã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†çŠ¶æ…‹ã®æ ¸ã¨ãªã‚‹è¦ç´ ã‚’æŠ½å‡ºã—ã€ãã‚Œã‚‰ã®é–¢é€£æ€§ã‚’è¡¨ç¾ã™ã‚‹ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
# ã‚°ãƒ©ãƒ•ç”Ÿæˆã®ãƒ«ãƒ¼ãƒ«
1. ãƒãƒ¼ãƒ‰ã®ç¨®é¡: `topic`, `issue`, `emotion`, `keyword`
2. ãƒãƒ¼ãƒ‰ã®éšå±¤: ä¸­å¿ƒã«`topic`ã¨`issue`ã‚’é…ç½®ã—ã€`emotion`ã‚„`keyword`ã¯ãã‚Œã‚‰ã‹ã‚‰æåˆ†ã‹ã‚Œã•ã›ã‚‹ã€‚
3. ãƒãƒ¼ãƒ‰æ•°ã®åˆ¶é™: ç·æ•°ã¯æœ€å¤§ã§ã‚‚15å€‹ç¨‹åº¦ã«å³é¸ã™ã‚‹ã€‚
4. IDã®è¨€èª: `id`ã¯å¿…ãšæ—¥æœ¬èªã®å˜èªã¾ãŸã¯çŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã«ã™ã‚‹ã€‚
# å‡ºåŠ›JSONã®ä»•æ§˜
å‡ºåŠ›ã¯ã€ä»¥ä¸‹ã®ä»•æ§˜ã«å³å¯†ã«å¾“ã£ãŸJSONå½¢å¼ã®ã¿ã¨ã™ã‚‹ã“ã¨ã€‚ { "nodes": [ ... ], "edges": [ ... ] }
# ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²
"""
CHAT_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æã®å°‚é–€å®¶ã§ã‚ã‚Šã€å…±æ„ŸåŠ›ã¨æ´å¯ŸåŠ›ã«å„ªã‚ŒãŸã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã€Œã‚³ã‚³ãƒ­ã®åˆ†æå®˜ã€ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€è‡ªèº«ã®æ€è€ƒã‚’å¯è¦–åŒ–ã—ãŸã‚°ãƒ©ãƒ•ã‚’è¦‹ãªãŒã‚‰ã€ã‚ãªãŸã¨å¯¾è©±ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚
# ã‚ãªãŸã®å½¹å‰²
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®éå»ã®ä¼šè©±å±¥æ­´ã¨ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã®è¦ç´„ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ï¼‰ã‚’å¸¸ã«å‚ç…§ã—ã€æ–‡è„ˆã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã‚’æ·±ãå‚¾è´ã—ã€ã¾ãšã¯è‚¯å®šçš„ã«å—ã‘æ­¢ã‚ã¦å…±æ„Ÿã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚
- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ã®å†…å®¹ã«åŸºã¥ãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‡ªåˆ†ã§ã‚‚æ°—ã¥ã„ã¦ã„ãªã„å†…é¢ã‚’å„ªã—ãæŒ‡æ‘˜ã—ãŸã‚Šã€æ·±ã„å•ã„ã‚’æŠ•ã’ã‹ã‘ãŸã‚Šã—ã¦ã€è‡ªå·±ç†è§£ã‚’ä¿ƒã—ã¦ãã ã•ã„ã€‚
- æ¯å›ã®è¿”ä¿¡ã‚’è‡ªå·±ç´¹ä»‹ã‹ã‚‰å§‹ã‚ã‚‹ã®ã§ã¯ãªãã€ä¼šè©±ã®æµã‚Œã‚’è‡ªç„¶ã«å¼•ãç¶™ã„ã§ãã ã•ã„ã€‚
- **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åå‰ï¼ˆã€Œã€‡ã€‡ã•ã‚“ã€ãªã©ï¼‰ã¯çµ¶å¯¾ã«ä½¿ç”¨ã›ãšã€å¸¸ã«å¯¾è©±ç›¸æ‰‹ã«ç›´æ¥èªã‚Šã‹ã‘ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚**
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼
{session_summary}
# ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
{chat_history}
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»Šå›ã®ç™ºè¨€
{user_message}
ã‚ãªãŸã®å¿œç­”:
"""
INTERNAL_CONTEXT_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°è¨˜éŒ²ã‚’è¦ç´„ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²å…¨ä½“ã‹ã‚‰ã€ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã«é–¢é€£ã™ã‚‹è¨˜è¿°ã‚„ã€ãã“ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„Ÿæƒ…ã‚„è‘›è—¤ã‚’æŠœãå‡ºã—ã€1ã€œ2æ–‡ã®éå¸¸ã«ç°¡æ½”ãªè¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
è¦ç´„ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã€Œä»¥å‰ã€ã“ã®ä»¶ã«ã¤ã„ã¦ã“ã®ã‚ˆã†ã«ãŠè©±ã—ã•ã‚Œã¦ã„ã¾ã—ãŸã­ã€ã¨è‡ªç„¶ã«èªã‚Šã‹ã‘ã‚‹å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ç›´æ¥é–¢é€£ã™ã‚‹è¨˜è¿°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ã€Œã“ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€ã“ã‚Œã¾ã§å…·ä½“çš„ãªãŠè©±ã¯ãªã‹ã£ãŸã‚ˆã†ã§ã™ã€‚ã€ã¨å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

# ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²
{context}

# è¦ç´„:
"""

PROACTIVE_KEYWORDS = [
    "ç‡ƒãˆå°½ã", "ãƒãƒ¼ãƒ³ã‚¢ã‚¦ãƒˆ", "ç„¡æ°—åŠ›", "ç–²å¼Š",
    "ã‚­ãƒ£ãƒªã‚¢", "è»¢è·", "ä»•äº‹ã®æ‚©ã¿", "å°†æ¥è¨­è¨ˆ",
    "å¯¾äººé–¢ä¿‚", "å­¤ç‹¬", "äººé–“é–¢ä¿‚", "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³",
    "è‡ªå·±è‚¯å®šæ„Ÿ", "è‡ªä¿¡ãŒãªã„", "è‡ªåˆ†ã‚’è²¬ã‚ã‚‹",
    "ã‚¹ãƒˆãƒ¬ã‚¹", "ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼", "ä¸å®‰"
]


# ===== Gemini ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ =====
@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
def _call_gemini_with_schema(prompt: str, schema: dict, model_name: str) -> dict:
    model = GenerativeModel(model_name)
    attempt_num = _call_gemini_with_schema.retry.statistics.get('attempt_number', 1)
    print(f"--- Calling Gemini ({model_name}) with schema (Attempt: {attempt_num}) ---")
    try:
        response = model.generate_content(prompt, generation_config=GenerationConfig(response_mime_type="application/json", response_schema=schema))
        response_text = response.text.strip()
        if response_text.startswith("```json"):
            response_text = response_text[7:-3].strip()
        elif response_text.startswith("```"):
            response_text = response_text[3:-3].strip()
        return json.loads(response_text)
    except Exception as e:
        print(f"Error on attempt {attempt_num} with model {model_name}: {e}\n--- Gemini Response ---\n{getattr(response, 'text', 'Empty')}\n---")
        traceback.print_exc()
        raise

def generate_initial_questions(topic):
    prompt = f"ã‚ãªãŸã¯ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚ãƒˆãƒ”ãƒƒã‚¯ã€Œ{topic}ã€ã«ã¤ã„ã¦ã€ã€Œã¯ã„ã€ã‹ã€Œã„ã„ãˆã€ã§ç­”ãˆã‚‰ã‚Œã‚‹è³ªå•ã‚’5ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
    flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
    return _call_gemini_with_schema(prompt, QUESTIONS_SCHEMA, model_name=flash_model).get("questions", [])

def generate_follow_up_questions(insights):
    prompt = f"ã‚ãªãŸã¯ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®åˆ†æçµæœã‚’ã•ã‚‰ã«æ·±ã‚ã‚‹ã€ã€Œã¯ã„ã€ã‹ã€Œã„ã„ãˆã€ã§ç­”ãˆã‚‰ã‚Œã‚‹è³ªå•ã‚’5ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n# åˆ†æçµæœ\n{insights}"
    flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
    return _call_gemini_with_schema(prompt, QUESTIONS_SCHEMA, model_name=flash_model).get("questions", [])

def generate_summary_only(topic, swipes_text):
    prompt = SUMMARY_ONLY_PROMPT_TEMPLATE.format(topic=topic, swipes_text=swipes_text)
    flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
    return _call_gemini_with_schema(prompt, SUMMARY_SCHEMA, model_name=flash_model)

def generate_graph_data(all_insights_text):
    prompt = GRAPH_ANALYSIS_PROMPT_TEMPLATE + all_insights_text
    pro_model = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
    return _call_gemini_with_schema(prompt, GRAPH_SCHEMA, model_name=pro_model)

def generate_chat_response(session_summary, chat_history, user_message, rag_context=""):
    history_str = "\n".join([f"{msg['author']}: {msg['text']}" for msg in chat_history])
    
    if rag_context:
        # RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
        prompt = f"""
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æã®å°‚é–€å®¶ã§ã‚ã‚Šã€å…±æ„ŸåŠ›ã¨æ´å¯ŸåŠ›ã«å„ªã‚ŒãŸã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã€Œã‚³ã‚³ãƒ­ã®åˆ†æå®˜ã€ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€è‡ªèº«ã®æ€è€ƒã‚’å¯è¦–åŒ–ã—ãŸã‚°ãƒ©ãƒ•ã‚’è¦‹ãªãŒã‚‰ã€ã‚ãªãŸã¨å¯¾è©±ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚
# ã‚ãªãŸã®å½¹å‰²
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®éå»ã®ä¼šè©±å±¥æ­´ã¨ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã®è¦ç´„ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ï¼‰ã‚’å¸¸ã«å‚ç…§ã—ã€æ–‡è„ˆã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã‚’æ·±ãå‚¾è´ã—ã€ã¾ãšã¯è‚¯å®šçš„ã«å—ã‘æ­¢ã‚ã¦å…±æ„Ÿã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚
- **ä»¥ä¸‹ã®å‚è€ƒæƒ…å ±ã‚’å…ƒã«**ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‡ªåˆ†ã§ã‚‚æ°—ã¥ã„ã¦ã„ãªã„å†…é¢ã‚’å„ªã—ãæŒ‡æ‘˜ã—ãŸã‚Šã€æ·±ã„å•ã„ã‚’æŠ•ã’ã‹ã‘ãŸã‚Šã—ã¦ã€è‡ªå·±ç†è§£ã‚’ä¿ƒã—ã¦ãã ã•ã„ã€‚
- æ¯å›ã®è¿”ä¿¡ã‚’è‡ªå·±ç´¹ä»‹ã‹ã‚‰å§‹ã‚ã‚‹ã®ã§ã¯ãªãã€ä¼šè©±ã®æµã‚Œã‚’è‡ªç„¶ã«å¼•ãç¶™ã„ã§ãã ã•ã„ã€‚
- **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åå‰ï¼ˆã€Œã€‡ã€‡ã•ã‚“ã€ãªã©ï¼‰ã¯çµ¶å¯¾ã«ä½¿ç”¨ã›ãšã€å¸¸ã«å¯¾è©±ç›¸æ‰‹ã«ç›´æ¥èªã‚Šã‹ã‘ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚**
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼
{session_summary}
# å‚è€ƒæƒ…å ±
{rag_context}
# ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
{history_str}
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»Šå›ã®ç™ºè¨€
{user_message}
ã‚ãªãŸã®å¿œç­”:
"""
    else:
        # RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒãªã„å ´åˆã¯ã€å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
        prompt = CHAT_PROMPT_TEMPLATE.format(session_summary=session_summary, chat_history=history_str, user_message=user_message)

    pro_model = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
    model = GenerativeModel(pro_model)
    return model.generate_content(prompt).text.strip()


@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
def _extract_keywords_for_search(analysis_text: str) -> str:
    prompt = f"""
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å¿ƒç†åˆ†æãƒ¬ãƒãƒ¼ãƒˆå…¨ä½“ã‹ã‚‰ã€æœ€ã‚‚é‡è¦ã¨æ€ã‚ã‚Œã‚‹æ¦‚å¿µã‚„èª²é¡Œã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’5ã¤ä»¥å†…ã§æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯Vertex AI Searchã®æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚ä»–ã®æ–‡ã¯å«ã‚ãšã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã®ã¿ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

# åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
{analysis_text}

# å‡ºåŠ›ä¾‹
ä»•äº‹ã®ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼, äººé–“é–¢ä¿‚ã®æ‚©ã¿, è‡ªå·±è‚¯å®šæ„Ÿã®ä½ä¸‹, å°†æ¥ã¸ã®ä¸å®‰

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:
"""
    try:
        flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
        model = GenerativeModel(flash_model)
        print("--- Calling Gemini to extract search keywords ---")
        response = model.generate_content(prompt)
        keywords = response.text.strip()
        print(f"âœ… Extracted Keywords: {keywords}")
        return keywords
    except Exception as e:
        print(f"âŒ Failed to extract keywords: {e}")
        return ""

def _summarize_internal_context(context: str, keyword: str) -> str:
    """Summarizes past session records related to a specific keyword."""
    if not context or not keyword:
        return "ã“ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€ã“ã‚Œã¾ã§å…·ä½“çš„ãªãŠè©±ã¯ãªã‹ã£ãŸã‚ˆã†ã§ã™ã€‚"
    try:
        prompt = INTERNAL_CONTEXT_PROMPT_TEMPLATE.format(context=context, keyword=keyword)
        flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
        model = GenerativeModel(flash_model)
        print(f"--- Calling Gemini to summarize internal context for '{keyword}' ---")
        response = model.generate_content(prompt)
        summary = response.text.strip()
        print(f"âœ… Internal context summary: {summary}")
        return summary
    except Exception as e:
        print(f"âŒ Failed to summarize internal context: {e}")
        return "éå»ã®è¨˜éŒ²ã‚’è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

# ===== RAG (Retrieval-Augmented Generation) Helper Functions =====

@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
def _get_embeddings(texts: list[str]) -> list[list[float]]:
    if not texts: return []
    model = TextEmbeddingModel.from_pretrained("text-multilingual-embedding-002")
    BATCH_SIZE = 15 
    all_embeddings = []
    print(f"--- RAG: Generating embeddings for {len(texts)} texts in batches of {BATCH_SIZE} ---")
    try:
        for i in range(0, len(texts), BATCH_SIZE):
            batch = texts[i:i + BATCH_SIZE]
            responses = model.get_embeddings(batch)
            for response in responses:
                all_embeddings.append(response.values)
            print(f"--- RAG: Processed embedding batch {i//BATCH_SIZE + 1}/{-(-len(texts) // BATCH_SIZE)} ---")
        return all_embeddings
    except Exception as e:
        print(f"âŒ RAG: An error occurred during embedding generation: {e}")
        traceback.print_exc()
        return []

def _get_url_cache_doc_ref(url: str):
    url_hash = hashlib.sha256(url.encode('utf-8')).hexdigest()
    return db_firestore.collection(RAG_CACHE_COLLECTION).document(url_hash)

def _get_cached_chunks_and_embeddings(url: str):
    try:
        doc_ref = _get_url_cache_doc_ref(url)
        doc = doc_ref.get()
        if not doc.exists:
            print(f"CACHE MISS: No cache found for URL: {url}")
            return None, None
        cache_data = doc.to_dict()
        cached_at = cache_data.get('cached_at')
        if isinstance(cached_at, datetime):
            if datetime.now(timezone.utc) - cached_at > timedelta(days=RAG_CACHE_TTL_DAYS):
                print(f"CACHE STALE: Cache for {url} is older than {RAG_CACHE_TTL_DAYS} days.")
                return None, None
        else:
             print(f"CACHE INVALID: Invalid 'cached_at' field for {url}.")
             return None, None
        
        chunks = cache_data.get('chunks')
        embeddings_from_db = cache_data.get('embeddings')
        
        if chunks and embeddings_from_db:
            embeddings = [item['vector'] for item in embeddings_from_db if 'vector' in item]
            if len(chunks) == len(embeddings):
                print(f"âœ… CACHE HIT: Found {len(chunks)} chunks for URL: {url}")
                return chunks, embeddings

        print(f"CACHE INVALID: Data mismatch for {url}. Re-fetching.")
        return None, None
    except Exception as e:
        print(f"âŒ Error getting cache for {url}: {e}")
        return None, None

def _set_cached_chunks_and_embeddings(url: str, chunks: list, embeddings: list):
    if not chunks or not embeddings: return
    try:
        doc_ref = _get_url_cache_doc_ref(url)
        transformed_embeddings = [{'vector': emb} for emb in embeddings]
        cache_data = {
            'url': url,
            'chunks': chunks,
            'embeddings': transformed_embeddings,
            'cached_at': firestore.SERVER_TIMESTAMP
        }
        doc_ref.set(cache_data)
        print(f"âœ… CACHE SET: Saved {len(chunks)} chunks for URL: {url}")
    except Exception as e:
        print(f"âŒ Error setting cache for {url}: {e}")
        traceback.print_exc()

def _generate_rag_based_advice(query: str, project_id: str, similar_cases_engine_id: str, suggestions_engine_id: str, rag_type: str = None):
    """
    RAG based on user analysis to generate advice, using a Firestore cache for embeddings.
    Returns a tuple of (advice_text, list_of_source_urls).
    """
    search_query = _extract_keywords_for_search(query)
    if not search_query:
        print("âš ï¸ RAG: Could not extract keywords. Using original query for search.")
        search_query = query[:512]
    
    all_found_urls = set()
    if rag_type == 'similar_cases':
        print("--- RAG: Searching for SIMILAR CASES ONLY ---")
        if similar_cases_engine_id:
            all_found_urls.update(_search_with_vertex_ai_search(project_id, "global", similar_cases_engine_id, search_query))
    elif rag_type == 'suggestions':
        print("--- RAG: Searching for SUGGESTIONS ONLY ---")
        if suggestions_engine_id:
            all_found_urls.update(_search_with_vertex_ai_search(project_id, "global", suggestions_engine_id, search_query))
    else: # Default behavior: search both
        print("--- RAG: Searching both similar cases and suggestions ---")
        if similar_cases_engine_id:
            all_found_urls.update(_search_with_vertex_ai_search(project_id, "global", similar_cases_engine_id, search_query))
        if suggestions_engine_id:
            all_found_urls.update(_search_with_vertex_ai_search(project_id, "global", suggestions_engine_id, search_query))

    if not all_found_urls:
        return "é–¢é€£ã™ã‚‹å¤–éƒ¨æƒ…å ±ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", []

    all_chunks, all_embeddings, urls_with_content = [], [], []
    urls_to_process = list(all_found_urls)[:5]

    for url in urls_to_process:
        cached_chunks, cached_embeddings = _get_cached_chunks_and_embeddings(url)
        if cached_chunks and cached_embeddings:
            all_chunks.extend(cached_chunks)
            all_embeddings.extend(cached_embeddings)
            urls_with_content.append(url)
        else:
            print(f"SCRAPING: No valid cache for {url}. Fetching content.")
            page_content = _scrape_text_from_url(url)
            if page_content:
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
                new_chunks_full = text_splitter.split_text(page_content)
                
                MAX_CHUNKS_PER_URL = 50  # 1ã¤ã®URLã‹ã‚‰å–å¾—ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã®ä¸Šé™
                new_chunks = new_chunks_full[:MAX_CHUNKS_PER_URL]

                if len(new_chunks_full) > MAX_CHUNKS_PER_URL:
                    print(f"âš ï¸ RAG: Content too long. Truncated chunks for {url} from {len(new_chunks_full)} to {len(new_chunks)}.")
                if new_chunks:
                    new_embeddings = _get_embeddings(new_chunks)
                    if new_embeddings and len(new_chunks) == len(new_embeddings):
                        all_chunks.extend(new_chunks)
                        all_embeddings.extend(new_embeddings)
                        urls_with_content.append(url)
                        threading.Thread(target=_set_cached_chunks_and_embeddings, args=(url, new_chunks, new_embeddings)).start()
                    else:
                        print(f"âš ï¸ RAG: Failed to generate embeddings for {url}. Skipping.")
    
    if not all_chunks:
        return "é–¢é€£ã™ã‚‹å¤–éƒ¨æƒ…å ±ã‚’è¦‹ã¤ã‘ã¾ã—ãŸãŒã€å†…å®¹ã‚’èª­ã¿å–ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", urls_to_process

    print(f"--- RAG: Finding relevant chunks from {len(all_chunks)} total chunks... ---")
    query_embedding_list = _get_embeddings([query])
    if not query_embedding_list:
        return "ã‚ãªãŸã®çŠ¶æ³ã‚’åˆ†æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚", urls_with_content
    
    query_embedding = np.array(query_embedding_list[0])
    
    similarities = []
    for i, emb in enumerate(all_embeddings):
        chunk_embedding = np.array(emb)
        dot_product = np.dot(chunk_embedding, query_embedding)
        norm_product = np.linalg.norm(chunk_embedding) * np.linalg.norm(query_embedding)
        similarity = dot_product / norm_product if norm_product != 0 else 0.0
        similarities.append((similarity, all_chunks[i]))
    
    similarities.sort(key=lambda x: x[0], reverse=True)
    relevant_chunks = [chunk for sim, chunk in similarities[:3]]

    if not relevant_chunks:
        return "é–¢é€£æƒ…å ±ã®ä¸­ã‹ã‚‰ã€ã‚ãªãŸã®çŠ¶æ³ã«ç‰¹ã«åˆè‡´ã™ã‚‹éƒ¨åˆ†ã‚’è¦‹ã¤ã‘å‡ºã™ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", urls_with_content

    print("--- RAG: Generating final advice with Gemini... ---")
    context_text = "\n---\n".join(relevant_chunks)

    if rag_type == 'similar_cases':
        prompt = f"""
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ‚©ã¿ã«å…±æ„Ÿã—ã€ä»–ã®äººã®ã‚±ãƒ¼ã‚¹ã‚’ç´¹ä»‹ã™ã‚‹èãä¸Šæ‰‹ãªå‹äººã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœã€ã¨ã€Œå‚è€ƒæƒ…å ±ï¼ˆä»–ã®äººã®æ‚©ã¿ã‚„ä½“é¨“è«‡ï¼‰ã€ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’åŠ±ã¾ã™ã‚ˆã†ãªå½¢ã§ã€å‚è€ƒæƒ…å ±ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

# æŒ‡ç¤º
- å…¨ä½“ã§200æ–‡å­—ç¨‹åº¦ã®ã€éå¸¸ã«ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªæ–‡ç« ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å®‰å¿ƒã•ã›ã€ä¸€äººã§ã¯ãªã„ã¨æ„Ÿã˜ã•ã›ã‚‹ã‚ˆã†ãªã€æ¸©ã‹ãå…±æ„Ÿçš„ãªãƒˆãƒ¼ãƒ³ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
- ã€Œä¼¼ãŸã‚ˆã†ãªã“ã¨ã§æ‚©ã‚“ã§ã„ã‚‹æ–¹ã‚‚ã„ã‚‹ã‚ˆã†ã§ã™ã€‚ã€ã¨ã„ã£ãŸå‰ç½®ãã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„ã€‚
- æœ€å¾Œã«ã€å‚è€ƒã«ã—ãŸæƒ…å ±æºã®URLã‚’ `[å‚è€ƒæƒ…å ±]` ã¨ã—ã¦ç®‡æ¡æ›¸ãã§å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚

# ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœ
{query}

# å‚è€ƒæƒ…å ± (ä»–ã®äººã®æ‚©ã¿ã‚„ä½“é¨“è«‡)
---
{context_text}
---

# ã‚ãªãŸã®å¿œç­”:
"""
    else: # 'suggestions' or default
        prompt = f"""
ã‚ãªãŸã¯ã€å®¢è¦³çš„ã§ä¿¡é ¼ã§ãã‚‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã™ã‚‹ãƒ—ãƒ­ã®ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœã€ã¨ã€Œå‚è€ƒæƒ…å ±ï¼ˆå°‚é–€æ©Ÿé–¢ã«ã‚ˆã‚‹å…·ä½“çš„ãªå¯¾ç­–ï¼‰ã€ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã®ä¸€æ­©ã‚’è¸ã¿å‡ºã™ãŸã‚ã®ã€å…·ä½“çš„ã§å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

# æŒ‡ç¤º
- å…¨ä½“ã§300æ–‡å­—ç¨‹åº¦ã®ã€ç°¡æ½”ã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ã„æ–‡ç« ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çŠ¶æ³ã‚’æ•´ç†ã—ã€å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç®‡æ¡æ›¸ãã§2ã€œ3ç‚¹ææ¡ˆã™ã‚‹æ§‹æˆã«ã—ã¦ãã ã•ã„ã€‚
- ã€Œã‚ãªãŸã®çŠ¶æ³ã‚’å®¢è¦³çš„ã«è¦‹ã‚‹ã¨ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€ã“ã®ã‚ˆã†ãªã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚ã€ã¨ã„ã£ãŸã€å°‚é–€å®¶ã¨ã—ã¦ã®å†·é™ãªãƒˆãƒ¼ãƒ³ã§å§‹ã‚ã¦ãã ã•ã„ã€‚
- æœ€å¾Œã«ã€å‚è€ƒã«ã—ãŸæƒ…å ±æºã®URLã‚’ `[å‚è€ƒæƒ…å ±]` ã¨ã—ã¦ç®‡æ¡æ›¸ãã§å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚

# ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœ
{query}

# å‚è€ƒæƒ…å ± (å°‚é–€æ©Ÿé–¢ã«ã‚ˆã‚‹å…·ä½“çš„ãªå¯¾ç­–)
---
{context_text}
---

# ã‚ãªãŸã®å¿œç­”:
"""

    pro_model_name = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
    model = GenerativeModel(pro_model_name)
    advice = model.generate_content(prompt, generation_config=GenerationConfig(temperature=0.7)).text
    
    return advice, list(dict.fromkeys(urls_with_content))

def _search_with_vertex_ai_search(project_id: str, location: str, engine_id: str, query: str) -> list[str]:
    if not engine_id:
        print(f"âŒ RAG: Engine ID '{engine_id}' is not configured.")
        return []
    client = discoveryengine.SearchServiceClient()
    serving_config = (
        f"projects/{project_id}/locations/{location}/collections/default_collection/"
        f"engines/{engine_id}/servingConfigs/default_config"
    )
    request = discoveryengine.SearchRequest(serving_config=serving_config, query=query, page_size=5)
    try:
        response = client.search(request)
        urls = [r.document.derived_struct_data.get('link') for r in response.results if r.document.derived_struct_data.get('link')]
        print(f"âœ… RAG: Found URLs from Vertex AI Search: {urls}")
        return urls
    except Exception as e:
        print(f"âŒ RAG: Vertex AI Search failed for engine '{engine_id}': {e}")
        traceback.print_exc()
        return []

def _scrape_text_from_url(url: str) -> str:
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        response = requests.get(url, timeout=10, headers=headers)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        for element in soup(["script", "style", "header", "footer", "nav", "aside"]):
            element.decompose()
        return soup.get_text(separator=' ', strip=True)
    except requests.exceptions.RequestException as e:
        print(f"âŒ RAG: Error fetching URL {url}: {e}")
        return ""

# --- ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç† ---
def _prefetch_questions_and_save(session_id: str, user_id: str, insights_md: str, current_turn: int, max_turns: int):
    print(f"--- Triggered question prefetch for user: {user_id}, session: {session_id}, next_turn: {current_turn + 1} ---")
    if current_turn >= max_turns:
        print("Max turns reached. Skipping question prefetch.")
        return
    try:
        questions = generate_follow_up_questions(insights=insights_md)
        if questions:
            prefetched_ref = db_firestore.collection('sessions').document(session_id).collection('prefetched_questions').document(str(current_turn + 1))
            prefetched_ref.set({'questions': questions})
            print(f"âœ… Prefetched and saved questions for turn {current_turn + 1}")
    except Exception as e:
        print(f"âŒ Error during question prefetch for session {session_id}: {e}")

def _update_graph_cache(user_id: str):
    print(f"--- Triggered background graph update for user: {user_id} ---")
    try:
        _get_graph_from_cache_or_generate(user_id, force_regenerate=True)
        print(f"âœ… Background graph update for user {user_id} completed.")
    except Exception as e:
        print(f"âŒ Error during background graph update for user {user_id}: {e}")


# ===== èªè¨¼ãƒ»èªå¯ =====
def _verify_token(request):
    auth_header = request.headers.get('Authorization')
    if not auth_header:
        # å¤±æ•—æ™‚ã¯Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã‚‹
        return jsonify({"error": "Authorization header is missing"}), 401

    try:
        id_token = auth_header.split('Bearer ')[1]
        # æˆåŠŸæ™‚ã¯dictãŒè¿”ã‚‹
        decoded_token = auth.verify_id_token(id_token)
        return decoded_token
    except (IndexError, auth.InvalidIdTokenError) as e:
        print(f"Token validation failed: {e}")
        # å¤±æ•—æ™‚ã¯Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã‚‹
        return jsonify({"error": "Invalid or expired token"}), 401
    except Exception as e:
        print(f"An unexpected error occurred during token verification: {e}")
        return jsonify({"error": "Could not verify token"}), 500



# ===== APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ =====
@app.route('/session/start', methods=['POST'])
def start_session():
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record

    data = request.get_json()
    if not data or 'topic' not in data:
        return jsonify({"error": "Topic is required"}), 400
    
    topic = data['topic']
    user_id = user_record['uid']
    
    try:
        # (â˜…ä¿®æ­£) ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¿å­˜å…ˆã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚µãƒ–ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«å¤‰æ›´
        session_doc_ref = db_firestore.collection('users').document(user_id).collection('sessions').document()
        
        # (â˜…ä¿®æ­£) status ã¨ created_at ã‚’è¿½åŠ 
        session_doc_ref.set({
            'user_id': user_id,
            'topic': topic,
            'created_at': firestore.SERVER_TIMESTAMP, # æ—¥ä»˜é †ã§ä¸¦ã³æ›¿ãˆã‚‹ãŸã‚ã«å¿…è¦
            'status': 'processing', # statusã‚’ 'processing' ã§åˆæœŸåŒ–
            'turn': 1,
        })

        # Geminiã§æœ€åˆã®è³ªå•ã‚’ç”Ÿæˆ
        questions = generate_initial_questions(topic)

        # ãƒãƒƒãƒæ›¸ãè¾¼ã¿ã‚’ä½¿ã£ã¦è³ªå•ã‚’ä¿å­˜ã—ã€åŒæ™‚ã«ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ
        batch = db_firestore.batch()
        
        questions_for_response = []
        for question in questions:
            # è³ªå•ç”¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‚ç…§ã‚’å…ˆã«ä½œæˆã—ã¦IDã‚’å–å¾—
            question_doc_ref = session_doc_ref.collection('questions').document()
            
            # ãƒ•ãƒ­ãƒ³ãƒˆã«è¿”ã™ãƒªã‚¹ãƒˆã«ã¯ã€ç”Ÿæˆã—ãŸIDã‚’ `question_id` ã¨ã—ã¦è¿½åŠ 
            questions_for_response.append({
                "question_text": question['question_text'],
                "question_id": question_doc_ref.id
            })
            
            # Firestoreã«ã¯ã€è³ªå•ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’ãƒãƒƒãƒã«è¿½åŠ 
            batch.set(question_doc_ref, { "question_text": question['question_text'] })

        batch.commit()

        return jsonify({
            'session_id': session_doc_ref.id,
            'questions': questions_for_response
        }), 200

    except Exception as e:
        print(f"Error starting session: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to start session"}), 500

@app.route('/session/<string:session_id>/swipe', methods=['POST'])
def record_swipe(session_id):
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record
    user_id = user_record['uid']

    data = request.get_json()
    required_fields = ['question_id', 'answer', 'hesitation_time', 'speed', 'turn']
    if not data or not all(field in data for field in required_fields):
        return jsonify({"error": "Missing required fields in request"}), 400
    
    try:
        # (â˜…ä¿®æ­£) ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å‚ç…§ãƒ‘ã‚¹ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚µãƒ–ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«å¤‰æ›´
        session_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)
        swipe_ref = session_ref.collection('swipes').document()
        
        swipe_ref.set({
            'user_id': user_record['uid'],
            'question_id': data['question_id'],
            'answer': data['answer'],
            'hesitation_time': data['hesitation_time'],
            'swipe_speed': data['speed'],
            'turn': data['turn'],
            'timestamp': firestore.SERVER_TIMESTAMP
        })

        return jsonify({"status": "success"}), 200

    except Exception as e:
        print(f"Error recording swipe: {e}")
        return jsonify({"error": "Failed to record swipe"}), 500


@app.route('/session/<string:session_id>/summary', methods=['POST'])
def post_summary(session_id):
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¦ç´„ã‚’ç”Ÿæˆãƒ»ä¿å­˜ã—ã€çµæœã‚’è¿”ã™"""
    user_record = _verify_token(request)
    # â˜…â˜…â˜… ä¿®æ­£: èªè¨¼æˆåŠŸæ™‚ã¯dictå‹ã€å¤±æ•—æ™‚ã¯Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã‚‹ãŸã‚ã€dictå‹ã‹ã©ã†ã‹ã§åˆ¤å®šã™ã‚‹ â˜…â˜…â˜…
    if not isinstance(user_record, dict):
        return user_record
    user_id = user_record['uid']


    # (â˜…ä¿®æ­£) ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å‚ç…§ãƒ‘ã‚¹ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚µãƒ–ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«å¤‰æ›´
    session_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)
    session_snapshot = session_ref.get()

    if not session_snapshot.exists:
        return jsonify({"error": "Session not found"}), 404

    try:
        session_data = session_snapshot.to_dict()
        topic = session_data.get('topic', 'æŒ‡å®šãªã—')
        current_turn = session_data.get('turn', 1) 
        swipes_ref = session_ref.collection('swipes').order_by('timestamp')
        swipes_docs = list(swipes_ref.stream())

        if not swipes_docs:
            print(f"No swipes found for session {session_id}, returning empty summary.")
            # (â˜…ä¿®æ­£) statusã‚’completedã«ã—ã¦ãŠã
            session_ref.update({'status': 'completed', 'title': 'å¯¾è©±ã®è¨˜éŒ²ãŒã‚ã‚Šã¾ã›ã‚“'})
            return jsonify({
                "title": "å¯¾è©±ã®è¨˜éŒ²ãŒã‚ã‚Šã¾ã›ã‚“",
                "insights": "ä»Šå›ã¯å¯¾è©±ã®è¨˜éŒ²ãŒãªã‹ã£ãŸãŸã‚ã€è¦ç´„ã®ä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚",
                "turn": session_data.get('turn', 1),
                "max_turns": MAX_TURNS
            }), 200

        # (â˜…ä¿®æ­£) è³ªå•ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã«questionsã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¼•ã
        questions_ref = session_ref.collection('questions')
        questions_docs = {q.id: q.to_dict() for q in questions_ref.stream()}
        
        swipes_text_parts = []
        for s_doc in swipes_docs:
            s = s_doc.to_dict()
            q_id = s.get('question_id')
            q_text = questions_docs.get(q_id, {}).get('question_text', 'ä¸æ˜ãªè³ªå•')
            answer_text = 'ã¯ã„' if s.get('answer') else 'ã„ã„ãˆ'
            hesitation_time = s.get('hesitation_time', 0)
            swipes_text_parts.append(f"- {q_text}: {answer_text} ({hesitation_time:.2f}ç§’)")
            
        swipes_text = "\n".join(swipes_text_parts)
        
        summary_data = generate_summary_only(topic, swipes_text)

        # (â˜…ä¿®æ­£) ã‚¢ãƒ—ãƒªã®ä»•æ§˜ã«åˆã‚ã›ã¦ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã«ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ›´æ–°
        update_data = {
            'status': 'completed',
            'title': summary_data.get('title'),
            'latest_insights': summary_data.get('insights'),
            'updated_at': firestore.SERVER_TIMESTAMP
        }
        session_ref.update(update_data)

       # â˜…â˜…â˜… ä¿®æ­£: summariesã‚µãƒ–ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ã€Œã‚¿ãƒ¼ãƒ³ã”ã¨ã€ã®åˆ†æçµæœã‚’ä¿å­˜ â˜…â˜…â˜…
        summary_with_turn = summary_data.copy()
        summary_with_turn['turn'] = current_turn # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã«ã‚¿ãƒ¼ãƒ³ç•ªå·ã‚’ä¿å­˜
        summary_ref = session_ref.collection('summaries').document(f'turn_{current_turn}')
        summary_ref.set(summary_with_turn)

        response_data = summary_data.copy()
        response_data['turn'] = session_data.get('turn', 1)
        response_data['max_turns'] = MAX_TURNS

        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã®å‘¼ã³å‡ºã—
        insights_text = summary_data.get('insights', '')
        current_turn = response_data['turn']
        threading.Thread(target=_prefetch_questions_and_save, args=(session_id, user_id, insights_text, current_turn, MAX_TURNS)).start()
        threading.Thread(target=_update_graph_cache, args=(user_id,)).start()
        
        return jsonify(response_data), 200
    except Exception as e:
        print(f"âŒ Error in post_summary for session {session_id}: {e}")
        traceback.print_exc()
        # (â˜…ä¿®æ­£) ã‚¨ãƒ©ãƒ¼æ™‚ã«ã‚‚statusã‚’æ›´æ–°
        session_ref.update({'status': 'error', 'error_message': str(e)})
        return jsonify({"error": "Failed to generate summary"}), 500


@app.route('/session/<string:session_id>/continue', methods=['POST'])
def continue_session(session_id):
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record

    user_id = user_record['uid']

    try:
        session_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)

        @firestore.transactional
        def update_turn(transaction, ref):
            snapshot = ref.get(transaction=transaction)
            if not snapshot.exists:
                raise Exception("Session not found")
            
            current_turn = snapshot.to_dict().get('turn', 1)
            new_turn = current_turn + 1
            
            if new_turn > MAX_TURNS:
                 return None, None

            transaction.update(ref, {
                'turn': new_turn,
                'status': 'processing', # â˜… çŠ¶æ…‹ã‚’ã€Œé€²è¡Œä¸­ã€ã«æˆ»ã™
                'last_updated': firestore.SERVER_TIMESTAMP
            })
            return new_turn

        transaction = db_firestore.transaction()
        new_turn = update_turn(transaction, session_ref)

        if new_turn is None:
            return jsonify({"error": "Maximum turns reached for this session."}), 400
        
        prefetched_ref = session_ref.collection('prefetched_questions').document(str(new_turn))
        prefetched_doc = prefetched_ref.get()

        generated_questions = []
        if prefetched_doc.exists:
            print(f"âœ… Using prefetched questions for turn {new_turn}")
            generated_questions = prefetched_doc.to_dict().get('questions', [])
            prefetched_ref.delete()
        else:
            print(f"âš ï¸ No prefetched questions found for turn {new_turn}. Generating now...")
            latest_summary_query = session_ref.collection('summaries').order_by('turn', direction=firestore.Query.DESCENDING).limit(1)
            latest_summary_docs = list(latest_summary_query.stream())
            if not latest_summary_docs:
                 return jsonify({"error": "Summary not found to generate follow-up questions"}), 404
            
            insights = latest_summary_docs[0].to_dict().get('insights', '')
            generated_questions = generate_follow_up_questions(insights)

        # â˜…â˜…â˜… ã“ã“ã‹ã‚‰ãŒä»Šå›ã®ä¿®æ­£ã®æ ¸å¿ƒéƒ¨åˆ†ã§ã™ â˜…â˜…â˜…
        # 1. ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹
        batch = db_firestore.batch()
        # 2. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«è¿”ã™ãŸã‚ã®ã€IDä»˜ãè³ªå•ãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–
        questions_with_ids = []

        # 3. ç”Ÿæˆã•ã‚ŒãŸè³ªå•ã‚’ãƒ«ãƒ¼ãƒ—å‡¦ç†
        for q in generated_questions:
            # a. æ–°ã—ã„è³ªå•ã®ãŸã‚ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‚ç…§ã‚’ä½œæˆï¼ˆã“ã“ã§IDãŒè‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ï¼‰
            q_ref = session_ref.collection('questions').document()
            # b. ãƒãƒƒãƒã«ã€Œè³ªå•ãƒ†ã‚­ã‚¹ãƒˆã‚’DBã«ä¿å­˜ã™ã‚‹ã€å‡¦ç†ã‚’è¿½åŠ 
            batch.set(q_ref, {"question_text": q['question_text']})
            # c. ãƒ•ãƒ­ãƒ³ãƒˆã«è¿”ã™ãƒªã‚¹ãƒˆã«ã€ã€ŒIDã€ã¨ã€Œè³ªå•ãƒ†ã‚­ã‚¹ãƒˆã€ã‚’è¿½åŠ 
            questions_with_ids.append({
                "question_id": q_ref.id,
                "question_text": q['question_text']
            })
        
        # 4. ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œã—ã€ã™ã¹ã¦ã®è³ªå•ã‚’DBã«ä¸€æ‹¬ä¿å­˜
        batch.commit()
        # â˜…â˜…â˜… ã“ã“ã¾ã§ãŒä¿®æ­£ã®æ ¸å¿ƒéƒ¨åˆ†ã§ã™ â˜…â˜…â˜…

        # 5. DBä¿å­˜å¾Œã®IDä»˜ãè³ªå•ãƒªã‚¹ãƒˆã‚’ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«è¿”ã™
        return jsonify({'questions': questions_with_ids, 'turn': new_turn}), 200

    except Exception as e:
        print(f"Error continuing session: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to continue session"}), 500


def _get_all_insights_as_text(user_id: str) -> str:
    """æŒ‡å®šã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ã‚’ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦çµåˆã™ã‚‹"""
    print(f"--- Fetching all session insights for user: {user_id} ---")
    all_insights_text = ""
    try:
        # (â˜…ä¿®æ­£) ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å‚ç…§ãƒ‘ã‚¹ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚µãƒ–ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«å¤‰æ›´
        sessions_ref = db_firestore.collection('users').document(user_id).collection('sessions').where('status', '==', 'completed').order_by('created_at', direction=firestore.Query.DESCENDING).limit(10)
        sessions_docs = sessions_ref.stream()

        for session in sessions_docs:
            session_dict = session.to_dict()
            # (â˜…ä¿®æ­£) created_at, topic, title, latest_insights ã‚’ç›´æ¥å–å¾—
            session_date = session_dict.get("created_at").strftime('%Y-%m-%d') if session_dict.get("created_at") else "ä¸æ˜ãªæ—¥ä»˜"
            session_topic = session_dict.get("topic", "ä¸æ˜ãªãƒˆãƒ”ãƒƒã‚¯")
            title = session_dict.get('title', 'ç„¡é¡Œ')
            insights = session_dict.get('latest_insights', 'åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚')
            
            summary_text_parts = [
                f"## ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ² ({session_date} - {session_topic})",
                f"### {title}\n{insights}"
            ]
            all_insights_text += "\n\n" + "\n".join(summary_text_parts)

        print(f"âœ… Found and compiled insights from past sessions.")
        return all_insights_text.strip()
    except Exception as e:
        print(f"âŒ Error fetching insights for user {user_id}: {e}")
        return ""



@app.route('/analysis/graph', methods=['GET'])
def get_analysis_graph():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´ã‹ã‚‰çµ±åˆåˆ†æã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã¾ãŸã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
    user_record = _verify_token(request)
    # â˜…â˜…â˜… ä¿®æ­£: èªè¨¼æˆåŠŸæ™‚ã¯dictå‹ã€å¤±æ•—æ™‚ã¯Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã‚‹ãŸã‚ã€dictå‹ã‹ã©ã†ã‹ã§åˆ¤å®šã™ã‚‹ â˜…â˜…â˜…
    if not isinstance(user_record, dict):
        return user_record
    
    user_id = user_record['uid']
    try:
        graph_data = _get_graph_from_cache_or_generate(user_id)
        if graph_data:
            return jsonify(graph_data), 200
        else:
            return jsonify({"error": "No data available to generate graph"}), 404
    except Exception as e:
        print(f"âŒ Error in get_analysis_graph: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to get analysis graph"}), 500


def _get_graph_from_cache_or_generate(user_id: str, force_regenerate: bool = False):
    """
    Firestoreã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã€‚
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã‚„force_regenerate=Trueã®å ´åˆã¯ã€æ–°ãŸã«ç”Ÿæˆã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    cache_ref = db_firestore.collection('analysis_cache').document(user_id)
    
    if not force_regenerate:
        cache_doc = cache_ref.get()
        if cache_doc.exists:
            cached_data = cache_doc.to_dict()
            # 24æ™‚é–“ä»¥å†…ã§ã‚ã‚Œã°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è¿”ã™
            if datetime.now(timezone.utc) - cached_data['timestamp'] < timedelta(hours=24):
                print(f"âœ… Returning cached graph data for user: {user_id}")
                return cached_data['graph_data']

    print(f"--- Generating new graph data for user: {user_id} (force_regenerate={force_regenerate}) ---")
    all_insights_text = _get_all_insights_as_text(user_id)
    if not all_insights_text:
        return None

    graph_data = generate_graph_data(all_insights_text)
    
    # æ–°ã—ã„ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    cache_ref.set({
        'graph_data': graph_data,
        'timestamp': firestore.SERVER_TIMESTAMP,
        'user_id': user_id
    })
    print(f"âœ… Generated and cached new graph data for user: {user_id}")
    
    return graph_data


@app.route('/home/suggestion', methods=['GET'])
def get_home_suggestion():
    """ãƒ›ãƒ¼ãƒ ç”»é¢ã«è¡¨ç¤ºã™ã‚‹ã€éå»ã®å¯¾è©±ã«åŸºã¥ãææ¡ˆã‚’è¿”ã™"""
    user_record = _verify_token(request)
    # â˜…â˜…â˜… ä¿®æ­£: èªè¨¼æˆåŠŸæ™‚ã¯dictå‹ã€å¤±æ•—æ™‚ã¯Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã‚‹ãŸã‚ã€dictå‹ã‹ã©ã†ã‹ã§åˆ¤å®šã™ã‚‹ â˜…â˜…â˜…
    if not isinstance(user_record, dict):
        return user_record

    user_id = user_record['uid'] 
    print(f"--- Received home suggestion request for user: {user_id} ---")

    try:
        graph_data = _get_graph_from_cache_or_generate(user_id)
        if not graph_data or 'nodes' not in graph_data or not graph_data['nodes']:
            print("No graph data available for suggestion.")
            return jsonify({}), 204 # ææ¡ˆãªã—

        nodes = graph_data['nodes']
        
        # ã‚¿ã‚¤ãƒ—ãŒ 'issue' ã¾ãŸã¯ 'topic' ã®ãƒãƒ¼ãƒ‰ã‚’å„ªå…ˆçš„ã«æŠ½å‡º
        priority_nodes = [n for n in nodes if n.get('type') in ['issue', 'topic']]
        
        # å„ªå…ˆãƒãƒ¼ãƒ‰ãŒãªã„å ´åˆã¯ã€å…¨ãƒãƒ¼ãƒ‰ã‹ã‚‰é¸ã¶
        target_nodes = priority_nodes if priority_nodes else nodes

        # ãƒãƒ¼ãƒ‰ã‚’ã‚µã‚¤ã‚ºï¼ˆé‡è¦åº¦ï¼‰ã§é™é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_nodes = sorted(target_nodes, key=lambda x: x.get('size', 0), reverse=True)
        
        if not sorted_nodes:
            print("No suitable nodes found for suggestion.")
            return jsonify({}), 204

        # æœ€ã‚‚é‡è¦ãªãƒãƒ¼ãƒ‰ã‚’ææ¡ˆã¨ã—ã¦é¸æŠ
        suggestion_node = sorted_nodes[0]
        node_label = suggestion_node.get('id', 'ä¸æ˜ãªãƒˆãƒ”ãƒƒã‚¯')
        
        response_data = {
            "title": "éå»ã®å¯¾è©±ã‚’æŒ¯ã‚Šè¿”ã£ã¦ã¿ã¾ã›ã‚“ã‹ï¼Ÿ",
            "subtitle": f"ã€Œ{node_label}ã€ã«ã¤ã„ã¦ã€æ–°ãŸãªç™ºè¦‹ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚",
            "nodeId": node_label, 
            "nodeLabel": node_label
        }
        print(f"âœ… Sending suggestion: {response_data}")
        return jsonify(response_data), 200

    except Exception as e:
        print(f"âŒ Error in get_home_suggestion: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to get home suggestion"}), 500


@app.route('/analysis/proactive_suggestion', methods=['GET'])
def get_proactive_suggestion():
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†æã‚°ãƒ©ãƒ•å…¨ä½“ã‹ã‚‰ã€èƒ½å‹•çš„ãªæ°—ä»˜ãã‚’ä¿ƒã™ãŸã‚ã®è³ªå•ã‚„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
    1. ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    2. æŠ½å‡ºã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å†…éƒ¨ï¼ˆéå»ã®å¯¾è©±ï¼‰ã¨å¤–éƒ¨ï¼ˆWebæ¤œç´¢ï¼‰ã‚’æ¤œç´¢
    3. çµæœã‚’Geminiã§è¦ç´„ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ææ¡ˆã‚’ç”Ÿæˆ
    """
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record

    user_id = user_record['uid']
    print(f"--- Received proactive suggestion request for user: {user_id} ---")

    try:
        # 1. ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ï¼ˆ=ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã®å…¨ä½“åƒï¼‰ã‚’å–å¾—
        graph_data = _get_graph_from_cache_or_generate(user_id)
        if not graph_data or 'nodes' not in graph_data or not graph_data['nodes']:
            print("No graph data available for proactive suggestion.")
            return jsonify({}), 204

        # 2. ã‚°ãƒ©ãƒ•ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º (nodeã®idã‚’çµåˆ)
        graph_keywords = ", ".join([node.get('id', '') for node in graph_data['nodes']])
        if not graph_keywords:
            print("No keywords found in graph.")
            return jsonify({}), 204
        
        print(f"Keywords from graph: {graph_keywords}")

        # 3. å†…éƒ¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆéå»ã®å¯¾è©±ï¼‰ã‚’è¦ç´„
        all_insights_text = _get_all_insights_as_text(user_id)
        # ã‚°ãƒ©ãƒ•å…¨ä½“ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸­ã‹ã‚‰ã€ç‰¹ã«é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã‚“ã§æ–‡è„ˆã‚’è¦ç´„
        chosen_keyword = np.random.choice(PROACTIVE_KEYWORDS)
        internal_summary = _summarize_internal_context(all_insights_text, chosen_keyword)

        # 4. å¤–éƒ¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆWebæ¤œç´¢ï¼‰ã‚’å–å¾—
        # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’Geminiã§ç”Ÿæˆ
        search_query_prompt = f"""
ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¾¤ã¯ã€ã‚ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ‚©ã¿ã‚„é–¢å¿ƒäº‹ã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚
ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦ã€ç¾çŠ¶ã‚’ä¹—ã‚Šè¶Šãˆã‚‹ãŸã‚ã®å…·ä½“çš„ãªãƒ’ãƒ³ãƒˆã‚„ã€å®¢è¦³çš„ãªæƒ…å ±ã‚’æä¾›ã™ã‚‹ãŸã‚ã®ã€åŠ¹æœçš„ãªWebæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’1ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {graph_keywords}
æ¤œç´¢ã‚¯ã‚¨ãƒª:"""
        
        flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
        model = GenerativeModel(flash_model)
        search_query = model.generate_content(search_query_prompt).text.strip()
        print(f"Generated search query: {search_query}")

        external_summary, sources = _generate_rag_based_advice(
            query=search_query,
            project_id=project_id,
            similar_cases_engine_id=SIMILAR_CASES_ENGINE_ID,
            suggestions_engine_id=SUGGESTIONS_ENGINE_ID,
            rag_type="suggestions" # å…·ä½“çš„ãªå¯¾ç­–ã‚’æ¤œç´¢
        )

        # 5. Geminiã§æœ€çµ‚çš„ãªææ¡ˆã‚’ç”Ÿæˆ
        final_prompt = f"""
ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‰¯ãç›¸è«‡ç›¸æ‰‹ã§ã‚ã‚Šã€æ–°ãŸãªè¦–ç‚¹ã‚’æä¾›ã™ã‚‹ã‚³ãƒ¼ãƒã§ã™ã€‚
ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œãªã‚‹ã»ã©ã€ãã‚“ãªè€ƒãˆæ–¹ã‚‚ã‚ã‚‹ã®ã‹ã€ã¨ãƒãƒƒã¨ã™ã‚‹ã‚ˆã†ãªã€å„ªã—ãã‚‚æ´å¯Ÿã«æº€ã¡ãŸèªã‚Šã‹ã‘ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

# ã‚ãªãŸã¸ã®ã‚¤ãƒ³ãƒ—ãƒƒãƒˆ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒéå»ã«è©±ã—ãŸå†…å®¹ã®è¦ç´„: {internal_summary}
- é–¢é€£ã™ã‚‹å¤–éƒ¨æƒ…å ±ã®è¦ç´„: {external_summary}
- å‚è€ƒæƒ…å ±æºURL: {", ".join(sources) if sources else "ãªã—"}

# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯
1. ä¸Šè¨˜ã®ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚’çµ±åˆã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®èªã‚Šã‹ã‘ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
2. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’åŠ±ã¾ã—ã€æ¬¡ã®ä¸€æ­©ã‚’è€ƒãˆã‚‹ãã£ã‹ã‘ã‚’ä¸ãˆã‚‹ã‚ˆã†ãªã€ãƒã‚¸ãƒ†ã‚£ãƒ–ãªãƒˆãƒ¼ãƒ³ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
3. å¿…ãšã€æœ€çµ‚çš„ãªå‡ºåŠ›ã¯ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONå½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚
   - `initialSummary`: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®èªã‚Šã‹ã‘ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆ200æ–‡å­—ç¨‹åº¦ï¼‰
   - `actions`: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã«ä½•ã‚’ã™ã¹ãã‹ã®å…·ä½“çš„ãªé¸æŠè‚¢ï¼ˆç©ºã®é…åˆ—ã§OKï¼‰
   - `nodeLabel`: 'AIã‹ã‚‰ã®ææ¡ˆ' ã¨ã„ã†å›ºå®šæ–‡å­—åˆ—
   - `nodeId`: 'proactive_suggestion' ã¨ã„ã†å›ºå®šæ–‡å­—åˆ—
"""
        
        pro_model = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
        response_json = _call_gemini_with_schema(
            final_prompt,
            schema={
                "type": "object",
                "properties": {
                    "initialSummary": {"type": "string"},
                    "actions": {"type": "array", "items": {"type": "string"}},
                    "nodeLabel": {"type": "string"},
                    "nodeId": {"type": "string"}
                },
                "required": ["initialSummary", "actions", "nodeLabel", "nodeId"]
            },
            model_name=pro_model
        )

        print(f"âœ… Sending proactive suggestion: {response_json}")
        return jsonify(response_json), 200

    except Exception as e:
        print(f"âŒ Error in get_proactive_suggestion: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to get proactive suggestion"}), 500


@app.route('/chat/node_tap', methods=['POST'])
def handle_node_tap():
    """ã‚°ãƒ©ãƒ•ä¸Šã®ãƒãƒ¼ãƒ‰ãŒã‚¿ãƒƒãƒ—ã•ã‚ŒãŸæ™‚ã«ã€é–¢é€£æƒ…å ±ã‚’è¿”ã™"""
    user_record = _verify_token(request)
    # â˜…â˜…â˜… ä¿®æ­£: èªè¨¼æˆåŠŸæ™‚ã¯dictå‹ã€å¤±æ•—æ™‚ã¯Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã‚‹ãŸã‚ã€dictå‹ã‹ã©ã†ã‹ã§åˆ¤å®šã™ã‚‹ â˜…â˜…â˜…
    if not isinstance(user_record, dict):
        return user_record

    data = request.get_json()
    if not data or 'node_label' not in data:
        return jsonify({"error": "node_label is required"}), 400

    node_label = data['node_label']
    user_id = user_record['uid']

    try:
        # 1. å†…éƒ¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆéå»ã®å¯¾è©±ï¼‰ã‚’è¦ç´„
        all_insights_text = _get_all_insights_as_text(user_id)
        internal_summary = _summarize_internal_context(all_insights_text, node_label)
        
        # 2. å¤–éƒ¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆWebæ¤œç´¢ï¼‰ã‚’å–å¾—
        external_summary_cases, sources_cases = _generate_rag_based_advice(
            query=f"{node_label}ã«é–¢ã™ã‚‹æ‚©ã¿",
            project_id=project_id,
            similar_cases_engine_id=SIMILAR_CASES_ENGINE_ID,
            suggestions_engine_id=SUGGESTIONS_ENGINE_ID,
            rag_type="similar_cases"
        )
        external_summary_sugs, sources_sugs = _generate_rag_based_advice(
            query=f"{node_label} è§£æ±ºç­–",
            project_id=project_id,
            similar_cases_engine_id=SIMILAR_CASES_ENGINE_ID,
            suggestions_engine_id=SUGGESTIONS_ENGINE_ID,
            rag_type="suggestions"
        )
        
        # 3. ãƒ•ãƒ­ãƒ³ãƒˆã«è¿”ã™æƒ…å ±ã‚’æ•´å½¢
        # ã“ã“ã§ã¯ç°¡æ½”ã«ã™ã‚‹ãŸã‚ã€Geminiã®æœ€çµ‚æ•´å½¢ã¯çœç•¥ã—ã€
        # æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ã€‚
        initial_summary = f"ã€Œ{node_label}ã€ã«ã¤ã„ã¦ã§ã™ã­ã€‚{internal_summary}"
        
        actions = []
        if external_summary_cases and "è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸ" not in external_summary_cases:
            actions.append({
                "type": "similar_cases",
                "title": "ä¼¼ãŸã‚ˆã†ãªæ‚©ã¿ã‚’æŒã¤äººã€…ã®å£°",
                "content": external_summary_cases,
                "sources": sources_cases
            })
        if external_summary_sugs and "è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸ" not in external_summary_sugs:
             actions.append({
                "type": "suggestions",
                "title": "å°‚é–€å®¶ã«ã‚ˆã‚‹å…·ä½“çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹",
                "content": external_summary_sugs,
                "sources": sources_sugs
            })

        response_data = {
            "initialSummary": initial_summary,
            "actions": actions,
            "nodeId": data.get('nodeId', node_label), # nodeIdãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†
            "nodeLabel": node_label
        }
        
        print(f"âœ… Sending node tap response for '{node_label}'")
        return jsonify(response_data), 200

    except Exception as e:
        print(f"âŒ Error in handle_node_tap: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to handle node tap"}), 500


@app.route('/analysis/chat', methods=['POST'])
def post_chat_message():
    user_record = _verify_token(request)
    # â˜…â˜…â˜… ä¿®æ­£: èªè¨¼æˆåŠŸæ™‚ã¯dictå‹ã€å¤±æ•—æ™‚ã¯Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã‚‹ãŸã‚ã€dictå‹ã‹ã©ã†ã‹ã§åˆ¤å®šã™ã‚‹ â˜…â˜…â˜…
    if not isinstance(user_record, dict):
        return user_record

    data = request.get_json()
    if not data:
        return jsonify({"error": "Invalid request: no data provided"}), 400

    chat_history = data.get('chat_history', [])
    message = data.get('message')
    use_rag = data.get('use_rag', False)
    rag_type = data.get('rag_type') # 'similar_cases' or 'suggestions'

    if not message:
        return jsonify({"error": "Invalid request: 'message' is required"}), 400

    try:
        user_id = user_record['uid']
        print(f"--- Received chat message from user: {user_id} ---")
        
        # 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        session_summary_text = _get_all_insights_as_text(user_id)
        if not session_summary_text:
             # ã‚µãƒãƒªãƒ¼ãŒãªã„å ´åˆã¯ã€RAGã‚’ä½¿ã‚ãšã«å¿œç­”ã™ã‚‹
            ai_response_text = generate_chat_response("", chat_history, message)
            return jsonify({"response": ai_response_text, "sources": []})

        # 2. RAGã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        rag_context = ""
        sources = []
        if use_rag:
            print(f"--- Generating RAG context (type: {rag_type}) ---")
            rag_context, sources = _generate_rag_based_advice(
                query=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æ:\n{session_summary_text}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{message}",
                project_id=project_id,
                similar_cases_engine_id=SIMILAR_CASES_ENGINE_ID,
                suggestions_engine_id=SUGGESTIONS_ENGINE_ID,
                rag_type=rag_type
            )
            print(f"âœ… RAG context generated. Sources: {sources}")

        # 3. Geminiã«æœ€çµ‚çš„ãªå¿œç­”ã‚’ç”Ÿæˆã•ã›ã‚‹
        ai_response_text = generate_chat_response(session_summary_text, chat_history, message, rag_context)
        
        return jsonify({"response": ai_response_text, "sources": sources})

    except Exception as e:
        print(f"âŒ Error in post_chat_message: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to process chat message"}), 500


@app.route('/home/suggestion_v2', methods=['GET'])
def get_home_suggestion_v2():
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€æ–°ã®ãƒ™ã‚¯ãƒˆãƒ«ã«åŸºã¥ãã€Vertex AI Vector Search ã‚’ä½¿ã£ã¦é¡ä¼¼ã—ãŸéå»ã®å¯¾è©±ãƒãƒ¼ãƒ‰ã‚’æ¤œç´¢ã—ã€
    ãƒ›ãƒ¼ãƒ ç”»é¢ã§æ–°ã—ã„å¯¾è©±ã®ãã£ã‹ã‘ã‚’ææ¡ˆã—ã¾ã™ã€‚
    """
    user_record = _verify_token(request)
    # â˜…â˜…â˜… ä¿®æ­£: èªè¨¼æˆåŠŸæ™‚ã¯dictå‹ã€å¤±æ•—æ™‚ã¯Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã‚‹ãŸã‚ã€dictå‹ã‹ã©ã†ã‹ã§åˆ¤å®šã™ã‚‹ â˜…â˜…â˜…
    if not isinstance(user_record, dict):
        return user_record

    user_id = user_record['uid']
    print(f"--- Received home suggestion v2 request for user: {user_id} ---")

    # (â˜…ä¿®æ­£) Vector Searchç”¨ã®ãƒªãƒ¼ã‚¸ãƒ§ãƒ³å¤‰æ•°ã‚’æ˜ç¤ºçš„ã«å–å¾—
    vector_search_region = os.getenv('GCP_VERTEX_AI_REGION', 'asia-northeast1')


    # ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if not all([VECTOR_SEARCH_INDEX_ID, VECTOR_SEARCH_ENDPOINT_ID, VECTOR_SEARCH_DEPLOYED_INDEX_ID]):
        print("âŒ ERROR: Vector Search environment variables are not set on the server.")
        return jsonify({"error": "Server configuration error for suggestions."}), 500

    try:
        # 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€æ–°ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
        query_ref = db_firestore.collection('vector_embeddings').where('user_id', '==', user_id).order_by('created_at', direction=firestore.Query.DESCENDING).limit(1)
        docs = list(query_ref.stream())

        if not docs:
            print(f"No vector embeddings found for user {user_id}.")
            return jsonify({}), 204 # ææ¡ˆãªã—

        latest_doc = docs[0]
        latest_doc_data = latest_doc.to_dict()
        latest_embedding = latest_doc_data.get('embedding')
        
        if not latest_embedding:
            print(f"Embedding not found in the latest document for user {user_id}.")
            return jsonify({}), 204

        print(f"Found latest embedding for user {user_id}. Searching for neighbors...")

        # 2. Vertex AI Vector Search ã§è¿‘å‚æ¢ç´¢
        endpoint_resource_name = f"projects/{project_id}/locations/{vector_search_region}/indexEndpoints/{VECTOR_SEARCH_ENDPOINT_ID}"
        my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(index_endpoint_name=endpoint_resource_name)

        response = my_index_endpoint.find_neighbors(
            queries=[latest_embedding],
            num_neighbors=5, # è‡ªåˆ†è‡ªèº«ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§å¤šã‚ã«å–å¾—
            deployed_index_id=VECTOR_SEARCH_DEPLOYED_INDEX_ID
        )

        if not response or not response[0]:
             print("No similar nodes found from vector search.")
             return jsonify({}), 204

        # 3. æ¤œç´¢çµæœã®å‡¦ç†
        # è‡ªåˆ†è‡ªèº«ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’é™¤å¤–
        filtered_neighbors = [neighbor for neighbor in response[0] if neighbor.id != latest_doc.id]

        if not filtered_neighbors:
            print("No other similar nodes found after filtering.")
            return jsonify({}), 204

        # 4. ææ¡ˆã™ã‚‹ãƒãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦è©³ç´°æƒ…å ±ã‚’å–å¾—
        # æœ€ã‚‚é¡ä¼¼åº¦ãŒé«˜ã„ã‚‚ã®ã‚’é¸æŠ
        suggestion_neighbor = filtered_neighbors[0]
        
        # Vector Searchã®IDã¯ `vector_embeddings` ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã¨ä¸€è‡´ã™ã‚‹
        suggestion_ref = db_firestore.collection('vector_embeddings').document(suggestion_neighbor.id)
        suggestion_doc = suggestion_ref.get()

        if not suggestion_doc.exists:
            print(f"Suggested document {suggestion_neighbor.id} not found in Firestore.")
            return jsonify({}), 204

        suggestion_data = suggestion_doc.to_dict()
        node_label = suggestion_data.get('nodeLabel')
        node_id = suggestion_data.get('nodeId')

        if not node_label or not node_id:
            print(f"nodeLabel or nodeId missing in suggested document {suggestion_neighbor.id}.")
            return jsonify({}), 204

        # 5. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«è¿”ã™ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
        response_data = {
            "title": "éå»ã®å¯¾è©±ã‚’æŒ¯ã‚Šè¿”ã£ã¦ã¿ã¾ã›ã‚“ã‹ï¼Ÿ",
            "subtitle": f"ã€Œ{node_label}ã€ã«ã¤ã„ã¦ã€æ–°ãŸãªç™ºè¦‹ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚",
            "nodeId": node_id,
            "nodeLabel": node_label
        }
        print(f"âœ… Sending suggestion v2: {response_data}")
        return jsonify(response_data), 200

    except Exception as e:
        print(f"âŒ Error in get_home_suggestion_v2: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to get home suggestion"}), 500


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 8080)), debug=True)