import firebase_admin
from firebase_admin import credentials, firestore, auth, app_check
from flask import Flask, request, jsonify, Blueprint, abort
from flask_cors import CORS

import os
import json
import re
import traceback
import threading
import requests
import urllib.parse
from bs4 import BeautifulSoup
import numpy as np
import hashlib
import uuid
from datetime import datetime, timedelta, timezone
from collections import Counter
import textwrap

from google.cloud import aiplatform
from google.cloud import tasks_v2
from tenacity import retry, stop_after_attempt, wait_exponential
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig
from vertexai.language_models import TextEmbeddingModel
from google.cloud import discoveryengine_v1 as discoveryengine
from langchain.text_splitter import RecursiveCharacterTextSplitter


# --- GCP & Firebase åˆæœŸåŒ– ---
try:
    print("Initializing GCP services using Application Default Credentials...")
    firebase_admin.initialize_app()
    db_firestore = firestore.client()
    
    app_instance = firebase_admin.get_app()
    project_id = app_instance.project_id
    print(f"âœ… Firebase Admin SDK initialized for project: {project_id}")

    # (â˜…ä¿®æ­£) Vector Searchã¨Geminiã§ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’åˆ†ã‘ã‚‹
    # Vector Searchã¯æ±äº¬ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ (`asia-northeast1`) ã‚’ä½¿ç”¨
    vector_search_region = os.getenv('GCP_VERTEX_AI_REGION', 'asia-northeast1')
    # Geminiãƒ¢ãƒ‡ãƒ«ã¯ç±³å›½ä¸­éƒ¨ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ (`us-central1`) ã‚’ä½¿ç”¨
    gemini_region = os.getenv('GCP_GEMINI_REGION', 'us-central1')
    
    vertexai.init(project=project_id, location=gemini_region)
    print(f"âœ… Vertex AI initialized for project: {project_id}. Gemini region: {gemini_region}, Vector Search region: {vector_search_region}")

    # Cloud Tasks Client Initialization
    tasks_client = None
    GCP_TASK_QUEUE = None
    GCP_TASK_QUEUE_LOCATION = None
    GCP_TASK_SA_EMAIL = None
    SERVICE_URL = None

    # Only attempt to initialize Cloud Tasks in the Cloud Run environment
    if 'K_SERVICE' in os.environ:
        GCP_TASK_QUEUE = os.getenv('GCP_TASK_QUEUE', '').strip()
        GCP_TASK_QUEUE_LOCATION = os.getenv('GCP_TASK_QUEUE_LOCATION', '').strip()
        GCP_TASK_SA_EMAIL = os.getenv('GCP_TASK_SA_EMAIL', '').strip()
        SERVICE_URL = os.getenv('K_SERVICE_URL', '').strip()

        # Check which variables are missing for better debugging
        required_vars = {
            'GCP_TASK_QUEUE': GCP_TASK_QUEUE,
            'GCP_TASK_QUEUE_LOCATION': GCP_TASK_QUEUE_LOCATION,
            'GCP_TASK_SA_EMAIL': GCP_TASK_SA_EMAIL,
            'K_SERVICE_URL': SERVICE_URL,
        }
        missing_vars = [key for key, value in required_vars.items() if not value]

        if not missing_vars:
            try:
                tasks_client = tasks_v2.CloudTasksClient()
                print(f"âœ… Cloud Tasks client initialized. Queue: {GCP_TASK_QUEUE} in {GCP_TASK_QUEUE_LOCATION}")
            except Exception as e:
                print(f"âŒ Failed to initialize Cloud Tasks client, even though variables were set: {e}")
                traceback.print_exc()
        else:
            # This is the key log message for debugging
            print(f"âš ï¸ Cloud Tasks is disabled. Missing environment variables: {', '.join(missing_vars)}. Background tasks will not be created.")
    else:
        print("â„¹ï¸ Not running in Cloud Run ('K_SERVICE' not set). Skipping Cloud Tasks initialization.")

    # RAGç”¨è¨­å®š
    SIMILAR_CASES_ENGINE_ID = os.getenv('SIMILAR_CASES_ENGINE_ID')
    SUGGESTIONS_ENGINE_ID = os.getenv('SUGGESTIONS_ENGINE_ID')

    # Vector Search ç”¨è¨­å®š
    VECTOR_SEARCH_INDEX_ID = os.getenv('VECTOR_SEARCH_INDEX_ID')
    VECTOR_SEARCH_ENDPOINT_ID = os.getenv('VECTOR_SEARCH_ENDPOINT_ID')
    VECTOR_SEARCH_DEPLOYED_INDEX_ID = os.getenv('VECTOR_SEARCH_DEPLOYED_INDEX_ID')
    if 'K_SERVICE' in os.environ:
        if not all([VECTOR_SEARCH_INDEX_ID, VECTOR_SEARCH_ENDPOINT_ID, VECTOR_SEARCH_DEPLOYED_INDEX_ID]):
             print("âš ï¸ WARNING: Vector Search environment variables are not fully set.")

    GOOGLE_BOOKS_API_KEY = None
    # Cloud Run v2ã®Secret Managerãƒã‚¦ãƒ³ãƒˆãƒ‘ã‚¹
    secret_path = '/secrets/google-books-api-key'
    if os.path.exists(secret_path):
        with open(secret_path, 'r') as f:
            GOOGLE_BOOKS_API_KEY = f.read().strip()
        print("âœ… Loaded Google Books API key from Secret Manager.")
    else:
        print("âš ï¸ Secret file not found. Trying to load Google Books API key from environment variable.")
        GOOGLE_BOOKS_API_KEY = os.environ.get('GOOGLE_BOOKS_API_KEY')

    OLLAMA_ENDPOINT = os.environ.get('OLLAMA_ENDPOINT')
    # â˜…â˜…â˜… ã“ã“ã«ãƒ¢ãƒ‡ãƒ«åèª­ã¿è¾¼ã¿ã‚’è¿½åŠ  â˜…â˜…â˜…
    OLLAMA_MODEL_NAME = os.environ.get('OLLAMA_MODEL_NAME', 'gemma3:12b') # ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç”¨ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š

    if OLLAMA_ENDPOINT:
        print(f"âœ… Ollama service endpoint is configured: {OLLAMA_ENDPOINT}")
        # â˜…â˜…â˜… ã“ã“ã«ãƒ­ã‚°ã‚’è¿½åŠ  â˜…â˜…â˜…
        print(f"âœ… Ollama model name is set to: {OLLAMA_MODEL_NAME}")
    else:
        # â˜…â˜…â˜… ä¿®æ­£: ã“ã®ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®Ÿæ…‹ã«åˆã‚ã›ã¦å¤‰æ›´ â˜…â˜…â˜…
        print("âš ï¸ Ollama service endpoint is not configured. PII check with Gemma will be disabled.")

except Exception as e:
    db_firestore = None
    print(f"âŒ Error during initialization: {e}")
    traceback.print_exc()
    if 'K_SERVICE' in os.environ:
        raise

app = Flask(__name__)

api_bp = Blueprint('api', __name__, url_prefix='/api')

@api_bp.before_request
def verify_app_check():
    # Cloud Tasksã‹ã‚‰ã®å†…éƒ¨å‘¼ã³å‡ºã—ï¼ˆ/api/tasks/..ï¼‰ã¯App Checkã®æ¤œè¨¼ã‹ã‚‰é™¤å¤–ã™ã‚‹ã€‚
    # ã¾ãŸã€ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒï¼ˆK_SERVICEç’°å¢ƒå¤‰æ•°ãŒãªã„ï¼‰ã§ã‚‚ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚
    if 'K_SERVICE' in os.environ and not request.path.startswith('/api/tasks/'):
        app_check_token = request.headers.get('X-Firebase-AppCheck')

        if app_check_token is None:
            # ãƒˆãƒ¼ã‚¯ãƒ³ãŒãªã„å ´åˆã¯401ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
            print("App Check: Token is missing.")
            # â˜…â˜…â˜… abort -> jsonify ã«å¤‰æ›´ â˜…â˜…â˜…
            return jsonify({"error": "App Check token is missing."}), 401

        try:
            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ¤œè¨¼ã€‚ç„¡åŠ¹ãªå ´åˆã¯ä¾‹å¤–ãŒç™ºç”Ÿã™ã‚‹ã€‚
            app_check.verify_token(app_check_token)
            print("âœ… App Check: Token verified.")
        except Exception as e:
            # æ¤œè¨¼ã«å¤±æ•—ã—ãŸå ´åˆã¯401ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
            print(f"âŒ App Check: Token verification failed: {e}")
            # â˜…â˜…â˜… abort -> jsonify ã«å¤‰æ›´ â˜…â˜…â˜…
            return jsonify({"error": f"Invalid App Check token: {e}"}), 401


# --- CORSè¨­å®š ---
prod_origin = os.getenv('PROD_ORIGIN_URL')

if 'K_SERVICE' in os.environ:
    # æœ¬ç•ªç’°å¢ƒã§ã¯ã€è¨­å®šã•ã‚ŒãŸã‚ªãƒªã‚¸ãƒ³ã®ã¿ã‚’è¨±å¯
    origins = [prod_origin] if prod_origin else []
else:
    # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™º/ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ›ã‚¹ãƒˆã¨è¨­å®šã•ã‚ŒãŸã‚ªãƒªã‚¸ãƒ³ã‚’è¨±å¯
    local_origins = [
        prod_origin,
        re.compile(r"http://localhost:.*"),
        re.compile(r"http://127.0.0.1:.*"),
    ]
    # NoneãŒå«ã¾ã‚Œãªã„ã‚ˆã†ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    origins = [origin for origin in local_origins if origin]

CORS(app, resources={r"/api/*": {"origins": origins}})


@api_bp.route('/', methods=['GET'])
def index():
    return "GuchiSwipe Gateway is running.", 200

# ===== RAG Cache Settings =====
RAG_CACHE_COLLECTION = 'rag_cache'
RAG_CACHE_TTL_DAYS = 7 # Cache expires after 7 days

# â˜…â˜…â˜… ä¿®æ­£: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ã‚’å®šç¾© â˜…â˜…â˜…
MAX_TURNS = 5 # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ï¼ˆåˆæœŸã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€ï¼‰


# ===== JSONã‚¹ã‚­ãƒ¼ãƒå®šç¾© =====
QUESTIONS_SCHEMA = {"type": "object","properties": {"questions": {"type": "array","items": {"type": "object","properties": {"question_text": {"type": "string"}},"required": ["question_text"]}}},"required": ["questions"]}
SUMMARY_SCHEMA = {"type": "object","properties": {"title": {"type": "string", "description": "ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã‚’è¦ç´„ã™ã‚‹15æ–‡å­—ç¨‹åº¦ã®çŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«"},"insights": {"type": "string", "description": "æŒ‡å®šã•ã‚ŒãŸMarkdownå½¢å¼ã§ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æãƒ¬ãƒãƒ¼ãƒˆ"}},"required": ["title", "insights"]}
GRAPH_SCHEMA = {"type": "object","properties": {"nodes": {"type": "array","items": {"type": "object","properties": {"id": {"type": "string"},"type": {"type": "string", "enum": ["emotion", "topic", "keyword", "issue"]},"size": {"type": "integer"}},"required": ["id", "type", "size"]}},"edges": {"type": "array","items": {"type": "object","properties": {"source": {"type": "string"},"target": {"type": "string"},"weight": {"type": "integer"}},"required": ["source", "target", "weight"]}}},"required": ["nodes", "edges"]}
TOPIC_SUGGESTION_SCHEMA = {
    "type": "object",
    "properties": {
        "suggestions": {
            "type": "array",
            "description": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ææ¡ˆã¨ãªã‚‹ã€å…·ä½“çš„ã§é­…åŠ›çš„ãªå¯¾è©±ãƒ†ãƒ¼ãƒã®ãƒªã‚¹ãƒˆï¼ˆ3ã¤ï¼‰",
            "items": {"type": "string"}
        }
    },
    "required": ["suggestions"]
}
KEYWORDS_SCHEMA = {
    "type": "object",
    "properties": {
        "keywords": {
            "type": "array",
            "description": "æ¤œç´¢ã«ä½¿ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆ3ã¤ï¼‰",
            "items": {"type": "string"}
        }
    },
    "required": ["keywords"]
}
BOOK_RECOMMENDATION_SCHEMA = {
    "type": "object",
    "properties": {
        "recommendations": {
            "type": "array",
            "description": "3å†Šã®ãŠã™ã™ã‚æ›¸ç±ã®ãƒªã‚¹ãƒˆ",
            "items": {
                "type": "object",
                "properties": {
                    "title": {"type": "string", "description": "æ›¸ç±ã®æ­£å¼ãªã‚¿ã‚¤ãƒˆãƒ«"},
                    "author": {"type": "string", "description": "è‘—è€…å"},
                    "reason": {"type": "string", "description": "ã“ã®æœ¬ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ãªãœãŠã™ã™ã‚ãªã®ã‹ã€å…·ä½“çš„ãªç†ç”±ï¼ˆ100æ–‡å­—ç¨‹åº¦ï¼‰"},
                    "search_url": {"type": "string", "description": "æ›¸ç±åã¨è‘—è€…åã§Googleæ¤œç´¢ã™ã‚‹ãŸã‚ã®URL"}
                },
                "required": ["title", "author", "reason", "search_url"]
            }
        }
    },
    "required": ["recommendations"]
}

# ===== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ =====
SUMMARY_ONLY_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã‚’å®¢è¦³çš„ã«æ•´ç†ã—ã€è¨€èªåŒ–ã™ã‚‹ã®ã‚’æ‰‹ä¼ã†AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€Œ{topic}ã€ã¨ã„ã†ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦å¯¾è©±ã—ã¦ã„ã¾ã™ã€‚
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ä¼šè©±å±¥æ­´ï¼ˆã¯ã„/ã„ã„ãˆ ã®å›ç­”ï¼‰ã‚’åˆ†æã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã‚’æ§‹é€ åŒ–ã—ã¦ãã ã•ã„ã€‚
å¿ƒç†çš„ãªåˆ†æã‚„æ–­å®šã¯é¿ã‘ã€ã‚ãã¾ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹äº‹å®Ÿã«åŸºã¥ã„ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# åˆ†æå¯¾è±¡ã®ä¼šè©±å±¥æ­´
{swipes_text}

# å‡ºåŠ›å½¢å¼ (JSON)
å¿…ãšä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
- `title`: ä¼šè©±å…¨ä½“ã‚’è±¡å¾´ã™ã‚‹15æ–‡å­—ç¨‹åº¦ã®çŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«ã€‚
- `insights`: ä»¥ä¸‹ã®Markdownå½¢å¼ã§ **å³å¯†ã«** è¨˜è¿°ã•ã‚ŒãŸæ€è€ƒæ•´ç†ãƒ¬ãƒãƒ¼ãƒˆã€‚
```markdown
### âœ¨ å…¨ä½“çš„ãªè¦ç´„
ï¼ˆã“ã“ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã“ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã©ã®ã‚ˆã†ãªè€ƒãˆã‚’æŒã£ã¦ã„ã‚‹ã‹ã€ä¸»ãªè«–ç‚¹ã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’2ã€œ3æ–‡ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ï¼‰
### ğŸ“ æ€è€ƒã®æ•´ç†
ï¼ˆã“ã“ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã‹ã‚‰è¦‹ãˆã‚‹æ€è€ƒã®æ§‹é€ ã‚’ç®‡æ¡æ›¸ãã§è¨˜è¿°ã—ã¦ãã ã•ã„ï¼‰
* **ä¸­å¿ƒçš„ãªè€ƒãˆ**: ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã“ã®ãƒˆãƒ”ãƒƒã‚¯ã§æœ€ã‚‚é‡è¦–ã—ã¦ã„ã‚‹ã¨æ€ã‚ã‚Œã‚‹è€ƒãˆã‚„ä¾¡å€¤è¦³ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ï¼‰
* **æ€è€ƒã®ãƒ‘ã‚¿ãƒ¼ãƒ³**: ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã‹ã‚‰è¦‹ãˆã‚‹ã€æ€è€ƒã®ç¹‹ãŒã‚Šã‚„å¯¾ç«‹ã™ã‚‹è€ƒãˆã€ç¹°ã‚Šè¿”ã—ç¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã©ã‚’å…·ä½“çš„ã«æŒ™ã’ã¦ãã ã•ã„ã€‚ä¾‹ï¼šã€ŒAã«ã¤ã„ã¦ã¯è‚¯å®šçš„ã ãŒã€Bã®å´é¢ã§ã¯å¦å®šçš„ã€ã¨ã„ã£ãŸæ§‹é€ ã‚’æŒ‡æ‘˜ã—ã¾ã™ï¼‰
* **æ˜ã‚Šä¸‹ã’ã‚‹ã¹ãå•ã„**: ï¼ˆã“ã®å¯¾è©±å…¨ä½“ã‚’è¸ã¾ãˆã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã«è€ƒãˆã‚‹ã¨è‰¯ã•ãã†ãªå•ã„ã‚’1ã€œ2å€‹æç¤ºã—ã¦ãã ã•ã„ã€‚ä¾‹ï¼šã€Œã€‡ã€‡ã‚’é”æˆã™ã‚‹ãŸã‚ã«ã¯ã€ä½•ãŒæœ€ã‚‚é‡è¦ã ã¨è€ƒãˆã¦ã„ã¾ã™ã‹ï¼Ÿã€ï¼‰
### ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®ææ¡ˆ
ï¼ˆä»Šå›ã®æ€è€ƒæ•´ç†ã‚’è¸ã¾ãˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡å›ã®å¯¾è©±ã§æ·±æ˜ã‚Šã™ã‚‹ã¨è‰¯ã•ãã†ãªãƒ†ãƒ¼ãƒã‚„ã€è€ƒãˆã‚’ã•ã‚‰ã«æ˜ç¢ºã«ã™ã‚‹ãŸã‚ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å…·ä½“çš„ã«ææ¡ˆã—ã¦ãã ã•ã„ï¼‰
```
"""

GRAPH_ANALYSIS_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²ã‚’åˆ†æã—ã€æ€è€ƒã®æ§‹é€ ã‚’ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã«ã—ã¦ãã ã•ã„ã€‚
# ã‚°ãƒ©ãƒ•ã®ãƒ«ãƒ¼ãƒ«
1. æ§‹é€ : ã‚°ãƒ©ãƒ•ã¯å¿…ãšã€Œtopic -> issue -> (keywordã¾ãŸã¯emotion)ã€ã¨ã„ã†å³å¯†ãªéšå±¤æ§‹é€ ã«å¾“ã£ã¦ãã ã•ã„ã€‚
+ topic: ä¸­å¿ƒãƒ†ãƒ¼ãƒ (1-2å€‹)
+ issue: å…·ä½“çš„ãªå•é¡Œ (topicã‹ã‚‰æ´¾ç”Ÿ)
+ keyword: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (issueã‹ã‚‰æ´¾ç”Ÿ)
+ emotion: æ„Ÿæƒ… (issueã‹ã‚‰æ´¾ç”Ÿ)
2. æ¥ç¶š:
+ topicãŒå§‹ç‚¹ã§ã™ã€‚
+ issueã¯topicã«æ¥ç¶šã—ã¾ã™ã€‚
+ keywordã¨emotionã¯ã€é–¢é€£ã™ã‚‹issueã«æ¥ç¶šã—ã¾ã™ã€‚
3. ãƒãƒ¼ãƒ‰:
+ idã¯æ—¥æœ¬èªã®çŸ­ã„å˜èªã«ã—ã¦ãã ã•ã„ã€‚
+ ç·ãƒãƒ¼ãƒ‰æ•°ã¯15å€‹ä»¥å†…ã«ã—ã¦ãã ã•ã„ã€‚
# å‡ºåŠ›å½¢å¼
å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸è¦ã§ã™ã€‚
{ "nodes": [ { "id": "...", "type": "...", "size": ... } ], "edges": [ { "source": "...", "target": "...", "weight": ... } ] }
# ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²
"""

CHAT_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æã®å°‚é–€å®¶ã§ã‚ã‚Šã€å…±æ„ŸåŠ›ã¨æ´å¯ŸåŠ›ã«å„ªã‚ŒãŸã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã€Œã‚³ã‚³ãƒ­ã®åˆ†æå®˜ã€ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€è‡ªèº«ã®æ€è€ƒã‚’å¯è¦–åŒ–ã—ãŸã‚°ãƒ©ãƒ•ã‚’è¦‹ãªãŒã‚‰ã€ã‚ãªãŸã¨å¯¾è©±ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™
"""
CHAT_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æã®å°‚é–€å®¶ã§ã‚ã‚Šã€å…±æ„ŸåŠ›ã¨æ´å¯ŸåŠ›ã«å„ªã‚ŒãŸã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã€Œã‚³ã‚³ãƒ­ã®åˆ†æå®˜ã€ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€è‡ªèº«ã®æ€è€ƒã‚’å¯è¦–åŒ–ã—ãŸã‚°ãƒ©ãƒ•ã‚’è¦‹ãªãŒã‚‰ã€ã‚ãªãŸã¨å¯¾è©±ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚
# ã‚ãªãŸã®å½¹å‰²
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®éå»ã®ä¼šè©±å±¥æ­´ã¨ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã®è¦ç´„ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ï¼‰ã‚’å¸¸ã«å‚ç…§ã—ã€æ–‡è„ˆã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã‚’æ·±ãå‚¾è´ã—ã€ã¾ãšã¯è‚¯å®šçš„ã«å—ã‘æ­¢ã‚ã¦å…±æ„Ÿã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚
- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ã®å†…å®¹ã«åŸºã¥ãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‡ªåˆ†ã§ã‚‚æ°—ã¥ã„ã¦ã„ãªã„å†…é¢ã‚’å„ªã—ãæŒ‡æ‘˜ã—ãŸã‚Šã€æ·±ã„å•ã„ã‚’æŠ•ã’ã‹ã‘ãŸã‚Šã—ã¦ã€è‡ªå·±ç†è§£ã‚’ä¿ƒã—ã¦ãã ã•ã„ã€‚
- æ¯å›ã®è¿”ä¿¡ã‚’è‡ªå·±ç´¹ä»‹ã‹ã‚‰å§‹ã‚ã‚‹ã®ã§ã¯ãªãã€ä¼šè©±ã®æµã‚Œã‚’è‡ªç„¶ã«å¼•ãç¶™ã„ã§ãã ã•ã„ã€‚
- **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åå‰ï¼ˆã€Œã€‡ã€‡ã•ã‚“ã€ãªã©ï¼‰ã¯çµ¶å¯¾ã«ä½¿ç”¨ã›ãšã€å¸¸ã«å¯¾è©±ç›¸æ‰‹ã«ç›´æ¥èªã‚Šã‹ã‘ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚**
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼
{session_summary}
# ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
{chat_history}
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»Šå›ã®ç™ºè¨€
{user_message}
ã‚ãªãŸã®å¿œç­”:
"""
INTERNAL_CONTEXT_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°è¨˜éŒ²ã‚’è¦ç´„ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²å…¨ä½“ã‹ã‚‰ã€ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã«é–¢é€£ã™ã‚‹è¨˜è¿°ã‚„ã€ãã“ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„Ÿæƒ…ã‚„è‘›è—¤ã‚’æŠœãå‡ºã—ã€1ã€œ2æ–‡ã®éå¸¸ã«ç°¡æ½”ãªè¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
è¦ç´„ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã€Œä»¥å‰ã€ã“ã®ä»¶ã«ã¤ã„ã¦ã“ã®ã‚ˆã†ã«ãŠè©±ã—ã•ã‚Œã¦ã„ã¾ã—ãŸã­ã€ã¨è‡ªç„¶ã«èªã‚Šã‹ã‘ã‚‹å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ç›´æ¥é–¢é€£ã™ã‚‹è¨˜è¿°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ã€Œã“ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€ã“ã‚Œã¾ã§å…·ä½“çš„ãªãŠè©±ã¯ãªã‹ã£ãŸã‚ˆã†ã§ã™ã€‚ã€ã¨å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

# ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²
{context}

# è¦ç´„:
"""

PROACTIVE_KEYWORDS = [
    "ç‡ƒãˆå°½ã", "ãƒãƒ¼ãƒ³ã‚¢ã‚¦ãƒˆ", "ç„¡æ°—åŠ›", "ç–²å¼Š",
    "ã‚­ãƒ£ãƒªã‚¢", "è»¢è·", "ä»•äº‹ã®æ‚©ã¿", "å°†æ¥è¨­è¨ˆ",
    "å¯¾äººé–¢ä¿‚", "å­¤ç‹¬", "äººé–“é–¢ä¿‚", "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³",
    "è‡ªå·±è‚¯å®šæ„Ÿ", "è‡ªä¿¡ãŒãªã„", "è‡ªåˆ†ã‚’è²¬ã‚ã‚‹",
    "ã‚¹ãƒˆãƒ¬ã‚¹", "ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼", "ä¸å®‰"
]

def _check_content_safety_with_gemma(text: str) -> bool:
    """
    Calls a Gemma model via Ollama to check for PII and harmful content.
    Returns True if problematic content is likely present, False otherwise.
    """
    if not OLLAMA_ENDPOINT or not OLLAMA_MODEL_NAME:
        print("âš ï¸ Gemma safety check is disabled (Ollama endpoint not configured).")
        return False # Fail-safe: assume content is safe if the checker is down.

    prompt = f"""
Analyze the following text for two types of issues:
1. Personally Identifiable Information (PII): full names, email addresses, phone numbers, physical addresses, etc.
2. Harmful or Abusive Content: slander, defamation, hate speech, or any other form of abusive language.

Respond with only 'YES' if either PII or harmful content is found, and 'NO' if the text is safe. Do not provide any explanation.

TEXT:
---
{text}
---

RESPONSE:
"""
    try:
        print(f"--- Checking for content safety with Gemma ({OLLAMA_MODEL_NAME}) ---")
        # ä¿®æ­£: Ollamaã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆURLã« `/api/generate` ã‚’è¿½åŠ 
        response = requests.post(
            f"{OLLAMA_ENDPOINT}/api/generate",
            json={
                "model": OLLAMA_MODEL_NAME,
                "prompt": prompt,
                "stream": False,
                "options": { "temperature": 0.0 }
            },
            timeout=20 # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’20ç§’ã«è¨­å®š
        )
        response.raise_for_status()
        gemma_response = response.json().get('response', '').strip().upper()
        print(f"âœ… Gemma safety check response: '{gemma_response}'")
        return 'YES' in gemma_response
    except requests.RequestException as e:
        # ä¿®æ­£: ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ã‚ˆã‚Šå…·ä½“çš„ã«
        print(f"âŒ Could not connect to Gemma service for safety check: {e}")
        return False # Fail-safe: assume content is safe if there's an error.
    except Exception as e:
        print(f"âŒ An unexpected error occurred during safety check: {e}")
        return False


# ===== Gemini ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ =====
@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
def _call_gemini_with_schema(prompt: str, schema: dict, model_name: str, pii_check: bool = True) -> dict:
    """
    Calls a Gemini model with a specified response schema, including an optional PII check with Gemma.
    If PII is detected, it will retry the call with a request to remove PII.
    """
    model = GenerativeModel(model_name)
    attempt_num = _call_gemini_with_schema.retry.statistics.get('attempt_number', 1)
    print(f"--- Calling Gemini ({model_name}) with schema (Attempt: {attempt_num}) ---")
    response = None # responseã‚’äº‹å‰ã«åˆæœŸåŒ–
    try:
        response = model.generate_content(prompt, generation_config=GenerationConfig(response_mime_type="application/json", response_schema=schema))
        response_text = response.text.strip()

        # Gemmaã«ã‚ˆã‚‹PIIãƒã‚§ãƒƒã‚¯
        if pii_check and _check_cotent_safety_with_gemma(response_text):
            print("âš ï¸ PII detected by Gemma. Retrying Gemini call with PII removal request.")
            # æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            pii_removal_prompt = f"""
The following text was generated, but it may contain personally identifiable information (PII).
Please regenerate the content based on the original request, ensuring that all PII (names, addresses, contact info, etc.) is removed or replaced with generic placeholders.
The output format MUST strictly adhere to the original JSON schema.

Original Text with Potential PII:
---
{response_text}
---

Original Prompt:
---
{prompt}
---

Please provide the revised, PII-free response now:
"""
            # PIIé™¤å»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å†å¸°çš„ã«è‡ªèº«ã‚’å‘¼ã³å‡ºã™ï¼ˆãŸã ã—ã€æ¬¡ã¯PIIãƒã‚§ãƒƒã‚¯ã‚’ã—ãªã„ï¼‰
            return _call_gemini_with_schema(pii_removal_prompt, schema, model_name, pii_check=False)

        # JSONã®æ•´å½¢
        if response_text.startswith("```json"):
            response_text = response_text[7:-3].strip()
        elif response_text.startswith("```"):
            response_text = response_text[3:-3].strip()
            
        return json.loads(response_text)
    except Exception as e:
        print(f"Error on attempt {attempt_num} with model {model_name}: {e}\n--- Gemini Response ---\n{getattr(response, 'text', 'Empty')}\n---")
        traceback.print_exc()
        raise

def generate_initial_questions(topic, user_id):
    """ãƒˆãƒ”ãƒƒã‚¯ã¨éå»ã®å¯¾è©±å±¥æ­´ã«åŸºã¥ã„ã¦ã€æ–°ã—ã„åˆæœŸè³ªå•ã‚’ç”Ÿæˆã™ã‚‹"""
    past_insights = _get_all_insights_as_text(user_id)

    if past_insights:
        prompt = f"""
ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã‚’æ•´ç†ã™ã‚‹ã€å„ªç§€ãªã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ä»Šå›ã€Œ{topic}ã€ã¨ã„ã†ãƒ†ãƒ¼ãƒã‚’é¸ã³ã¾ã—ãŸã€‚

ä»¥ä¸‹ã®ã€Œéå»ã®å¯¾è©±ã®è¦ç´„ã€ã‚’è¸ã¾ãˆã¦ã€ä»Šå›ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå–ã‚Šçµ„ã‚€ã¹ãã€æ–°ã—ã„åˆ‡ã‚Šå£ã®è³ªå•ã‚’5ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

# éå»ã®å¯¾è©±ã®è¦ç´„
{past_insights}
---

# è³ªå•ç”Ÿæˆã®ãƒ«ãƒ¼ãƒ«
- è³ªå•ã¯ã€Œ{topic}ã€ã«é–¢é€£ã™ã‚‹ã‚‚ã®ã«ã—ã¦ãã ã•ã„ã€‚
- éå»ã®å¯¾è©±ã§æ—¢ã«è§¦ã‚Œã‚‰ã‚Œã¦ã„ã‚‹å†…å®¹ã‚„ã€åŒã˜ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®è³ªå•ã¯é¿ã‘ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ·±ãå†…çœã§ãã‚‹ã‚ˆã†ãªã€æœ¬è³ªçš„ãªå•ã„ã«ã—ã¦ãã ã•ã„ã€‚
- å¿…ãšã€Œã¯ã„ã€ã‹ã€Œã„ã„ãˆã€ã§ç­”ãˆã‚‰ã‚Œã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªå½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚
- ç”Ÿæˆã™ã‚‹ã®ã¯è³ªå•ãƒªã‚¹ãƒˆã®ã¿ã¨ã—ã€ç•ªå·ã‚„å‰ç½®ãã€è§£èª¬ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""
    else:
        # éå»ã®å¯¾è©±ãŒãªã„æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        prompt = f"""
ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã‚’æ•´ç†ã™ã‚‹ã€å„ªç§€ãªã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ä»Šå›ã€Œ{topic}ã€ã¨ã„ã†ãƒ†ãƒ¼ãƒã‚’é¸ã³ã¾ã—ãŸã€‚
ã“ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ·±ãå†…çœã§ãã‚‹ã‚ˆã†ãªã€ã€Œã¯ã„ã€ã‹ã€Œã„ã„ãˆã€ã§ç­”ãˆã‚‰ã‚Œã‚‹æœ¬è³ªçš„ãªè³ªå•ã‚’5ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ç”Ÿæˆã™ã‚‹ã®ã¯è³ªå•ãƒªã‚¹ãƒˆã®ã¿ã¨ã—ã€ç•ªå·ã‚„å‰ç½®ãã€è§£èª¬ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""
    prompt = textwrap.dedent(prompt)
    flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
    return _call_gemini_with_schema(prompt, QUESTIONS_SCHEMA, model_name=flash_model).get("questions", [])

def generate_follow_up_questions(insights):
    """å¯¾è©±ã®è¦ç´„ã«åŸºã¥ã„ã¦ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—è³ªå•ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰"""
    prompt = f"""
ã‚ãªãŸã¯ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®åˆ†æçµæœã‚’ã•ã‚‰ã«æ·±ã‚ã‚‹ã€ã€Œã¯ã„ã€ã‹ã€Œã„ã„ãˆã€ã§ç­”ãˆã‚‰ã‚Œã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªè³ªå•ã‚’5ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
è³ªå•ä»¥å¤–ã®ä½™è¨ˆãªãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚

# åˆ†æçµæœ
{insights}
"""
    prompt = textwrap.dedent(prompt)
    flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
    try:
        result = _call_gemini_with_schema(prompt, QUESTIONS_SCHEMA, model_name=flash_model)
        return result.get("questions", []) if result else None
    except Exception as e:
        print(f"âŒ Failed to generate follow up questions: {e}")
        return None

def generate_summary_only(topic, swipes_text):
    prompt = SUMMARY_ONLY_PROMPT_TEMPLATE.format(topic=topic, swipes_text=swipes_text)
    flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
    try:
        return _call_gemini_with_schema(prompt, SUMMARY_SCHEMA, model_name=flash_model)
    except Exception as e:
        print(f"âŒ Failed to generate summary: {e}")
        return None

def generate_graph_data(all_insights_text):
    prompt = GRAPH_ANALYSIS_PROMPT_TEMPLATE + all_insights_text
    pro_model = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
    try:
        return _call_gemini_with_schema(prompt, GRAPH_SCHEMA, model_name=pro_model)
    except Exception as e:
        print(f"âŒ Failed to generate graph data: {e}")
        return None

def generate_chat_response(session_summary, chat_history, user_message, rag_context=""):
    history_str = "\n".join([f"{msg['author']}: {msg['text']}" for msg in chat_history])
    
    if rag_context:
        # RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
        prompt = f"""
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æã®å°‚é–€å®¶ã§ã‚ã‚Šã€å…±æ„ŸåŠ›ã¨æ´å¯ŸåŠ›ã«å„ªã‚ŒãŸã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã€Œã‚³ã‚³ãƒ­ã®åˆ†æå®˜ã€ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€è‡ªèº«ã®æ€è€ƒã‚’å¯è¦–åŒ–ã—ãŸã‚°ãƒ©ãƒ•ã‚’è¦‹ãªãŒã‚‰ã€ã‚ãªãŸã¨å¯¾è©±ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚
# ã‚ãªãŸã®å½¹å‰²
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®éå»ã®ä¼šè©±å±¥æ­´ã¨ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã®è¦ç´„ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ï¼‰ã‚’å¸¸ã«å‚ç…§ã—ã€æ–‡è„ˆã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã‚’æ·±ãå‚¾è´ã—ã€ã¾ãšã¯è‚¯å®šçš„ã«å—ã‘æ­¢ã‚ã¦å…±æ„Ÿã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚
- **ä»¥ä¸‹ã®å‚è€ƒæƒ…å ±ã‚’å…ƒã«**ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‡ªåˆ†ã§ã‚‚æ°—ã¥ã„ã¦ã„ãªã„å†…é¢ã‚’å„ªã—ãæŒ‡æ‘˜ã—ãŸã‚Šã€æ·±ã„å•ã„ã‚’æŠ•ã’ã‹ã‘ãŸã‚Šã—ã¦ã€è‡ªå·±ç†è§£ã‚’ä¿ƒã—ã¦ãã ã•ã„ã€‚
- æ¯å›ã®è¿”ä¿¡ã‚’è‡ªå·±ç´¹ä»‹ã‹ã‚‰å§‹ã‚ã‚‹ã®ã§ã¯ãªãã€ä¼šè©±ã®æµã‚Œã‚’è‡ªç„¶ã«å¼•ãç¶™ã„ã§ãã ã•ã„ã€‚
- **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åå‰ï¼ˆã€Œã€‡ã€‡ã•ã‚“ã€ãªã©ï¼‰ã¯çµ¶å¯¾ã«ä½¿ç”¨ã›ãšã€å¸¸ã«å¯¾è©±ç›¸æ‰‹ã«ç›´æ¥èªã‚Šã‹ã‘ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚**
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼
{session_summary}
# å‚è€ƒæƒ…å ±
{rag_context}
# ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
{history_str}
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»Šå›ã®ç™ºè¨€
{user_message}
ã‚ãªãŸã®å¿œç­”:
"""
    else:
        # RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒãªã„å ´åˆã¯ã€å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
        prompt = CHAT_PROMPT_TEMPLATE.format(session_summary=session_summary, chat_history=history_str, user_message=user_message)

    pro_model = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
    model = GenerativeModel(pro_model)
    return model.generate_content(prompt).text.strip()

def generate_topic_suggestions(insights_text: str):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®å¯¾è©±å±¥æ­´ã®ã‚µãƒãƒªãƒ¼ã«åŸºã¥ãã€æ–°ã—ã„å¯¾è©±ãƒˆãƒ”ãƒƒã‚¯ã‚’3ã¤ææ¡ˆã™ã‚‹"""
    prompt = f"""
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã®æ•´ç†ã‚’æ‰‹ä¼ã†ã€å„ªã‚ŒãŸã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®å¯¾è©±ã®ã‚µãƒãƒªãƒ¼ã€ã‚’èª­ã¿ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã«é–¢å¿ƒã‚’æŒã¡ãã†ãªæ–°ã—ã„å¯¾è©±ã®ãƒ†ãƒ¼ãƒã‚’3ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®å¯¾è©±ã®ã‚µãƒãƒªãƒ¼
{insights_text}

# æŒ‡ç¤º
- ææ¡ˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œã“ã‚Œã«ã¤ã„ã¦è©±ã—ã¦ã¿ãŸã„ï¼ã€ã¨æ€ãˆã‚‹ã‚ˆã†ãªã€å…·ä½“çš„ã§é­…åŠ›çš„ãªçŸ­ã„å•ã„ã‹ã‘ã®å½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚
- ææ¡ˆã¯3ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
- å¿…ãšã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
    pro_model = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
    try:
        result = _call_gemini_with_schema(prompt, TOPIC_SUGGESTION_SCHEMA, model_name=pro_model)
        return result.get("suggestions", []) if result else None
    except Exception as e:
        print(f"âŒ Failed to generate topic suggestions: {e}")
        return None


@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
def _extract_keywords_for_search(analysis_text: str) -> str:
    prompt = f"""
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å¿ƒç†åˆ†æãƒ¬ãƒãƒ¼ãƒˆå…¨ä½“ã‹ã‚‰ã€æœ€ã‚‚é‡è¦ã¨æ€ã‚ã‚Œã‚‹æ¦‚å¿µã‚„èª²é¡Œã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’5ã¤ä»¥å†…ã§æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯Vertex AI Searchã®æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚ä»–ã®æ–‡ã¯å«ã‚ãšã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã®ã¿ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

# åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
{analysis_text}

# å‡ºåŠ›ä¾‹
ä»•äº‹ã®ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼, äººé–“é–¢ä¿‚ã®æ‚©ã¿, è‡ªå·±è‚¯å®šæ„Ÿã®ä½ä¸‹, å°†æ¥ã¸ã®ä¸å®‰

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:
"""
    try:
        flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
        model = GenerativeModel(flash_model)
        print("--- Calling Gemini to extract search keywords ---")
        response = model.generate_content(prompt)
        keywords = response.text.strip()
        print(f"âœ… Extracted Keywords: {keywords}")
        return keywords
    except Exception as e:
        print(f"âŒ Failed to extract keywords: {e}")
        return ""

def _summarize_internal_context(context: str, keyword: str) -> str:
    """Summarizes past session records related to a specific keyword."""
    if not context or not keyword:
        return "ã“ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€ã“ã‚Œã¾ã§å…·ä½“çš„ãªãŠè©±ã¯ãªã‹ã£ãŸã‚ˆã†ã§ã™ã€‚"
    try:
        prompt = INTERNAL_CONTEXT_PROMPT_TEMPLATE.format(context=context, keyword=keyword)
        flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
        model = GenerativeModel(flash_model)
        print(f"--- Calling Gemini to summarize internal context for '{keyword}' ---")
        response = model.generate_content(prompt)
        summary = response.text.strip()
        print(f"âœ… Internal context summary: {summary}")
        return summary
    except Exception as e:
        print(f"âŒ Failed to summarize internal context: {e}")
        return "éå»ã®è¨˜éŒ²ã‚’è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"


# ===== RAG (Retrieval-Augmented Generation) Helper Functions =====

@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
def _get_embeddings(texts: list[str]) -> list[list[float]]:
    if not texts: return []
    model = TextEmbeddingModel.from_pretrained("text-multilingual-embedding-002")
    BATCH_SIZE = 15 
    all_embeddings = []
    print(f"--- RAG: Generating embeddings for {len(texts)} texts in batches of {BATCH_SIZE} ---")
    try:
        for i in range(0, len(texts), BATCH_SIZE):
            batch = texts[i:i + BATCH_SIZE]
            responses = model.get_embeddings(batch)
            for response in responses:
                all_embeddings.append(response.values)
            print(f"--- RAG: Processed embedding batch {i//BATCH_SIZE + 1}/{-(-len(texts) // BATCH_SIZE)} ---")
        return all_embeddings
    except Exception as e:
        print(f"âŒ RAG: An error occurred during embedding generation: {e}")
        traceback.print_exc()
        # â˜…â˜…â˜… ä¿®æ­£: ä¾‹å¤–ã‚’å†raiseã—ã¦retryã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ â˜…â˜…â˜…
        raise

def _get_url_cache_doc_ref(url: str):
    url_hash = hashlib.sha256(url.encode('utf-8')).hexdigest()
    return db_firestore.collection(RAG_CACHE_COLLECTION).document(url_hash)

def _get_cached_chunks_and_embeddings(url: str):
    try:
        doc_ref = _get_url_cache_doc_ref(url)
        doc = doc_ref.get()
        if not doc.exists:
            print(f"CACHE MISS: No cache found for URL: {url}")
            return None, None
        cache_data = doc.to_dict()
        cached_at = cache_data.get('cached_at')
        if isinstance(cached_at, datetime):
            if datetime.now(timezone.utc) - cached_at > timedelta(days=RAG_CACHE_TTL_DAYS):
                print(f"CACHE STALE: Cache for {url} is older than {RAG_CACHE_TTL_DAYS} days.")
                return None, None
        else:
             print(f"CACHE INVALID: Invalid 'cached_at' field for {url}.")
             return None, None
        
        chunks = cache_data.get('chunks')
        embeddings_from_db = cache_data.get('embeddings')
        
        if chunks and embeddings_from_db:
            embeddings = [item['vector'] for item in embeddings_from_db if 'vector' in item]
            if len(chunks) == len(embeddings):
                print(f"âœ… CACHE HIT: Found {len(chunks)} chunks for URL: {url}")
                return chunks, embeddings

        print(f"CACHE INVALID: Data mismatch for {url}. Re-fetching.")
        return None, None
    except Exception as e:
        print(f"âŒ Error getting cache for {url}: {e}")
        return None, None

def _set_cached_chunks_and_embeddings(url: str, chunks: list, embeddings: list):
    if not chunks or not embeddings: return
    try:
        doc_ref = _get_url_cache_doc_ref(url)
        transformed_embeddings = [{'vector': emb} for emb in embeddings]
        cache_data = {
            'url': url,
            'chunks': chunks,
            'embeddings': transformed_embeddings,
            'cached_at': firestore.SERVER_TIMESTAMP
        }
        doc_ref.set(cache_data)
        print(f"âœ… CACHE SET: Saved {len(chunks)} chunks for URL: {url}")
    except Exception as e:
        print(f"âŒ Error setting cache for {url}: {e}")
        traceback.print_exc()

def _generate_rag_based_advice(query: str, project_id: str, similar_cases_engine_id: str, suggestions_engine_id: str, rag_type: str = None):
    """
    RAG based on user analysis to generate advice, using a Firestore cache for embeddings.
    Returns a tuple of (advice_text, list_of_source_urls).
    """
    search_query = _extract_keywords_for_search(query)
    if not search_query:
        print("âš ï¸ RAG: Could not extract keywords. Using original query for search.")
        search_query = query[:512]
    
    all_found_urls = set()
    if rag_type == 'similar_cases':
        print("--- RAG: Searching for SIMILAR CASES ONLY ---")
        if similar_cases_engine_id:
            all_found_urls.update(_search_with_vertex_ai_search(project_id, "global", similar_cases_engine_id, search_query))
    elif rag_type == 'suggestions':
        print("--- RAG: Searching for SUGGESTIONS ONLY ---")
        if suggestions_engine_id:
            all_found_urls.update(_search_with_vertex_ai_search(project_id, "global", suggestions_engine_id, search_query))
    else: # Default behavior: search both
        print("--- RAG: Searching both similar cases and suggestions ---")
        if similar_cases_engine_id:
            all_found_urls.update(_search_with_vertex_ai_search(project_id, "global", similar_cases_engine_id, search_query))
        if suggestions_engine_id:
            all_found_urls.update(_search_with_vertex_ai_search(project_id, "global", suggestions_engine_id, search_query))

    if not all_found_urls:
        return "é–¢é€£ã™ã‚‹å¤–éƒ¨æƒ…å ±ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", []

    all_chunks, all_embeddings, urls_with_content = [], [], []
    urls_to_process = list(all_found_urls)[:5]

    for url in urls_to_process:
        cached_chunks, cached_embeddings = _get_cached_chunks_and_embeddings(url)
        if cached_chunks and cached_embeddings:
            all_chunks.extend(cached_chunks)
            all_embeddings.extend(cached_embeddings)
            urls_with_content.append(url)
        else:
            print(f"SCRAPING: No valid cache for {url}. Fetching content.")
            page_content = _scrape_text_from_url(url)
            if page_content:
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
                new_chunks_full = text_splitter.split_text(page_content)
                
                MAX_CHUNKS_PER_URL = 50  # 1ã¤ã®URLã‹ã‚‰å–å¾—ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã®ä¸Šé™
                new_chunks = new_chunks_full[:MAX_CHUNKS_PER_URL]

                if len(new_chunks_full) > MAX_CHUNKS_PER_URL:
                    print(f"âš ï¸ RAG: Content too long. Truncated chunks for {url} from {len(new_chunks_full)} to {len(new_chunks)}.")
                if new_chunks:
                    new_embeddings = _get_embeddings(new_chunks)
                    if new_embeddings and len(new_chunks) == len(new_embeddings):
                        all_chunks.extend(new_chunks)
                        all_embeddings.extend(new_embeddings)
                        urls_with_content.append(url)
                        threading.Thread(target=_set_cached_chunks_and_embeddings, args=(url, new_chunks, new_embeddings)).start()
                    else:
                        print(f"âš ï¸ RAG: Failed to generate embeddings for {url}. Skipping.")
    
    if not all_chunks:
        return "é–¢é€£ã™ã‚‹å¤–éƒ¨æƒ…å ±ã‚’è¦‹ã¤ã‘ã¾ã—ãŸãŒã€å†…å®¹ã‚’èª­ã¿å–ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", urls_to_process

    print(f"--- RAG: Finding relevant chunks from {len(all_chunks)} total chunks... ---")
    query_embedding_list = _get_embeddings([query])
    if not query_embedding_list:
        return "ã‚ãªãŸã®çŠ¶æ³ã‚’åˆ†æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚", urls_with_content
    
    query_embedding = np.array(query_embedding_list[0])
    
    similarities = []
    for i, emb in enumerate(all_embeddings):
        chunk_embedding = np.array(emb)
        dot_product = np.dot(chunk_embedding, query_embedding)
        norm_product = np.linalg.norm(chunk_embedding) * np.linalg.norm(query_embedding)
        similarity = dot_product / norm_product if norm_product != 0 else 0.0
        similarities.append((similarity, all_chunks[i]))
    
    similarities.sort(key=lambda x: x[0], reverse=True)
    relevant_chunks = [chunk for sim, chunk in similarities[:3]]

    if not relevant_chunks:
        return "é–¢é€£æƒ…å ±ã®ä¸­ã‹ã‚‰ã€ã‚ãªãŸã®çŠ¶æ³ã«ç‰¹ã«åˆè‡´ã™ã‚‹éƒ¨åˆ†ã‚’è¦‹ã¤ã‘å‡ºã™ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", urls_with_content

    print("--- RAG: Generating final advice with Gemini... ---")
    context_text = "\n---\n".join(relevant_chunks)

    if rag_type == 'similar_cases':
        prompt = f"""
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ‚©ã¿ã«å…±æ„Ÿã—ã€ä»–ã®äººã®ã‚±ãƒ¼ã‚¹ã‚’ç´¹ä»‹ã™ã‚‹èãä¸Šæ‰‹ãªå‹äººã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœã€ã¨ã€Œå‚è€ƒæƒ…å ±ï¼ˆä»–ã®äººã®æ‚©ã¿ã‚„ä½“é¨“è«‡ï¼‰ã€ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’åŠ±ã¾ã™ã‚ˆã†ãªå½¢ã§ã€å‚è€ƒæƒ…å ±ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

# æŒ‡ç¤º
- å…¨ä½“ã§200æ–‡å­—ç¨‹åº¦ã®ã€éå¸¸ã«ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªæ–‡ç« ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å®‰å¿ƒã•ã›ã€ä¸€äººã§ã¯ãªã„ã¨æ„Ÿã˜ã•ã›ã‚‹ã‚ˆã†ãªã€æ¸©ã‹ãå…±æ„Ÿçš„ãªãƒˆãƒ¼ãƒ³ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
- ã€Œä¼¼ãŸã‚ˆã†ãªã“ã¨ã§æ‚©ã‚“ã§ã„ã‚‹æ–¹ã‚‚ã„ã‚‹ã‚ˆã†ã§ã™ã€‚ã€ã¨ã„ã£ãŸå‰ç½®ãã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„ã€‚
- æœ€å¾Œã«ã€å‚è€ƒã«ã—ãŸæƒ…å ±æºã®URLã‚’ `[å‚è€ƒæƒ…å ±]` ã¨ã—ã¦ç®‡æ¡æ›¸ãã§å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚

# ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœ
{query}

# å‚è€ƒæƒ…å ± (ä»–ã®äººã®æ‚©ã¿ã‚„ä½“é¨“è«‡)
---
{context_text}
---

# ã‚ãªãŸã®å¿œç­”:
"""
    else: # 'suggestions' or default
        prompt = f"""
ã‚ãªãŸã¯ã€å®¢è¦³çš„ã§ä¿¡é ¼ã§ãã‚‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã™ã‚‹ãƒ—ãƒ­ã®ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœã€ã¨ã€Œå‚è€ƒæƒ…å ±ï¼ˆå°‚é–€æ©Ÿé–¢ã«ã‚ˆã‚‹å…·ä½“çš„ãªå¯¾ç­–ï¼‰ã€ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã®ä¸€æ­©ã‚’è¸ã¿å‡ºã™ãŸã‚ã®ã€å…·ä½“çš„ã§å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

# æŒ‡ç¤º
- å…¨ä½“ã§300æ–‡å­—ç¨‹åº¦ã®ã€ç°¡æ½”ã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ã„æ–‡ç« ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çŠ¶æ³ã‚’æ•´ç†ã—ã€å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç®‡æ¡æ›¸ãã§2ã€œ3ç‚¹ææ¡ˆã™ã‚‹æ§‹æˆã«ã—ã¦ãã ã•ã„ã€‚
- ã€Œã‚ãªãŸã®çŠ¶æ³ã‚’å®¢è¦³çš„ã«è¦‹ã‚‹ã¨ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€ã“ã®ã‚ˆã†ãªã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚ã€ã¨ã„ã£ãŸã€å°‚é–€å®¶ã¨ã—ã¦ã®å†·é™ãªãƒˆãƒ¼ãƒ³ã§å§‹ã‚ã¦ãã ã•ã„ã€‚
- æœ€å¾Œã«ã€å‚è€ƒã«ã—ãŸæƒ…å ±æºã®URLã‚’ `[å‚è€ƒæƒ…å ±]` ã¨ã—ã¦ç®‡æ¡æ›¸ãã§å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚

# ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœ
{query}

# å‚è€ƒæƒ…å ± (å°‚é–€æ©Ÿé–¢ã«ã‚ˆã‚‹å…·ä½“çš„ãªå¯¾ç­–)
---
{context_text}
---

# ã‚ãªãŸã®å¿œç­”:
"""

    pro_model_name = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
    model = GenerativeModel(pro_model_name)
    advice = model.generate_content(prompt, generation_config=GenerationConfig(temperature=0.7)).text
    
    return advice, list(dict.fromkeys(urls_with_content))

def _search_with_vertex_ai_search(project_id: str, location: str, engine_id: str, query: str) -> list[str]:
    if not engine_id:
        print(f"âŒ RAG: Engine ID '{engine_id}' is not configured.")
        return []
    client = discoveryengine.SearchServiceClient()
    serving_config = (
        f"projects/{project_id}/locations/{location}/collections/default_collection/"
        f"engines/{engine_id}/servingConfigs/default_config"
    )
    request = discoveryengine.SearchRequest(serving_config=serving_config, query=query, page_size=5)
    try:
        response = client.search(request)
        urls = [r.document.derived_struct_data.get('link') for r in response.results if r.document.derived_struct_data.get('link')]
        print(f"âœ… RAG: Found URLs from Vertex AI Search: {urls}")
        return urls
    except Exception as e:
        print(f"âŒ RAG: Vertex AI Search failed for engine '{engine_id}': {e}")
        traceback.print_exc()
        return []

def _scrape_text_from_url(url: str) -> str:
    # â˜… è¿½åŠ : ç‰¹å®šã®SNSãƒ‰ãƒ¡ã‚¤ãƒ³ã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
    forbidden_domains = ['twitter.com', 'x.com', 'facebook.com', 'instagram.com', 'detail.chiebukuro.yahoo.co.jp']
    # URLã«ç¦æ­¢ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã„ãšã‚Œã‹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if any(domain in url for domain in forbidden_domains):
        print(f"âš ï¸ RAG: Skipping scraping for forbidden domain: {url}")
        return ""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        response = requests.get(url, timeout=10, headers=headers)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        for element in soup(["script", "style", "header", "footer", "nav", "aside"]):
            element.decompose()
        return soup.get_text(separator=' ', strip=True)
    except requests.exceptions.RequestException as e:
        print(f"âŒ RAG: Error fetching URL {url}: {e}")
        return ""

# --- ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç† ---
def _prefetch_questions_and_save(session_id: str, user_id: str, insights_md: str, current_turn: int, max_turns: int):
    print(f"--- Triggered question prefetch for user: {user_id}, session: {session_id}, next_turn: {current_turn + 1} ---")
    if current_turn >= max_turns:
        print("Max turns reached. Skipping question prefetch.")
        return
    try:
        questions = generate_follow_up_questions(insights=insights_md)
        if questions:
            prefetched_ref = db_firestore.collection('sessions').document(session_id).collection('prefetched_questions').document(str(current_turn + 1))
            prefetched_ref.set({'questions': questions})
            print(f"âœ… Prefetched and saved questions for turn {current_turn + 1}")
    except Exception as e:
        print(f"âŒ Error during question prefetch for session {session_id}: {e}")

def _update_graph_cache(user_id: str):
    print(f"--- Triggered background graph update for user: {user_id} ---")
    try:
        _get_graph_from_cache_or_generate(user_id, force_regenerate=True)
        print(f"âœ… Background graph update for user {user_id} completed.")
    except Exception as e:
        print(f"âŒ Error during background graph update for user {user_id}: {e}")


# ===== èªè¨¼ãƒ»èªå¯ =====
def _verify_token(request):
    auth_header = request.headers.get('Authorization')
    if not auth_header:
        # å¤±æ•—æ™‚ã¯Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã‚‹
        return jsonify({"error": "Authorization header is missing"}), 401

    try:
        id_token = auth_header.split('Bearer ')[1]
        # æˆåŠŸæ™‚ã¯dictãŒè¿”ã‚‹
        decoded_token = auth.verify_id_token(id_token)
        return decoded_token
    except (IndexError, auth.InvalidIdTokenError) as e:
        print(f"Token validation failed: {e}")
        # å¤±æ•—æ™‚ã¯Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã‚‹
        return jsonify({"error": "Invalid or expired token"}), 401
    except Exception as e:
        print(f"An unexpected error occurred during token verification: {e}")
        # session_ref ã¯ã“ã®ã‚¹ã‚³ãƒ¼ãƒ—ã«å­˜åœ¨ã—ãªã„ãŸã‚ã€ã“ã®è¡Œã‚’å‰Šé™¤ã—ã¾ã™ã€‚
        return jsonify({"error": "Could not verify token"}), 500

# --- Cloud Tasks ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def _create_cloud_task(payload: dict, target_uri: str):
    """Cloud Tasksã«HTTPã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã™ã‚‹ã€‚"""
    # ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„
    if not tasks_client:
        print("âš ï¸ Cloud Tasks is not configured. Skipping task creation.")
        return

    parent = tasks_client.queue_path(project_id, GCP_TASK_QUEUE_LOCATION, GCP_TASK_QUEUE)

    # ã‚¿ã‚¹ã‚¯ã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURLã‚’è¨­å®š
    task = {
        "http_request": {
            "http_method": tasks_v2.HttpMethod.POST,
            "url": f"{SERVICE_URL.rstrip('/')}{target_uri}",
            "headers": {"Content-type": "application/json"},
            "body": json.dumps(payload).encode(),
            # Cloud Runã®IAMèªè¨¼ã‚’é€šéã™ã‚‹ãŸã‚ã«OIDCãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹
            "oidc_token": {
                 "service_account_email": GCP_TASK_SA_EMAIL,
            }
        }
    }

    try:
        response = tasks_client.create_task(parent=parent, task=task)
        print(f"âœ… Created Cloud Task for {target_uri}. Task name: {response.name}")
    except Exception as e:
        print(f"âŒ Failed to create Cloud Task for {target_uri}: {e}")
        traceback.print_exc()



# ===== APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ =====
@api_bp.route('/session/start', methods=['POST'])
def start_session():
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record

    data = request.get_json()
    if not data or 'topic' not in data:
        return jsonify({"error": "Topic is required"}), 400
    
    topic = data['topic']
    user_id = user_record['uid']
    
    try:
        # (â˜…ä¿®æ­£) ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¿å­˜å…ˆã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚µãƒ–ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«å¤‰æ›´
        session_doc_ref = db_firestore.collection('users').document(user_id).collection('sessions').document()
        
        # (â˜…ä¿®æ­£) status ã¨ created_at ã‚’è¿½åŠ 
        session_doc_ref.set({
            'user_id': user_id,
            'topic': topic,
            'created_at': firestore.SERVER_TIMESTAMP, # æ—¥ä»˜é †ã§ä¸¦ã³æ›¿ãˆã‚‹ãŸã‚ã«å¿…è¦
            'status': 'processing', # statusã‚’ 'processing' ã§åˆæœŸåŒ–
            'turn': 1,
        })
        # Geminiã§æœ€åˆã®è³ªå•ã‚’ç”Ÿæˆ
        questions = generate_initial_questions(topic, user_id)

        # â˜…â˜…â˜… ä¿®æ­£ã¯ã“ã“ã‹ã‚‰ã§ã™ â˜…â˜…â˜…
        # è³ªå•ãŒç”Ÿæˆã•ã‚Œãªã‹ã£ãŸå ´åˆã®ãƒã‚§ãƒƒã‚¯ã‚’ã€ã™ãã«å®Ÿè¡Œã™ã‚‹ã‚ˆã†ã«ç§»å‹•ã—ã¾ã™
        if not questions:
            print("Failed to generate initial questions.")
            return jsonify({"error": "Failed to generate initial questions"}), 500

        # ãƒãƒƒãƒæ›¸ãè¾¼ã¿ã‚’ä½¿ã£ã¦è³ªå•ã‚’ä¿å­˜ã—ã€åŒæ™‚ã«ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ
        batch = db_firestore.batch()
        
        questions_for_response = []
        for question in questions:
            # è³ªå•ç”¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‚ç…§ã‚’å…ˆã«ä½œæˆã—ã¦IDã‚’å–å¾—
            question_doc_ref = session_doc_ref.collection('questions').document()
            
            # ãƒ•ãƒ­ãƒ³ãƒˆã«è¿”ã™ãƒªã‚¹ãƒˆã«ã¯ã€ç”Ÿæˆã—ãŸIDã‚’ `question_id` ã¨ã—ã¦è¿½åŠ 
            questions_for_response.append({
                "question_text": question['question_text'],
                "question_id": question_doc_ref.id
            })
            
            # Firestoreã«ã¯ã€è³ªå•ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’ãƒãƒƒãƒã«è¿½åŠ 
            batch.set(question_doc_ref, { "question_text": question['question_text'] })

        batch.commit()

        return jsonify({
            'session_id': session_doc_ref.id,
            'questions': questions_for_response
        }), 200

    except Exception as e:
        print(f"Error starting session: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to start session"}), 500

@api_bp.route('/session/<string:session_id>/swipe', methods=['POST'])
def record_swipe(session_id):
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record
    user_id = user_record['uid']

    data = request.get_json()
    required_fields = ['question_id', 'answer', 'hesitation_time', 'speed', 'turn']
    if not data or not all(field in data for field in required_fields):
        return jsonify({"error": "Missing required fields in request"}), 400
    
    try:
        # (â˜…ä¿®æ­£) ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å‚ç…§ãƒ‘ã‚¹ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚µãƒ–ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«å¤‰æ›´
        session_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)
        swipe_ref = session_ref.collection('swipes').document()
        
        swipe_ref.set({
            'user_id': user_record['uid'],
            'question_id': data['question_id'],
            'answer': data['answer'],
            'hesitation_time': data['hesitation_time'],
            'swipe_speed': data['speed'],
            'turn': data['turn'],
            'timestamp': firestore.SERVER_TIMESTAMP
        })

        return jsonify({"status": "success"}), 200

    except Exception as e:
        print(f"Error recording swipe: {e}")
        return jsonify({"error": "Failed to record swipe"}), 500


@api_bp.route('/session/<string:session_id>/summary', methods=['POST'])
def post_summary(session_id):
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¦ç´„ã‚’ç”Ÿæˆãƒ»ä¿å­˜ã—ã€çµæœã‚’è¿”ã™"""
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record
    user_id = user_record['uid']

    session_ref = None  # å¤‰æ•°ã‚’Noneã§åˆæœŸåŒ–
    try:
        session_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)
        session_snapshot = session_ref.get()

        if not session_snapshot.exists:
            return jsonify({"error": "Session not found"}), 404

        session_data = session_snapshot.to_dict()
        topic = session_data.get('topic', 'æŒ‡å®šãªã—')
        current_turn = session_data.get('turn', 1) 
        swipes_ref = session_ref.collection('swipes').order_by('timestamp')
        swipes_docs = list(swipes_ref.stream())

        if not swipes_docs:
            print(f"No swipes found for session {session_id}, returning empty summary.")
            session_ref.update({'status': 'completed', 'title': 'å¯¾è©±ã®è¨˜éŒ²ãŒã‚ã‚Šã¾ã›ã‚“'})
            return jsonify({
                "title": "å¯¾è©±ã®è¨˜éŒ²ãŒã‚ã‚Šã¾ã›ã‚“",
                "insights": "ä»Šå›ã¯å¯¾è©±ã®è¨˜éŒ²ãŒãªã‹ã£ãŸãŸã‚ã€è¦ç´„ã®ä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚",
                "turn": session_data.get('turn', 1),
                "max_turns": MAX_TURNS
            }), 200

        questions_ref = session_ref.collection('questions')
        questions_docs = {q.id: q.to_dict() for q in questions_ref.stream()}
        
        swipes_text_parts = []
        for s_doc in swipes_docs:
            s = s_doc.to_dict()
            q_id = s.get('question_id')
            q_text = questions_docs.get(q_id, {}).get('question_text', 'ä¸æ˜ãªè³ªå•')
            answer_text = 'ã¯ã„' if s.get('answer') else 'ã„ã„ãˆ'
            swipes_text_parts.append(f"- {q_text}: {answer_text}")
            
        swipes_text = "\n".join(swipes_text_parts)
        
        summary_data = generate_summary_only(topic, swipes_text)

        update_data = {
            'status': 'completed',
            'title': summary_data.get('title'),
            'latest_insights': summary_data.get('insights'),
            'updated_at': firestore.SERVER_TIMESTAMP
        }
        session_ref.update(update_data)

        summary_with_turn = summary_data.copy()
        summary_with_turn['turn'] = current_turn
        summary_ref = session_ref.collection('summaries').document(f'turn_{current_turn}')
        summary_ref.set(summary_with_turn)

        response_data = summary_data.copy()
        response_data['turn'] = session_data.get('turn', 1)
        response_data['max_turns'] = MAX_TURNS

        insights_text = summary_data.get('insights', '')
        current_turn = response_data['turn']

        if current_turn < MAX_TURNS:
            prefetch_payload = {
                'session_id': session_id,
                'user_id': user_id,
                'insights_md': insights_text,
                'current_turn': current_turn
            }
            _create_cloud_task(prefetch_payload, '/api/tasks/prefetch_questions')

        graph_payload = {'user_id': user_id}
        _create_cloud_task(graph_payload, '/api/tasks/update_graph')
        
        return jsonify(response_data), 200
    except Exception as e:
        print(f"âŒ Error in post_summary for session {session_id}: {e}")
        traceback.print_exc()
        if session_ref:
            session_ref.update({'status': 'error', 'error_message': str(e)})
        return jsonify({"error": "Failed to generate summary"}), 500



@api_bp.route('/session/<string:session_id>/continue', methods=['POST'])
def continue_session(session_id):
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record

    user_id = user_record['uid']

    try:
        session_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)

        @firestore.transactional
        def update_turn(transaction, ref):
            snapshot = ref.get(transaction=transaction)
            if not snapshot.exists:
                raise Exception("Session not found")
            
            current_turn = snapshot.to_dict().get('turn', 1)
            new_turn = current_turn + 1
            
            if new_turn > MAX_TURNS:
                 return None

            transaction.update(ref, {
                'turn': new_turn,
                'status': 'processing', # â˜… çŠ¶æ…‹ã‚’ã€Œé€²è¡Œä¸­ã€ã«æˆ»ã™
                'last_updated': firestore.SERVER_TIMESTAMP
            })
            return new_turn

        transaction = db_firestore.transaction()
        new_turn = update_turn(transaction, session_ref)

        if new_turn is None:
            return jsonify({"error": "Maximum turns reached for this session."}), 400
        
        prefetched_ref = session_ref.collection('prefetched_questions').document(str(new_turn))
        prefetched_doc = prefetched_ref.get()

        generated_questions = []
        if prefetched_doc.exists:
            print(f"âœ… Using prefetched questions for turn {new_turn}")
            generated_questions = prefetched_doc.to_dict().get('questions', [])
            prefetched_ref.delete()
        else:
            print(f"âš ï¸ No prefetched questions found for turn {new_turn}. Generating now...")
            latest_summary_query = session_ref.collection('summaries').order_by('turn', direction=firestore.Query.DESCENDING).limit(1)
            latest_summary_docs = list(latest_summary_query.stream())
            if not latest_summary_docs:
                 return jsonify({"error": "Summary not found to generate follow-up questions"}), 404
            
            insights = latest_summary_docs[0].to_dict().get('insights', '')
            generated_questions = generate_follow_up_questions(insights)

        # â˜…â˜…â˜… ã“ã“ã‹ã‚‰ãŒä»Šå›ã®ä¿®æ­£ã®æ ¸å¿ƒéƒ¨åˆ†ã§ã™ â˜…â˜…â˜…
        # 1. ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹
        batch = db_firestore.batch()
        # 2. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«è¿”ã™ãŸã‚ã®ã€IDä»˜ãè³ªå•ãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–
        questions_with_ids = []

        # 3. ç”Ÿæˆã•ã‚ŒãŸè³ªå•ã‚’ãƒ«ãƒ¼ãƒ—å‡¦ç†
        for q in generated_questions:
            # a. æ–°ã—ã„è³ªå•ã®ãŸã‚ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‚ç…§ã‚’ä½œæˆï¼ˆã“ã“ã§IDãŒè‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ï¼‰
            q_ref = session_ref.collection('questions').document()
            # b. ãƒãƒƒãƒã«ã€Œè³ªå•ãƒ†ã‚­ã‚¹ãƒˆã‚’DBã«ä¿å­˜ã™ã‚‹ã€å‡¦ç†ã‚’è¿½åŠ 
            batch.set(q_ref, {"question_text": q['question_text']})
            # c. ãƒ•ãƒ­ãƒ³ãƒˆã«è¿”ã™ãƒªã‚¹ãƒˆã«ã€ã€ŒIDã€ã¨ã€Œè³ªå•ãƒ†ã‚­ã‚¹ãƒˆã€ã‚’è¿½åŠ 
            questions_with_ids.append({
                "question_id": q_ref.id,
                "question_text": q['question_text']
            })
        
        # 4. ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œã—ã€ã™ã¹ã¦ã®è³ªå•ã‚’DBã«ä¸€æ‹¬ä¿å­˜
        batch.commit()
        # â˜…â˜…â˜… ã“ã“ã¾ã§ãŒä¿®æ­£ã®æ ¸å¿ƒéƒ¨åˆ†ã§ã™ â˜…â˜…â˜…

        # 5. DBä¿å­˜å¾Œã®IDä»˜ãè³ªå•ãƒªã‚¹ãƒˆã‚’ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«è¿”ã™
        return jsonify({'questions': questions_with_ids, 'turn': new_turn}), 200

    except Exception as e:
        print(f"Error continuing session: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to continue session"}), 500

@api_bp.route('/session/topic_suggestions', methods=['GET'])
def get_topic_suggestion():
    """éå»ã®å¯¾è©±å±¥æ­´ã«åŸºã¥ã„ã¦ã€æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’ææ¡ˆã™ã‚‹"""
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record
    user_id = user_record['uid']

    try:
        all_insights_text = _get_all_insights_as_text(user_id)

        if not all_insights_text:
            print(f"No past insights found for user {user_id}. Returning empty suggestions.")
            # éå»ã®å¯¾è©±ãŒãªã„å ´åˆã¯ã€ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            return jsonify({"suggestions": []}), 200

        suggestions = generate_topic_suggestions(all_insights_text)

        print(f"âœ… Generated {len(suggestions)} topic suggestions for user {user_id}.")
        return jsonify({"suggestions": suggestions}), 200

    except Exception as e:
        print(f"âŒ Error in get_topic_suggestion: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to get topic suggestions"}), 500

@api_bp.route('/analysis/summary', methods=['GET'])
def get_analysis_summary():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¯¾è©±å±¥æ­´ã®çµ±è¨ˆæƒ…å ±ã‚’è¿”ã™"""
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record
    user_id = user_record['uid']

    try:
        # 'completed'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ã‚’å–å¾—
        sessions_ref = db_firestore.collection('users').document(user_id).collection('sessions').where('status', '==', 'completed').stream()
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯ã‚’ãƒªã‚¹ãƒˆã«æŠ½å‡º
        topics = [
            session.to_dict().get('topic')
            for session in sessions_ref
            if session.to_dict().get('topic')
        ]

        if not topics:
            return jsonify({
                "total_sessions": 0,
                "topic_counts": [], # â˜… å¤‰æ›´: top_topicsã‹ã‚‰å¤‰æ›´
            }), 200

        # ãƒˆãƒ”ãƒƒã‚¯ã”ã¨ã®å›æ•°ã‚’é›†è¨ˆ
        topic_counts = Counter(topics)
        total_sessions = len(topics)

        # â˜… å¤‰æ›´: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§ä½¿ã„ã‚„ã™ã„ã‚ˆã†ã«ã€å…¨ãƒˆãƒ”ãƒƒã‚¯ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        # [{"topic": "ãƒˆãƒ”ãƒƒã‚¯å", "count": å›æ•°}, ...] ã®å½¢å¼
        topic_counts_list = [
            {"topic": item, "count": count}
            for item, count in topic_counts.items()
        ]

        # â˜… å¤‰æ›´: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚­ãƒ¼ã‚’ `topic_counts` ã«çµ±ä¸€
        response_data = {
            "total_sessions": total_sessions,
            "topic_counts": topic_counts_list,
        }
        
        print(f"âœ… Generated analysis summary for user {user_id}.")
        return jsonify(response_data), 200

    except Exception as e:
        print(f"âŒ Error in get_analysis_summary: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to get analysis summary"}), 500

@api_bp.route('/analysis/book_recommendations', methods=['GET'])
def get_book_recommendations():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã®å‚¾å‘ã«åŸºã¥ãã€ãŠã™ã™ã‚ã®æ›¸ç±ã‚’è¿”ã™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆï¼‰"""
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record
    user_id = user_record['uid']

    if not GOOGLE_BOOKS_API_KEY:
        print("âŒ Google Books API key is not configured.")
        return jsonify({"error": "Book recommendation service is not configured."}), 500

    try:
        cache_ref = db_firestore.collection('recommendation_cache').document(user_id)
        cache_doc = cache_ref.get()

        if cache_doc.exists:
            cached_data = cache_doc.to_dict()
            print(f"âœ… Returning cached book recommendations for user: {user_id}")
            return jsonify(cached_data.get("recommendations", [])), 200
        
        print(f"âš ï¸ No cached recommendations found for user {user_id}. Returning empty list for now.")
        return jsonify([]), 200

    except Exception as e:
        print(f"âŒ Error in get_book_recommendations: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to get book recommendations"}), 500

def search_books_from_api(keyword: str, api_key: str):
    """Google Books APIã‚’å©ã„ã¦æ›¸ç±æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
    books_api_url = f"https://www.googleapis.com/books/v1/volumes?q={urllib.parse.quote_plus(keyword)}&key={api_key}&langRestrict=ja&maxResults=5&printType=books&orderBy=relevance"
    books_found = []
    try:
        print(f"--- Calling Google Books API with keyword: {keyword} ---")
        response = requests.get(books_api_url, timeout=10)
        response.raise_for_status()
        search_results = response.json()
        
        if 'items' in search_results:
            for item in search_results.get('items', []):
                volume_info = item.get('volumeInfo', {})
                title = volume_info.get('title')
                authors = volume_info.get('authors', ['è‘—è€…ä¸æ˜'])
                book_id = item.get('id')
                if title and book_id:
                    books_found.append({
                        "id": book_id,
                        "title": title,
                        "author": ", ".join(authors)
                    })
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ Warning: Google Books API call failed for keyword '{keyword}': {e}")
    return books_found

def _generate_book_recommendations(insights_text: str, api_key: str):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã‚µãƒãƒªãƒ¼ã«åŸºã¥ãã€Google Books APIã¨Geminiã‚’é€£æºã•ã›ã¦æ›¸ç±ã‚’æ¨è–¦ã™ã‚‹ (å …ç‰¢ç‰ˆ)"""
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ‚©ã¿ã‹ã‚‰æ›¸ç±æ¤œç´¢ç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹
    keyword_extraction_prompt = f"""
ä»¥ä¸‹ã®ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã‚µãƒãƒªãƒ¼ã€ã‚’åˆ†æã—ã€ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ‚©ã¿ã‚’è§£æ±ºã™ã‚‹ã®ã«å½¹ç«‹ã¤æ›¸ç±ã‚’æ¢ã™ãŸã‚ã®ã€æœ€ã‚‚åŠ¹æœçš„ãªæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’3ã¤ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã‚µãƒãƒªãƒ¼
{insights_text}

# å‡ºåŠ›ä¾‹
ä»•äº‹è¡“, äººé–“é–¢ä¿‚ã®æ‚©ã¿, ãƒã‚¤ãƒ³ãƒ‰ãƒ•ãƒ«ãƒã‚¹

# æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:
"""
    try:
        flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
        print("--- Calling Gemini to extract book search keywords ---")
        
        # â˜… ä¿®æ­£: _call_gemini_with_schema ã‚’ä½¿ã£ã¦JSONå‡ºåŠ›ã‚’å¼·åˆ¶ã™ã‚‹
        keywords_dict = _call_gemini_with_schema(keyword_extraction_prompt, KEYWORDS_SCHEMA, flash_model)
        keywords = keywords_dict.get("keywords", [])
        
        print(f"âœ… Extracted book search keywords: {keywords}")
    except Exception as e:
        print(f"âŒ Failed to extract book search keywords: {e}")
        return {"recommendations": []}

    if not keywords:
        return {"recommendations": []}
        
    # ã‚¹ãƒ†ãƒƒãƒ—2: Google Books APIã§æ›¸ç±æƒ…å ±ã‚’æ¤œç´¢ã—ã€é‡è¤‡ã‚’é™¤ã„ãŸãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹
    all_books_info = []
    unique_book_ids = set()

    for keyword in keywords:
        # æ–°ã—ã„ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’å‘¼ã³å‡ºã™
        found_books = search_books_from_api(keyword, api_key)
        for book in found_books:
            if book['id'] not in unique_book_ids:
                all_books_info.append({"title": book["title"], "author": book["author"]})
                unique_book_ids.add(book['id'])

    if not all_books_info:
        print("No books found from Google Books API across all keywords.")
        return {"recommendations": []}
    
    selected_books = all_books_info[:5]
    print(f"âœ… Found {len(selected_books)} unique books to process.")

    # ã‚¹ãƒ†ãƒƒãƒ—3: å„æ›¸ç±ã«ã¤ã„ã¦ã€Geminiã«æ¨è–¦ç†ç”±ã®ã¿ã‚’ç”Ÿæˆã•ã›ã‚‹
    final_recommendations = []
    reason_generation_prompt_template = """
ã‚ãªãŸã¯ã€åˆ©ç”¨è€…ã®æ‚©ã¿ã«å¯„ã‚Šæ·»ã†å„ªç§€ãªå¸æ›¸ã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã‚µãƒãƒªãƒ¼ã€ã¨ã€Œæ›¸ç±æƒ…å ±ã€ã‚’å…ƒã«ã€ã“ã®æœ¬ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ãªãœãŠã™ã™ã‚ãªã®ã‹ã€å…·ä½“çš„ãªæ¨è–¦ç†ç”±ã‚’100æ–‡å­—ç¨‹åº¦ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚ç†ç”±ä»¥å¤–ã®ä½™è¨ˆãªæ–‡ç« ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã‚µãƒãƒªãƒ¼
{insights}

# æ›¸ç±æƒ…å ±
- æ›¸ç±å: {title}
- è‘—è€…: {author}

# æ¨è–¦ç†ç”±:
"""
    for book in selected_books:
        try:
            # â˜…â˜…â˜… ä¿®æ­£: Ollama/Gemmaã®å‡¦ç†ã‚’å®Œå…¨ã«å‰Šé™¤ã—ã€Geminiã®å‡¦ç†ã«ä¸€æœ¬åŒ– â˜…â˜…â˜…
            print(f"--- Calling Vertex AI(Gemini) for book: {book['title']} ---")
            prompt = reason_generation_prompt_template.format(insights=insights_text, title=book["title"], author=book["author"])
            flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-2.5-flash-preview-05-20')
            model = GenerativeModel(flash_model)
            response = model.generate_content(prompt)
            reason = response.text.strip()
            print(f"âœ… Generated reason from Gemini: {reason[:50]}...")

            if not reason:
                print(f"âš ï¸ Could not generate reason for book '{book['title']}'. Skipping.")
                continue

            search_query = f"{book['title']} {book['author']}"
            search_url = f"https://www.google.com/search?q={urllib.parse.quote_plus(search_query)}"

            final_recommendations.append({
                "title": book["title"],
                "author": book["author"],
                "reason": reason,
                "search_url": search_url
            })
            
            if len(final_recommendations) >= 3:
                break
        
        except Exception as e:
            print(f"âš ï¸ Failed to process book '{book['title']}': {e}")
            continue

    return {"recommendations": final_recommendations}

def _get_all_insights_as_text(user_id: str) -> str:
    """æŒ‡å®šã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ã‚’ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦çµåˆã™ã‚‹"""
    print(f"--- Fetching all session insights for user: {user_id} ---")
    all_insights_text = ""
    try:
        # (â˜…ä¿®æ­£) ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å‚ç…§ãƒ‘ã‚¹ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚µãƒ–ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«å¤‰æ›´
        sessions_ref = db_firestore.collection('users').document(user_id).collection('sessions').where('status', '==', 'completed').order_by('created_at', direction=firestore.Query.DESCENDING).limit(10)
        sessions_docs = sessions_ref.stream()

        for session in sessions_docs:
            session_dict = session.to_dict()
            # (â˜…ä¿®æ­£) created_at, topic, title, latest_insights ã‚’ç›´æ¥å–å¾—
            session_date = session_dict.get("created_at").strftime('%Y-%m-%d') if session_dict.get("created_at") else "ä¸æ˜ãªæ—¥ä»˜"
            session_topic = session_dict.get("topic", "ä¸æ˜ãªãƒˆãƒ”ãƒƒã‚¯")
            title = session_dict.get('title', 'ç„¡é¡Œ')
            insights = session_dict.get('latest_insights', 'åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚')
            
            summary_text_parts = [
                f"## ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ² ({session_date} - {session_topic})",
                f"### {title}\n{insights}"
            ]
            all_insights_text += "\n\n" + "\n".join(summary_text_parts)

        print(f"âœ… Found and compiled insights from past sessions.")
        return all_insights_text.strip()
    except Exception as e:
        print(f"âŒ Error fetching insights for user {user_id}: {e}")
        return ""

@api_bp.route('/analysis/graph', methods=['GET'])
def get_analysis_graph():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´ã‹ã‚‰çµ±åˆåˆ†æã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã¾ãŸã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
    user_record = _verify_token(request)
    # â˜…â˜…â˜… ä¿®æ­£: èªè¨¼æˆåŠŸæ™‚ã¯dictå‹ã€å¤±æ•—æ™‚ã¯Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã‚‹ãŸã‚ã€dictå‹ã‹ã©ã†ã‹ã§åˆ¤å®šã™ã‚‹ â˜…â˜…â˜…
    if not isinstance(user_record, dict):
        return user_record
    
    user_id = user_record['uid']
    try:
        graph_data = _get_graph_from_cache_or_generate(user_id)
        if graph_data:
            return jsonify(graph_data), 200
        else:
            return jsonify({"error": "No data available to generate graph"}), 404
    except Exception as e:
        print(f"âŒ Error in get_analysis_graph: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to get analysis graph"}), 500


def _get_graph_from_cache_or_generate(user_id: str, force_regenerate: bool = False):
    """
    Firestoreã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã€‚
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã‚„force_regenerate=Trueã®å ´åˆã¯ã€æ–°ãŸã«ç”Ÿæˆã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã™ã‚‹ã€‚
    """
    cache_ref = db_firestore.collection('analysis_cache').document(user_id)
    
    if not force_regenerate:
        cache_doc = cache_ref.get()
        if cache_doc.exists:
            cached_data = cache_doc.to_dict()
            # 24æ™‚é–“ä»¥å†…ã§ã‚ã‚Œã°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è¿”ã™
            if datetime.now(timezone.utc) - cached_data.get('timestamp', datetime.min.replace(tzinfo=timezone.utc)) < timedelta(hours=24):
                print(f"âœ… Returning cached graph data for user: {user_id}")
                return cached_data['graph_data']

    print(f"--- Generating new graph data for user: {user_id} (force_regenerate={force_regenerate}) ---")
    all_insights_text = _get_all_insights_as_text(user_id)
    if not all_insights_text:
        return None

    graph_data = generate_graph_data(all_insights_text)

    # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã€ã¾ãŸã¯ãƒãƒ¼ãƒ‰ãŒãªã„å ´åˆã¯ã“ã“ã§çµ‚äº†
    if not graph_data or not graph_data.get('nodes'):
        print(f"No nodes found in graph data for user: {user_id}. Skipping embedding generation.")
        # æ–°ã—ã„ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿(ç©ºã®å¯èƒ½æ€§ã‚ã‚Š)ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        cache_ref.set({
            'graph_data': graph_data,
            'timestamp': firestore.SERVER_TIMESTAMP,
            'user_id': user_id
        })
        return graph_data

    try:
        print(f"--- Generating and upserting node embeddings for user: {user_id} ---")

        # 1. ã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒãƒ¼ãƒ‰ã®ãƒ†ã‚­ã‚¹ãƒˆ(ãƒ©ãƒ™ãƒ«)ã‚’æŠ½å‡º
        nodes = graph_data.get('nodes', [])
        node_texts = [node.get('id', '') for node in nodes]
        
        if not node_texts:
            print(f"No node texts found to generate embeddings for user: {user_id}")
        else:
            # 2. å…¨ãƒãƒ¼ãƒ‰ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¸€æ‹¬ç”Ÿæˆ
            node_embeddings = _get_embeddings(node_texts)

            if node_embeddings and len(node_embeddings) == len(nodes):
                datapoints_to_upsert = []
                batch = db_firestore.batch()

                for i, node in enumerate(nodes):
                    node_label = node.get('id')
                    node_id = node.get('id') # ã“ã“ã§ã¯ãƒ©ãƒ™ãƒ«ã‚’IDã¨ã—ã¦ä½¿ã†
                    embedding = node_embeddings[i]
                    
                    # a. Firestoreã«ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã€ãƒãƒƒãƒã«è¿½åŠ 
                    embedding_ref = db_firestore.collection('vector_embeddings').document()
                    batch.set(embedding_ref, {
                        'user_id': user_id,
                        'embedding': embedding,
                        'created_at': firestore.SERVER_TIMESTAMP,
                        'nodeId': node_id,
                        'nodeLabel': node_label,
                        'source_text': node_label # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚ä¿å­˜
                    })
                    
                    # b. Vector Searchã«Upsertã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’è¿½åŠ 
                    datapoints_to_upsert.append({
                        "datapoint_id": embedding_ref.id,
                        "feature_vector": embedding
                    })

                # c. ãƒãƒƒãƒå‡¦ç†ã§Firestoreã«ä¸€æ‹¬æ›¸ãè¾¼ã¿
                batch.commit()
                print(f"âœ… Saved {len(nodes)} node embeddings to Firestore for user: {user_id}")

                # d. Vector Search Index ã«ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¸€æ‹¬ç™»éŒ²(Upsert)
                if datapoints_to_upsert:
                    vector_search_region = os.getenv('GCP_VERTEX_AI_REGION', 'asia-northeast1')
                    index_resource_name = f"projects/{project_id}/locations/{vector_search_region}/indexes/{VECTOR_SEARCH_INDEX_ID}"
                    vector_search_index = aiplatform.MatchingEngineIndex(index_name=index_resource_name)
                    
                    vector_search_index.upsert_datapoints(datapoints=datapoints_to_upsert)
                    print(f"âœ… Upserted {len(datapoints_to_upsert)} datapoints to Vector Search for user: {user_id}")
            else:
                print(f"âš ï¸ Failed to generate embeddings or count mismatch for user: {user_id}")

    except Exception as e:
        print(f"âŒ Error during node embedding generation/upsert: {e}")
        traceback.print_exc()
    
    # æ–°ã—ã„ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    cache_ref.set({
        'graph_data': graph_data,
        'timestamp': firestore.SERVER_TIMESTAMP,
        'user_id': user_id
    })
    print(f"âœ… Generated and cached new graph data for user: {user_id}")

    # â˜… ä¿®æ­£: ã“ã®ãƒ–ãƒ­ãƒƒã‚¯å…¨ä½“ã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’ä¿®æ­£ã—ã¾ã™
    try:
        if all_insights_text and GOOGLE_BOOKS_API_KEY:
            print(f"--- Triggering background book recommendation update for user: {user_id} ---")
            recommendations = _generate_book_recommendations(all_insights_text, GOOGLE_BOOKS_API_KEY)
            if recommendations and recommendations.get("recommendations"):
                reco_cache_ref = db_firestore.collection('recommendation_cache').document(user_id)
                reco_cache_ref.set({
                    'recommendations': recommendations.get("recommendations", []),
                    'timestamp': firestore.SERVER_TIMESTAMP
                })
                print(f"âœ… Background book recommendation update for user {user_id} completed.")
    except Exception as e:
        print(f"âŒ Error during background book recommendation update: {e}")

    return graph_data

@api_bp.route('/home/suggestion', methods=['GET'])
def get_home_suggestion():
    """ãƒ›ãƒ¼ãƒ ç”»é¢ã«è¡¨ç¤ºã™ã‚‹ã€éå»ã®å¯¾è©±ã«åŸºã¥ãææ¡ˆã‚’è¿”ã™"""
    user_record = _verify_token(request)
    # â˜…â˜…â˜… ä¿®æ­£: èªè¨¼æˆåŠŸæ™‚ã¯dictå‹ã€å¤±æ•—æ™‚ã¯Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã‚‹ãŸã‚ã€dictå‹ã‹ã©ã†ã‹ã§åˆ¤å®šã™ã‚‹ â˜…â˜…â˜…
    if not isinstance(user_record, dict):
        return user_record

    user_id = user_record['uid'] 
    print(f"--- Received home suggestion request for user: {user_id} ---")

    try:
        graph_data = _get_graph_from_cache_or_generate(user_id)
        if not graph_data or 'nodes' not in graph_data or not graph_data['nodes']:
            print("No graph data available for suggestion.")
            return jsonify({}), 204 # ææ¡ˆãªã—

        nodes = graph_data['nodes']
        
        # ã‚¿ã‚¤ãƒ—ãŒ 'issue' ã¾ãŸã¯ 'topic' ã®ãƒãƒ¼ãƒ‰ã‚’å„ªå…ˆçš„ã«æŠ½å‡º
        priority_nodes = [n for n in nodes if n.get('type') in ['issue', 'topic']]
        
        # å„ªå…ˆãƒãƒ¼ãƒ‰ãŒãªã„å ´åˆã¯ã€å…¨ãƒãƒ¼ãƒ‰ã‹ã‚‰é¸ã¶
        target_nodes = priority_nodes if priority_nodes else nodes

        # ãƒãƒ¼ãƒ‰ã‚’ã‚µã‚¤ã‚ºï¼ˆé‡è¦åº¦ï¼‰ã§é™é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_nodes = sorted(target_nodes, key=lambda x: x.get('size', 0), reverse=True)
        
        if not sorted_nodes:
            print("No suitable nodes found for suggestion.")
            return jsonify({}), 204

        # æœ€ã‚‚é‡è¦ãªãƒãƒ¼ãƒ‰ã‚’ææ¡ˆã¨ã—ã¦é¸æŠ
        suggestion_node = sorted_nodes[0]
        node_label = suggestion_node.get('id', 'ä¸æ˜ãªãƒˆãƒ”ãƒƒã‚¯')
        
        response_data = {
            "title": "éå»ã®å¯¾è©±ã‚’æŒ¯ã‚Šè¿”ã£ã¦ã¿ã¾ã›ã‚“ã‹ï¼Ÿ",
            "subtitle": f"ã€Œ{node_label}ã€ã«ã¤ã„ã¦ã€æ–°ãŸãªç™ºè¦‹ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚",
            "nodeId": node_label, 
            "nodeLabel": node_label
        }
        print(f"âœ… Sending suggestion: {response_data}")
        return jsonify(response_data), 200

    except Exception as e:
        print(f"âŒ Error in get_home_suggestion: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to get home suggestion"}), 500


@api_bp.route('/analysis/proactive_suggestion', methods=['GET'])
def get_proactive_suggestion():
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ†æã‚°ãƒ©ãƒ•å…¨ä½“ã‹ã‚‰ã€èƒ½å‹•çš„ãªæ°—ä»˜ãã‚’ä¿ƒã™ãŸã‚ã®è³ªå•ã‚„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
    1. ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    2. æŠ½å‡ºã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å†…éƒ¨ï¼ˆéå»ã®å¯¾è©±ï¼‰ã¨å¤–éƒ¨ï¼ˆWebæ¤œç´¢ï¼‰ã‚’æ¤œç´¢
    3. çµæœã‚’Geminiã§è¦ç´„ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ææ¡ˆã‚’ç”Ÿæˆ
    """
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record

    user_id = user_record['uid']
    print(f"--- Received proactive suggestion request for user: {user_id} ---")

    try:
        # 1. ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ï¼ˆ=ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã®å…¨ä½“åƒï¼‰ã‚’å–å¾—
        graph_data = _get_graph_from_cache_or_generate(user_id)
        if not graph_data or 'nodes' not in graph_data or not graph_data['nodes']:
            print("No graph data available for proactive suggestion.")
            return jsonify({}), 204

        # 2. ã‚°ãƒ©ãƒ•ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º (nodeã®idã‚’çµåˆ)
        graph_keywords = ", ".join([node.get('id', '') for node in graph_data['nodes']])
        if not graph_keywords:
            print("No keywords found in graph.")
            return jsonify({}), 204
        
        print(f"Keywords from graph: {graph_keywords}")

        # 3. å†…éƒ¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆéå»ã®å¯¾è©±ï¼‰ã‚’è¦ç´„
        all_insights_text = _get_all_insights_as_text(user_id)
        # ã‚°ãƒ©ãƒ•å…¨ä½“ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸­ã‹ã‚‰ã€ç‰¹ã«é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã‚“ã§æ–‡è„ˆã‚’è¦ç´„
        chosen_keyword = np.random.choice(PROACTIVE_KEYWORDS)
        internal_summary = _summarize_internal_context(all_insights_text, chosen_keyword)

        # 4. å¤–éƒ¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆWebæ¤œç´¢ï¼‰ã‚’å–å¾—
        # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’Geminiã§ç”Ÿæˆ
        search_query_prompt = f"""
ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¾¤ã¯ã€ã‚ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ‚©ã¿ã‚„é–¢å¿ƒäº‹ã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚
ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦ã€ç¾çŠ¶ã‚’ä¹—ã‚Šè¶Šãˆã‚‹ãŸã‚ã®å…·ä½“çš„ãªãƒ’ãƒ³ãƒˆã‚„ã€å®¢è¦³çš„ãªæƒ…å ±ã‚’æä¾›ã™ã‚‹ãŸã‚ã®ã€åŠ¹æœçš„ãªWebæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’1ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {graph_keywords}
æ¤œç´¢ã‚¯ã‚¨ãƒª:"""
        
        flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
        model = GenerativeModel(flash_model)
        search_query = model.generate_content(search_query_prompt).text.strip()
        print(f"Generated search query: {search_query}")

        external_summary, sources = _generate_rag_based_advice(
            query=search_query,
            project_id=project_id,
            similar_cases_engine_id=SIMILAR_CASES_ENGINE_ID,
            suggestions_engine_id=SUGGESTIONS_ENGINE_ID,
            rag_type="suggestions" # å…·ä½“çš„ãªå¯¾ç­–ã‚’æ¤œç´¢
        )

        # 5. Geminiã§æœ€çµ‚çš„ãªææ¡ˆã‚’ç”Ÿæˆ
        final_prompt = f"""
ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‰¯ãç›¸è«‡ç›¸æ‰‹ã§ã‚ã‚Šã€æ–°ãŸãªè¦–ç‚¹ã‚’æä¾›ã™ã‚‹ã‚³ãƒ¼ãƒã§ã™ã€‚
ä»¥ä¸‹ã®æƒ…å ±ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œãªã‚‹ã»ã©ã€ãã‚“ãªè€ƒãˆæ–¹ã‚‚ã‚ã‚‹ã®ã‹ã€ã¨ãƒãƒƒã¨ã™ã‚‹ã‚ˆã†ãªã€å„ªã—ãã‚‚æ´å¯Ÿã«æº€ã¡ãŸèªã‚Šã‹ã‘ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

# ã‚ãªãŸã¸ã®ã‚¤ãƒ³ãƒ—ãƒƒãƒˆ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒéå»ã«è©±ã—ãŸå†…å®¹ã®è¦ç´„: {internal_summary}
- é–¢é€£ã™ã‚‹å¤–éƒ¨æƒ…å ±ã®è¦ç´„: {external_summary}
- å‚è€ƒæƒ…å ±æºURL: {", ".join(sources) if sources else "ãªã—"}

# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯
1. ä¸Šè¨˜ã®ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚’çµ±åˆã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®èªã‚Šã‹ã‘ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
2. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’åŠ±ã¾ã—ã€æ¬¡ã®ä¸€æ­©ã‚’è€ƒãˆã‚‹ãã£ã‹ã‘ã‚’ä¸ãˆã‚‹ã‚ˆã†ãªã€ãƒã‚¸ãƒ†ã‚£ãƒ–ãªãƒˆãƒ¼ãƒ³ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
3. å¿…ãšã€æœ€çµ‚çš„ãªå‡ºåŠ›ã¯ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONå½¢å¼ã«ã—ã¦ãã ã•ã„ã€‚
   - `initialSummary`: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®èªã‚Šã‹ã‘ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆ200æ–‡å­—ç¨‹åº¦ï¼‰
   - `actions`: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã«ä½•ã‚’ã™ã¹ãã‹ã®å…·ä½“çš„ãªé¸æŠè‚¢ï¼ˆç©ºã®é…åˆ—ã§OKï¼‰
   - `nodeLabel`: 'AIã‹ã‚‰ã®ææ¡ˆ' ã¨ã„ã†å›ºå®šæ–‡å­—åˆ—
   - `nodeId`: 'proactive_suggestion' ã¨ã„ã†å›ºå®šæ–‡å­—åˆ—
"""
        
        pro_model = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
        response_json = _call_gemini_with_schema(
            final_prompt,
            schema={
                "type": "object",
                "properties": {
                    "initialSummary": {"type": "string"},
                    "actions": {"type": "array", "items": {"type": "string"}},
                    "nodeLabel": {"type": "string"},
                    "nodeId": {"type": "string"}
                },
                "required": ["initialSummary", "actions", "nodeLabel", "nodeId"]
            },
            model_name=pro_model
        )

        print(f"âœ… Sending proactive suggestion: {response_json}")
        return jsonify(response_json), 200

    except Exception as e:
        print(f"âŒ Error in get_proactive_suggestion: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to get proactive suggestion"}), 500


@api_bp.route('/chat/node_tap', methods=['POST'])
def handle_node_tap():
    """ã‚°ãƒ©ãƒ•ä¸Šã®ãƒãƒ¼ãƒ‰ãŒã‚¿ãƒƒãƒ—ã•ã‚ŒãŸæ™‚ã«ã€é–¢é€£æƒ…å ±ã‚’è¿”ã™"""
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record

    data = request.get_json()
    if not data or 'node_label' not in data:
        return jsonify({"error": "node_label is required"}), 400

    node_label = data['node_label']
    user_id = user_record['uid']

    try:
        # 1. å†…éƒ¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆéå»ã®å¯¾è©±ï¼‰ã‚’è¦ç´„
        all_insights_text = _get_all_insights_as_text(user_id)
        internal_summary = _summarize_internal_context(all_insights_text, node_label)

        # â˜…â˜…â˜… ä¿®æ­£ç‚¹ â˜…â˜…â˜…
        # RAGã«ã‚ˆã‚‹å¤–éƒ¨æƒ…å ±ã®æ¤œç´¢ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸæ™‚ã«å®Ÿè¡Œã•ã‚Œã‚‹ã‚ˆã†ã«å¤‰æ›´ã€‚
        # ã“ã“ã§ã¯ã€ãã®ãŸã‚ã®ãƒœã‚¿ãƒ³å®šç¾©ï¼ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼‰ã®ã¿ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        # ã“ã‚Œã«ã‚ˆã‚Šã€ãƒãƒ¼ãƒ‰ã‚¿ãƒƒãƒ—æ™‚ã®å¿œç­”ãŒå¤§å¹…ã«é«˜é€ŸåŒ–ã•ã‚Œã¾ã™ã€‚
        initial_summary = f"ã€Œ{node_label}ã€ã«ã¤ã„ã¦ã§ã™ã­ã€‚{internal_summary}"

        actions = [
            {
                "id": "similar_cases", # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®ãƒ¢ãƒ‡ãƒ«ã«åˆã‚ã›ã¦ "type" ã‹ã‚‰ "id" ã«å¤‰æ›´
                "title": "ä¼¼ãŸã‚ˆã†ãªæ‚©ã¿ã‚’æŒã¤ä»–ã®äººã®å£°ã‚’èã"
            },
            {
                "id": "suggestions", # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®ãƒ¢ãƒ‡ãƒ«ã«åˆã‚ã›ã¦ "type" ã‹ã‚‰ "id" ã«å¤‰æ›´
                "title": "å…·ä½“çš„ãªè§£æ±ºç­–ã‚„ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’è¦‹ã‚‹"
            }
        ]

        response_data = {
            "initialSummary": initial_summary,
            "actions": actions,
            "nodeId": data.get('nodeId', node_label),
            "nodeLabel": node_label
        }

        print(f"âœ… Sending node tap response for '{node_label}'")
        return jsonify(response_data), 200

    except Exception as e:
        print(f"âŒ Error in handle_node_tap: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to handle node tap"}), 500

@api_bp.route('/analysis/chat', methods=['POST'])
def post_chat_message():
    user_record = _verify_token(request)
    if not isinstance(user_record, dict):
        return user_record

    data = request.get_json()
    if not data:
        return jsonify({"error": "Invalid request: no data provided"}), 400

    chat_history = data.get('chat_history', [])
    message = data.get('message')
    use_rag = data.get('use_rag', False)
    rag_type = data.get('rag_type')
    user_id = user_record['uid']

    if not message:
        return jsonify({"error": "Invalid request: 'message' is required"}), 400

    try:
        if use_rag:
            # RAGã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼ˆãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸå ´åˆï¼‰
            print(f"--- Triggering RAG task (type: {rag_type}) for user: {user_id} ---")
            
            # 1. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§çµæœã‚’å¾…ã¤ãŸã‚ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªIDã‚’ç”Ÿæˆ
            request_id = str(uuid.uuid4())
            
            # 2. ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã«æ¸¡ã™æƒ…å ±ã‚’ä½œæˆ
            task_payload = {
                'user_id': user_id,
                'request_id': request_id,
                'chat_history': chat_history,
                'message': message,
                'rag_type': rag_type,
            }
            # 3. Cloud Tasksã«å‡¦ç†ã‚’ä¾é ¼
            _create_cloud_task(task_payload, '/api/tasks/execute_rag')

            # 4. RAGå‡¦ç†ã®å®Œäº†ã‚’å¾…ãŸãšã«ã€ã™ãã«ä¸­é–“å¿œç­”ã‚’è¿”ã™
            return jsonify({
                "response": "æ‰¿çŸ¥ã—ã¾ã—ãŸã€‚é–¢é€£æƒ…å ±ã‚’æ¢ã—ã¦ãã¾ã™ã®ã§ã€å°‘ã€…ãŠå¾…ã¡ãã ã•ã„...",
                "request_id": request_id, # ãƒ•ãƒ­ãƒ³ãƒˆãŒçµæœã‚’å¾…ã¤ãŸã‚ã®ID
                "sources": []
            })
        else:
            # RAGã‚’ä½¿ç”¨ã—ãªã„é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆ
            print(f"--- Received chat message from user: {user_id} ---")
            session_summary_text = _get_all_insights_as_text(user_id)
            if not session_summary_text:
                ai_response_text = generate_chat_response("", chat_history, message)
            else:
                ai_response_text = generate_chat_response(session_summary_text, chat_history, message)
            return jsonify({"response": ai_response_text, "sources": []})

    except Exception as e:
        print(f"âŒ Error in post_chat_message: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to process chat message"}), 500

@api_bp.route('/tasks/execute_rag', methods=['POST'])
def handle_execute_rag():
    try:
        data = request.get_json()
        if not data:
            return "No data received", 400

        # ã‚¿ã‚¹ã‚¯ã«å¿…è¦ãªæƒ…å ±ã‚’ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—
        user_id = data.get('user_id')
        request_id = data.get('request_id')
        chat_history = data.get('chat_history', [])
        message = data.get('message')
        rag_type = data.get('rag_type')

        if not all([user_id, request_id, message, rag_type]):
            print(f"Task handler missing required data: {data}")
            return "Missing data", 400

        print(f"--- Executing RAG task (type: {rag_type}) for request: {request_id} ---")
        
        # 1. RAGå‡¦ç†ã‚’å®Ÿè¡Œã—ã¦ã€æœ€çµ‚çš„ãªAIã®å¿œç­”ã¨æƒ…å ±æºã‚’å–å¾—
        session_summary_text = _get_all_insights_as_text(user_id)
        rag_query = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æ:\n{session_summary_text}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{message}"
        
        ai_response_text, sources = _generate_rag_based_advice(
            query=rag_query,
            project_id=project_id,
            similar_cases_engine_id=SIMILAR_CASES_ENGINE_ID,
            suggestions_engine_id=SUGGESTIONS_ENGINE_ID,
            rag_type=rag_type
        )

        # 2. çµæœã‚’Firestoreã«ä¿å­˜
        #    ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ 'rag_responses' ã®ä¸­ã«ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆIDã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã¨ã—ã¦ä¿å­˜
        result_ref = db_firestore.collection('rag_responses').document(request_id)
        result_ref.set({
            'user_id': user_id,
            'response': ai_response_text,
            'sources': sources,
            'created_at': firestore.SERVER_TIMESTAMP,
            'status': 'completed'
        })
        
        print(f"âœ… Successfully executed RAG task and saved result for request: {request_id}")
        return "Successfully processed RAG task", 200

    except Exception as e:
        print(f"âŒ Error in /tasks/execute_rag: {e}")
        traceback.print_exc()
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã“ã¨ã‚’Firestoreã«è¨˜éŒ²
        if 'request_id' in locals() and request_id:
             result_ref = db_firestore.collection('rag_responses').document(request_id)
             result_ref.set({ 'status': 'error', 'error_message': str(e) }, merge=True)
        return "Error processing task", 200

@api_bp.route('/home/suggestion_v2', methods=['GET'])
def get_home_suggestion_v2():
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€æ–°ã®ãƒ™ã‚¯ãƒˆãƒ«ã«åŸºã¥ãã€Vertex AI Vector Search ã‚’ä½¿ã£ã¦é¡ä¼¼ã—ãŸéå»ã®å¯¾è©±ãƒãƒ¼ãƒ‰ã‚’æ¤œç´¢ã—ã€
    ãƒ›ãƒ¼ãƒ ç”»é¢ã§æ–°ã—ã„å¯¾è©±ã®ãã£ã‹ã‘ã‚’ææ¡ˆã—ã¾ã™ã€‚
    """
    user_record = _verify_token(request)
    # â˜…â˜…â˜… ä¿®æ­£: èªè¨¼æˆåŠŸæ™‚ã¯dictå‹ã€å¤±æ•—æ™‚ã¯Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¿”ã‚‹ãŸã‚ã€dictå‹ã‹ã©ã†ã‹ã§åˆ¤å®šã™ã‚‹ â˜…â˜…â˜…
    if not isinstance(user_record, dict):
        return user_record

    user_id = user_record['uid']
    print(f"--- Received home suggestion v2 request for user: {user_id} ---")

    # (â˜…ä¿®æ­£) Vector Searchç”¨ã®ãƒªãƒ¼ã‚¸ãƒ§ãƒ³å¤‰æ•°ã‚’æ˜ç¤ºçš„ã«å–å¾—
    vector_search_region = os.getenv('GCP_VERTEX_AI_REGION', 'asia-northeast1')


    # ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if not all([VECTOR_SEARCH_INDEX_ID, VECTOR_SEARCH_ENDPOINT_ID, VECTOR_SEARCH_DEPLOYED_INDEX_ID]):
        print("âŒ ERROR: Vector Search environment variables are not set on the server.")
        return jsonify({"error": "Server configuration error for suggestions."}), 500

    try:
        # 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€æ–°ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
        query_ref = db_firestore.collection('vector_embeddings').where('user_id', '==', user_id).order_by('created_at', direction=firestore.Query.DESCENDING).limit(1)
        docs = list(query_ref.stream())

        if not docs:
            print(f"No vector embeddings found for user {user_id}.")
            return jsonify({}), 204 # ææ¡ˆãªã—

        latest_doc = docs[0]
        latest_doc_data = latest_doc.to_dict()
        latest_embedding = latest_doc_data.get('embedding')
        
        if not latest_embedding:
            print(f"Embedding not found in the latest document for user {user_id}.")
            return jsonify({}), 204

        print(f"Found latest embedding for user {user_id}. Searching for neighbors...")

        # 2. Vertex AI Vector Search ã§è¿‘å‚æ¢ç´¢
        endpoint_resource_name = f"projects/{project_id}/locations/{vector_search_region}/indexEndpoints/{VECTOR_SEARCH_ENDPOINT_ID}"
        my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(index_endpoint_name=endpoint_resource_name)

        response = my_index_endpoint.find_neighbors(
            queries=[latest_embedding],
            num_neighbors=5, # è‡ªåˆ†è‡ªèº«ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§å¤šã‚ã«å–å¾—
            deployed_index_id=VECTOR_SEARCH_DEPLOYED_INDEX_ID
        )

        if not response or not response[0]:
             print("No similar nodes found from vector search.")
             return jsonify({}), 204

        # 3. æ¤œç´¢çµæœã®å‡¦ç†
        # è‡ªåˆ†è‡ªèº«ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’é™¤å¤–
        filtered_neighbors = [neighbor for neighbor in response[0] if neighbor.id != latest_doc.id]

        if not filtered_neighbors:
            print("No other similar nodes found after filtering.")
            return jsonify({}), 204

        # 4. ææ¡ˆã™ã‚‹ãƒãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦è©³ç´°æƒ…å ±ã‚’å–å¾—
        # æœ€ã‚‚é¡ä¼¼åº¦ãŒé«˜ã„ã‚‚ã®ã‚’é¸æŠ
        suggestion_neighbor = filtered_neighbors[0]
        
        # Vector Searchã®IDã¯ `vector_embeddings` ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã¨ä¸€è‡´ã™ã‚‹
        suggestion_ref = db_firestore.collection('vector_embeddings').document(suggestion_neighbor.id)
        suggestion_doc = suggestion_ref.get()

        if not suggestion_doc.exists:
            print(f"Suggested document {suggestion_neighbor.id} not found in Firestore.")
            return jsonify({}), 204

        suggestion_data = suggestion_doc.to_dict()
        node_label = suggestion_data.get('nodeLabel')
        node_id = suggestion_data.get('nodeId')

        if not node_label or not node_id:
            print(f"nodeLabel or nodeId missing in suggested document {suggestion_neighbor.id}.")
            return jsonify({}), 204

        # 5. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«è¿”ã™ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
        response_data = {
            "title": "éå»ã®å¯¾è©±ã‚’æŒ¯ã‚Šè¿”ã£ã¦ã¿ã¾ã›ã‚“ã‹ï¼Ÿ",
            "subtitle": f"ã€Œ{node_label}ã€ã«ã¤ã„ã¦ã€æ–°ãŸãªç™ºè¦‹ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚",
            "nodeId": node_id,
            "nodeLabel": node_label
        }
        print(f"âœ… Sending suggestion v2: {response_data}")
        return jsonify(response_data), 200

    except Exception as e:
        print(f"âŒ Error in get_home_suggestion_v2: {e}")
        traceback.print_exc()
        return jsonify({"error": "Failed to get home suggestion"}), 500

@api_bp.route('/tasks/prefetch_questions', methods=['POST'])
def handle_prefetch_questions():
    """Cloud Tasksã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹ã€è³ªå•ã‚’å…ˆèª­ã¿ã™ã‚‹ã‚¿ã‚¹ã‚¯"""
    try:
        data = request.get_json()
        if not data:
            print("Task handler received no data.")
            return "No data received", 400

        session_id = data.get('session_id')
        user_id = data.get('user_id')
        insights_md = data.get('insights_md')
        current_turn = data.get('current_turn')
        
        if not all([session_id, user_id, insights_md, isinstance(current_turn, int)]):
            print(f"Task handler missing required data: {data}")
            return "Missing data", 400

        _prefetch_questions_and_save(session_id, user_id, insights_md, current_turn, MAX_TURNS)
        return "Successfully processed prefetch task", 200
    except Exception as e:
        print(f"âŒ Error in /tasks/prefetch_questions: {e}")
        traceback.print_exc()
        # Cloud TasksãŒãƒªãƒˆãƒ©ã‚¤ã—ãªã„ã‚ˆã†ã« 200 OK ã‚’è¿”ã™
        return "Error processing task, but acknowledging to prevent retry", 200

@api_bp.route('/tasks/update_graph', methods=['POST'])
def handle_update_graph():
    """Cloud Tasksã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹ã€åˆ†æã‚°ãƒ©ãƒ•ã‚’æ›´æ–°ã™ã‚‹ã‚¿ã‚¹ã‚¯"""
    try:
        data = request.get_json()
        if not data or 'user_id' not in data:
            print(f"Task handler missing user_id: {data}")
            return "user_id is required", 400
        
        user_id = data['user_id']
        _update_graph_cache(user_id)
        return "Successfully processed graph update task", 200
    except Exception as e:
        print(f"âŒ Error in /tasks/update_graph: {e}")
        traceback.print_exc()
        return "Error processing task, but acknowledging to prevent retry", 200

app.register_blueprint(api_bp)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 8080)), debug=True)