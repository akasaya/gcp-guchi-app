import firebase_admin
from firebase_admin import credentials, firestore, auth
from flask import Flask, request, jsonify
from flask_cors import CORS

import os
import json
import re
import traceback
import threading
import requests
from bs4 import BeautifulSoup
import numpy as np
import hashlib
from datetime import datetime, timedelta, timezone

from tenacity import retry, stop_after_attempt, wait_exponential
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig
from vertexai.language_models import TextEmbeddingModel
from google.cloud import discoveryengine_v1 as discoveryengine
from langchain.text_splitter import RecursiveCharacterTextSplitter


# --- GCP & Firebase åˆæœŸåŒ– ---
try:
    print("Initializing GCP services using Application Default Credentials...")
    firebase_admin.initialize_app()
    db_firestore = firestore.client()
    
    app_instance = firebase_admin.get_app()
    project_id = app_instance.project_id
    print(f"âœ… Firebase Admin SDK initialized for project: {project_id}")

    vertex_ai_region = os.getenv('GCP_VERTEX_AI_REGION', 'us-central1')
    vertexai.init(project=project_id, location=vertex_ai_region)
    print(f"âœ… Vertex AI initialized for project: {project_id} in {vertex_ai_region}")

    # RAGç”¨è¨­å®š (2ã¤ã®ã‚¨ãƒ³ã‚¸ãƒ³IDã«å¯¾å¿œ)
    SIMILAR_CASES_ENGINE_ID = os.getenv('SIMILAR_CASES_ENGINE_ID')
    SUGGESTIONS_ENGINE_ID = os.getenv('SUGGESTIONS_ENGINE_ID')
    if 'K_SERVICE' in os.environ and (not SIMILAR_CASES_ENGINE_ID or not SUGGESTIONS_ENGINE_ID):
        print("âš ï¸ WARNING: One or both of SIMILAR_CASES_ENGINE_ID and SUGGESTIONS_ENGINE_ID environment variables are not set.")

except Exception as e:
    db_firestore = None
    print(f"âŒ Error during initialization: {e}")
    traceback.print_exc()
    if 'K_SERVICE' in os.environ:
        raise

app = Flask(__name__)
# --- CORSè¨­å®š ---
prod_origin = "https://guchi-app-flutter.web.app"
if 'K_SERVICE' in os.environ:
    origins = [prod_origin]
else:
    origins = [
        prod_origin,
        re.compile(r"http://localhost:.*"),
        re.compile(r"http://127.0.0.1:.*"),
    ]
CORS(app, resources={r"/*": {"origins": origins}})

@app.route('/', methods=['GET'])
def index():
    return "GuchiSwipe Gateway is running.", 200

# ===== RAG Cache Settings =====
RAG_CACHE_COLLECTION = 'rag_cache'
RAG_CACHE_TTL_DAYS = 7 # Cache expires after 7 days


# ===== JSONã‚¹ã‚­ãƒ¼ãƒå®šç¾© =====
QUESTIONS_SCHEMA = {"type": "object","properties": {"questions": {"type": "array","items": {"type": "object","properties": {"question_text": {"type": "string"}},"required": ["question_text"]}}},"required": ["questions"]}
SUMMARY_SCHEMA = {"type": "object","properties": {"title": {"type": "string", "description": "ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã‚’è¦ç´„ã™ã‚‹15æ–‡å­—ç¨‹åº¦ã®çŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«"},"insights": {"type": "string", "description": "æŒ‡å®šã•ã‚ŒãŸMarkdownå½¢å¼ã§ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æãƒ¬ãƒãƒ¼ãƒˆ"}},"required": ["title", "insights"]}
GRAPH_SCHEMA = {"type": "object","properties": {"nodes": {"type": "array","items": {"type": "object","properties": {"id": {"type": "string"},"type": {"type": "string", "enum": ["emotion", "topic", "keyword", "issue"]},"size": {"type": "integer"}},"required": ["id", "type", "size"]}},"edges": {"type": "array","items": {"type": "object","properties": {"source": {"type": "string"},"target": {"type": "string"},"weight": {"type": "integer"}},"required": ["source", "target", "weight"]}}},"required": ["nodes", "edges"]}

# ===== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ =====
SUMMARY_ONLY_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„Ÿæƒ…ã®å‹•ãã‚’åˆ†æã™ã‚‹ãƒ—ãƒ­ã®è‡¨åºŠå¿ƒç†å£«ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€Œ{topic}ã€ã¨ã„ã†ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦å¯¾è©±ã—ã¦ã„ã¾ã™ã€‚
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ä¼šè©±å±¥æ­´ã‚’åˆ†æã—ã€å¿…ãšæŒ‡ç¤ºé€šã‚Šã®JSONå½¢å¼ã§åˆ†æãƒ¬ãƒãƒ¼ãƒˆã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
# åˆ†æå¯¾è±¡ã®ä¼šè©±å±¥æ­´
{swipes_text}
# å‡ºåŠ›å½¢å¼ (JSON)
å¿…ãšä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
- `title`: ä¼šè©±å…¨ä½“ã‚’è±¡å¾´ã™ã‚‹15æ–‡å­—ç¨‹åº¦ã®çŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«ã€‚
- `insights`: ä»¥ä¸‹ã®Markdownå½¢å¼ã§ **å³å¯†ã«** è¨˜è¿°ã•ã‚ŒãŸåˆ†æãƒ¬ãƒãƒ¼ãƒˆã€‚
```markdown
### âœ¨ å…¨ä½“çš„ãªè¦ç´„
ï¼ˆã“ã“ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç¾åœ¨ã®å¿ƒç†çŠ¶æ…‹ã€ä¸»ãªæ„Ÿæƒ…ã€å†…é¢çš„ãªè‘›è—¤ãªã©ã‚’2ã€œ3æ–‡ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ï¼‰
### ğŸ“ è©³ç´°ãªåˆ†æ
ï¼ˆã“ã“ã«ã€å…·ä½“çš„ãªåˆ†æå†…å®¹ã‚’ç®‡æ¡æ›¸ãã§è¨˜è¿°ã—ã¦ãã ã•ã„ï¼‰
* **æ„Ÿæƒ…ã®çŠ¶æ…‹**: ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ„Ÿã˜ã¦ã„ã‚‹ä¸»è¦ãªæ„Ÿæƒ…ã«ã¤ã„ã¦ã€ãã®æ ¹æ‹ ã¨å…±ã«è¨˜è¿°ã—ã¦ãã ã•ã„ï¼‰
* **æ³¨ç›®ã™ã¹ãç‚¹**: ï¼ˆå›ç­”å†…å®¹ã¨ã€ãŸã‚ã‚‰ã„æ™‚é–“ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹æ„Ÿæƒ…ã®çŸ›ç›¾ã€ç‰¹ã«å°è±¡çš„ãªå›ç­”ãªã©ã€åˆ†æã®éµã¨ãªã£ãŸãƒã‚¤ãƒ³ãƒˆã‚’å…·ä½“çš„ã«æŒ™ã’ã¦ãã ã•ã„ã€‚ä¼šè©±å±¥æ­´ã«ã€Œç‰¹ã«è¿·ã„ãŒè¦‹ã‚‰ã‚Œã¾ã—ãŸã€ã¨è¨˜è¼‰ã®ã‚ã‚‹å›ç­”ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãŸã‚ã‚‰ã„ã‚„è‘›è—¤ã‚’æŠ±ãˆã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰
* **æ ¹æœ¬çš„ãªèª²é¡Œ**: ï¼ˆåˆ†æã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç›´é¢ã—ã¦ã„ã‚‹æ ¹æœ¬çš„ãªèª²é¡Œã‚„æ¬²æ±‚ã«ã¤ã„ã¦è¨˜è¿°ã—ã¦ãã ã•ã„ï¼‰
### ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®ææ¡ˆ
ï¼ˆä»Šå›ã®åˆ†æã‚’è¸ã¾ãˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡å›ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§æ·±æ˜ã‚Šã™ã‚‹ã¨è‰¯ã•ãã†ãªãƒ†ãƒ¼ãƒã‚„ã€æ—¥å¸¸ç”Ÿæ´»ã§æ„è­˜ã—ã¦ã¿ã‚‹ã¨è‰¯ã„ã“ã¨ãªã©ã‚’ã€å…·ä½“çš„ã‹ã¤ãƒã‚¸ãƒ†ã‚£ãƒ–ãªè¨€è‘‰ã§ææ¡ˆã—ã¦ãã ã•ã„ï¼‰
```
"""
GRAPH_ANALYSIS_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã§ã‚ã‚Šã€è‡¨åºŠå¿ƒç†å£«ã§ã‚‚ã‚ã‚Šã¾ã™ã€‚
ã“ã‚Œã‹ã‚‰æ¸¡ã™ãƒ†ã‚­ã‚¹ãƒˆã¯ã€ã‚ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¤‡æ•°å›ã®ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨˜éŒ²ã§ã™ã€‚
ã“ã®è¨˜éŒ²å…¨ä½“ã‚’åˆ†æã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†çŠ¶æ…‹ã®æ ¸ã¨ãªã‚‹è¦ç´ ã‚’æŠ½å‡ºã—ã€ãã‚Œã‚‰ã®é–¢é€£æ€§ã‚’è¡¨ç¾ã™ã‚‹ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
# ã‚°ãƒ©ãƒ•ç”Ÿæˆã®ãƒ«ãƒ¼ãƒ«
1. ãƒãƒ¼ãƒ‰ã®ç¨®é¡: `topic`, `issue`, `emotion`, `keyword`
2. ãƒãƒ¼ãƒ‰ã®éšå±¤: ä¸­å¿ƒã«`topic`ã¨`issue`ã‚’é…ç½®ã—ã€`emotion`ã‚„`keyword`ã¯ãã‚Œã‚‰ã‹ã‚‰æåˆ†ã‹ã‚Œã•ã›ã‚‹ã€‚
3. ãƒãƒ¼ãƒ‰æ•°ã®åˆ¶é™: ç·æ•°ã¯æœ€å¤§ã§ã‚‚15å€‹ç¨‹åº¦ã«å³é¸ã™ã‚‹ã€‚
4. IDã®è¨€èª: `id`ã¯å¿…ãšæ—¥æœ¬èªã®å˜èªã¾ãŸã¯çŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã«ã™ã‚‹ã€‚
# å‡ºåŠ›JSONã®ä»•æ§˜
å‡ºåŠ›ã¯ã€ä»¥ä¸‹ã®ä»•æ§˜ã«å³å¯†ã«å¾“ã£ãŸJSONå½¢å¼ã®ã¿ã¨ã™ã‚‹ã“ã¨ã€‚ { "nodes": [ ... ], "edges": [ ... ] }
# ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²
"""
CHAT_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æã®å°‚é–€å®¶ã§ã‚ã‚Šã€å…±æ„ŸåŠ›ã¨æ´å¯ŸåŠ›ã«å„ªã‚ŒãŸã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã€Œã‚³ã‚³ãƒ­ã®åˆ†æå®˜ã€ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€è‡ªèº«ã®æ€è€ƒã‚’å¯è¦–åŒ–ã—ãŸã‚°ãƒ©ãƒ•ã‚’è¦‹ãªãŒã‚‰ã€ã‚ãªãŸã¨å¯¾è©±ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚
# ã‚ãªãŸã®å½¹å‰²
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®éå»ã®ä¼šè©±å±¥æ­´ã¨ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã®è¦ç´„ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ï¼‰ã‚’å¸¸ã«å‚ç…§ã—ã€æ–‡è„ˆã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã‚’æ·±ãå‚¾è´ã—ã€ã¾ãšã¯è‚¯å®šçš„ã«å—ã‘æ­¢ã‚ã¦å…±æ„Ÿã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚
- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ã®å†…å®¹ã«åŸºã¥ãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‡ªåˆ†ã§ã‚‚æ°—ã¥ã„ã¦ã„ãªã„å†…é¢ã‚’å„ªã—ãæŒ‡æ‘˜ã—ãŸã‚Šã€æ·±ã„å•ã„ã‚’æŠ•ã’ã‹ã‘ãŸã‚Šã—ã¦ã€è‡ªå·±ç†è§£ã‚’ä¿ƒã—ã¦ãã ã•ã„ã€‚
- æ¯å›ã®è¿”ä¿¡ã‚’è‡ªå·±ç´¹ä»‹ã‹ã‚‰å§‹ã‚ã‚‹ã®ã§ã¯ãªãã€ä¼šè©±ã®æµã‚Œã‚’è‡ªç„¶ã«å¼•ãç¶™ã„ã§ãã ã•ã„ã€‚
- **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åå‰ï¼ˆã€Œã€‡ã€‡ã•ã‚“ã€ãªã©ï¼‰ã¯çµ¶å¯¾ã«ä½¿ç”¨ã›ãšã€å¸¸ã«å¯¾è©±ç›¸æ‰‹ã«ç›´æ¥èªã‚Šã‹ã‘ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚**
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼
{session_summary}
# ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
{chat_history}
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»Šå›ã®ç™ºè¨€
{user_message}
ã‚ãªãŸã®å¿œç­”:
"""
# â˜…â˜…â˜… æ–°è¦è¿½åŠ  â˜…â˜…â˜…
INTERNAL_CONTEXT_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°è¨˜éŒ²ã‚’è¦ç´„ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²å…¨ä½“ã‹ã‚‰ã€ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã«é–¢é€£ã™ã‚‹è¨˜è¿°ã‚„ã€ãã“ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„Ÿæƒ…ã‚„è‘›è—¤ã‚’æŠœãå‡ºã—ã€1ã€œ2æ–‡ã®éå¸¸ã«ç°¡æ½”ãªè¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
è¦ç´„ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã€Œä»¥å‰ã€ã“ã®ä»¶ã«ã¤ã„ã¦ã“ã®ã‚ˆã†ã«ãŠè©±ã—ã•ã‚Œã¦ã„ã¾ã—ãŸã­ã€ã¨è‡ªç„¶ã«èªã‚Šã‹ã‘ã‚‹å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ç›´æ¥é–¢é€£ã™ã‚‹è¨˜è¿°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ã€Œã“ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€ã“ã‚Œã¾ã§å…·ä½“çš„ãªãŠè©±ã¯ãªã‹ã£ãŸã‚ˆã†ã§ã™ã€‚ã€ã¨å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

# ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²
{context}

# è¦ç´„:
"""

# â˜…â˜…â˜… æ–°è¦è¿½åŠ  â˜…â˜…â˜…
# AIãŒèƒ½å‹•çš„ã«ææ¡ˆã‚’è¡Œã†ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
PROACTIVE_KEYWORDS = [
    "ç‡ƒãˆå°½ã", "ãƒãƒ¼ãƒ³ã‚¢ã‚¦ãƒˆ", "ç„¡æ°—åŠ›", "ç–²å¼Š",
    "ã‚­ãƒ£ãƒªã‚¢", "è»¢è·", "ä»•äº‹ã®æ‚©ã¿", "å°†æ¥è¨­è¨ˆ",
    "å¯¾äººé–¢ä¿‚", "å­¤ç‹¬", "äººé–“é–¢ä¿‚", "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³",
    "è‡ªå·±è‚¯å®šæ„Ÿ", "è‡ªä¿¡ãŒãªã„", "è‡ªåˆ†ã‚’è²¬ã‚ã‚‹",
    "ã‚¹ãƒˆãƒ¬ã‚¹", "ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼", "ä¸å®‰"
]


# ===== Gemini ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ =====
@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
def _call_gemini_with_schema(prompt: str, schema: dict, model_name: str) -> dict:
    model = GenerativeModel(model_name)
    attempt_num = _call_gemini_with_schema.retry.statistics.get('attempt_number', 1)
    print(f"--- Calling Gemini ({model_name}) with schema (Attempt: {attempt_num}) ---")
    try:
        response = model.generate_content(prompt, generation_config=GenerationConfig(response_mime_type="application/json", response_schema=schema))
        response_text = response.text.strip()
        if response_text.startswith("```json"):
            response_text = response_text[7:-3].strip()
        elif response_text.startswith("```"):
            response_text = response_text[3:-3].strip()
        return json.loads(response_text)
    except Exception as e:
        print(f"Error on attempt {attempt_num} with model {model_name}: {e}\n--- Gemini Response ---\n{getattr(response, 'text', 'Empty')}\n---")
        traceback.print_exc()
        raise

def generate_initial_questions(topic):
    prompt = f"ã‚ãªãŸã¯ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚ãƒˆãƒ”ãƒƒã‚¯ã€Œ{topic}ã€ã«ã¤ã„ã¦ã€ã€Œã¯ã„ã€ã‹ã€Œã„ã„ãˆã€ã§ç­”ãˆã‚‰ã‚Œã‚‹è³ªå•ã‚’5ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
    flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
    return _call_gemini_with_schema(prompt, QUESTIONS_SCHEMA, model_name=flash_model).get("questions", [])

def generate_follow_up_questions(insights):
    prompt = f"ã‚ãªãŸã¯ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®åˆ†æçµæœã‚’ã•ã‚‰ã«æ·±ã‚ã‚‹ã€ã€Œã¯ã„ã€ã‹ã€Œã„ã„ãˆã€ã§ç­”ãˆã‚‰ã‚Œã‚‹è³ªå•ã‚’5ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n# åˆ†æçµæœ\n{insights}"
    flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
    return _call_gemini_with_schema(prompt, QUESTIONS_SCHEMA, model_name=flash_model).get("questions", [])

def generate_summary_only(topic, swipes_text):
    prompt = SUMMARY_ONLY_PROMPT_TEMPLATE.format(topic=topic, swipes_text=swipes_text)
    flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
    return _call_gemini_with_schema(prompt, SUMMARY_SCHEMA, model_name=flash_model)

def generate_graph_data(all_insights_text):
    prompt = GRAPH_ANALYSIS_PROMPT_TEMPLATE + all_insights_text
    pro_model = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
    return _call_gemini_with_schema(prompt, GRAPH_SCHEMA, model_name=pro_model)

def generate_chat_response(session_summary, chat_history, user_message):
    history_str = "\n".join([f"{msg['author']}: {msg['text']}" for msg in chat_history])
    prompt = CHAT_PROMPT_TEMPLATE.format(session_summary=session_summary, chat_history=history_str, user_message=user_message)
    pro_model = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
    model = GenerativeModel(pro_model)
    return model.generate_content(prompt).text.strip()

@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
def _extract_keywords_for_search(analysis_text: str) -> str:
    prompt = f"""
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å¿ƒç†åˆ†æãƒ¬ãƒãƒ¼ãƒˆå…¨ä½“ã‹ã‚‰ã€æœ€ã‚‚é‡è¦ã¨æ€ã‚ã‚Œã‚‹æ¦‚å¿µã‚„èª²é¡Œã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’5ã¤ä»¥å†…ã§æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯Vertex AI Searchã®æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚ä»–ã®æ–‡ã¯å«ã‚ãšã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã®ã¿ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

# åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
{analysis_text}

# å‡ºåŠ›ä¾‹
ä»•äº‹ã®ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼, äººé–“é–¢ä¿‚ã®æ‚©ã¿, è‡ªå·±è‚¯å®šæ„Ÿã®ä½ä¸‹, å°†æ¥ã¸ã®ä¸å®‰

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:
"""
    try:
        flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
        model = GenerativeModel(flash_model)
        print("--- Calling Gemini to extract search keywords ---")
        response = model.generate_content(prompt)
        keywords = response.text.strip()
        print(f"âœ… Extracted Keywords: {keywords}")
        return keywords
    except Exception as e:
        print(f"âŒ Failed to extract keywords: {e}")
        return ""

# â˜…â˜…â˜… æ–°è¦è¿½åŠ  â˜…â˜…â˜…
def _summarize_internal_context(context: str, keyword: str) -> str:
    """Summarizes past session records related to a specific keyword."""
    if not context or not keyword:
        return "ã“ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€ã“ã‚Œã¾ã§å…·ä½“çš„ãªãŠè©±ã¯ãªã‹ã£ãŸã‚ˆã†ã§ã™ã€‚"
    try:
        prompt = INTERNAL_CONTEXT_PROMPT_TEMPLATE.format(context=context, keyword=keyword)
        flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
        model = GenerativeModel(flash_model)
        print(f"--- Calling Gemini to summarize internal context for '{keyword}' ---")
        response = model.generate_content(prompt)
        summary = response.text.strip()
        print(f"âœ… Internal context summary: {summary}")
        return summary
    except Exception as e:
        print(f"âŒ Failed to summarize internal context: {e}")
        return "éå»ã®è¨˜éŒ²ã‚’è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

# ===== RAG (Retrieval-Augmented Generation) Helper Functions =====

@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
def _get_embeddings(texts: list[str]) -> list[list[float]]:
    if not texts: return []
    model = TextEmbeddingModel.from_pretrained("text-multilingual-embedding-002")
    BATCH_SIZE = 15 
    all_embeddings = []
    print(f"--- RAG: Generating embeddings for {len(texts)} texts in batches of {BATCH_SIZE} ---")
    try:
        for i in range(0, len(texts), BATCH_SIZE):
            batch = texts[i:i + BATCH_SIZE]
            responses = model.get_embeddings(batch)
            for response in responses:
                all_embeddings.append(response.values)
            print(f"--- RAG: Processed embedding batch {i//BATCH_SIZE + 1}/{-(-len(texts) // BATCH_SIZE)} ---")
        return all_embeddings
    except Exception as e:
        print(f"âŒ RAG: An error occurred during embedding generation: {e}")
        traceback.print_exc()
        return []

def _get_url_cache_doc_ref(url: str):
    url_hash = hashlib.sha256(url.encode('utf-8')).hexdigest()
    return db_firestore.collection(RAG_CACHE_COLLECTION).document(url_hash)

def _get_cached_chunks_and_embeddings(url: str):
    try:
        doc_ref = _get_url_cache_doc_ref(url)
        doc = doc_ref.get()
        if not doc.exists:
            print(f"CACHE MISS: No cache found for URL: {url}")
            return None, None
        cache_data = doc.to_dict()
        cached_at = cache_data.get('cached_at')
        if isinstance(cached_at, datetime):
            if datetime.now(timezone.utc) - cached_at > timedelta(days=RAG_CACHE_TTL_DAYS):
                print(f"CACHE STALE: Cache for {url} is older than {RAG_CACHE_TTL_DAYS} days.")
                return None, None
        else:
             print(f"CACHE INVALID: Invalid 'cached_at' field for {url}.")
             return None, None
        
        chunks = cache_data.get('chunks')
        embeddings_from_db = cache_data.get('embeddings')
        
        if chunks and embeddings_from_db:
            embeddings = [item['vector'] for item in embeddings_from_db if 'vector' in item]
            if len(chunks) == len(embeddings):
                print(f"âœ… CACHE HIT: Found {len(chunks)} chunks for URL: {url}")
                return chunks, embeddings

        print(f"CACHE INVALID: Data mismatch for {url}. Re-fetching.")
        return None, None
    except Exception as e:
        print(f"âŒ Error getting cache for {url}: {e}")
        return None, None

def _set_cached_chunks_and_embeddings(url: str, chunks: list, embeddings: list):
    if not chunks or not embeddings: return
    try:
        doc_ref = _get_url_cache_doc_ref(url)
        transformed_embeddings = [{'vector': emb} for emb in embeddings]
        cache_data = {
            'url': url,
            'chunks': chunks,
            'embeddings': transformed_embeddings,
            'cached_at': firestore.SERVER_TIMESTAMP
        }
        doc_ref.set(cache_data)
        print(f"âœ… CACHE SET: Saved {len(chunks)} chunks for URL: {url}")
    except Exception as e:
        print(f"âŒ Error setting cache for {url}: {e}")
        traceback.print_exc()

# â˜…â˜…â˜… ã“ã®é–¢æ•°ã‚’ä¿®æ­£ â˜…â˜…â˜…
def _generate_rag_based_advice(query: str, project_id: str, similar_cases_engine_id: str, suggestions_engine_id: str, rag_type: str = None):
    """
    RAG based on user analysis to generate advice, using a Firestore cache for embeddings.
    Returns a tuple of (advice_text, list_of_source_urls).
    """
    search_query = _extract_keywords_for_search(query)
    if not search_query:
        print("âš ï¸ RAG: Could not extract keywords. Using original query for search.")
        search_query = query[:512]
    
    all_found_urls = set()
    if rag_type == 'similar_cases':
        print("--- RAG: Searching for SIMILAR CASES ONLY ---")
        if similar_cases_engine_id:
            all_found_urls.update(_search_with_vertex_ai_search(project_id, "global", similar_cases_engine_id, search_query))
    elif rag_type == 'suggestions':
        print("--- RAG: Searching for SUGGESTIONS ONLY ---")
        if suggestions_engine_id:
            all_found_urls.update(_search_with_vertex_ai_search(project_id, "global", suggestions_engine_id, search_query))
    else: # Default behavior: search both
        print("--- RAG: Searching both similar cases and suggestions ---")
        if similar_cases_engine_id:
            all_found_urls.update(_search_with_vertex_ai_search(project_id, "global", similar_cases_engine_id, search_query))
        if suggestions_engine_id:
            all_found_urls.update(_search_with_vertex_ai_search(project_id, "global", suggestions_engine_id, search_query))

    if not all_found_urls:
        return "é–¢é€£ã™ã‚‹å¤–éƒ¨æƒ…å ±ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", []

    all_chunks, all_embeddings, urls_with_content = [], [], []
    urls_to_process = list(all_found_urls)[:5]

    for url in urls_to_process:
        cached_chunks, cached_embeddings = _get_cached_chunks_and_embeddings(url)
        if cached_chunks and cached_embeddings:
            all_chunks.extend(cached_chunks)
            all_embeddings.extend(cached_embeddings)
            urls_with_content.append(url)
        else:
            print(f"SCRAPING: No valid cache for {url}. Fetching content.")
            page_content = _scrape_text_from_url(url)
            if page_content:
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
                new_chunks_full = text_splitter.split_text(page_content)
                
                MAX_CHUNKS_PER_URL = 50  # 1ã¤ã®URLã‹ã‚‰å–å¾—ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã®ä¸Šé™
                new_chunks = new_chunks_full[:MAX_CHUNKS_PER_URL]

                if len(new_chunks_full) > MAX_CHUNKS_PER_URL:
                    print(f"âš ï¸ RAG: Content too long. Truncated chunks for {url} from {len(new_chunks_full)} to {len(new_chunks)}.")
                if new_chunks:
                    new_embeddings = _get_embeddings(new_chunks)
                    if new_embeddings and len(new_chunks) == len(new_embeddings):
                        all_chunks.extend(new_chunks)
                        all_embeddings.extend(new_embeddings)
                        urls_with_content.append(url)
                        threading.Thread(target=_set_cached_chunks_and_embeddings, args=(url, new_chunks, new_embeddings)).start()
                    else:
                        print(f"âš ï¸ RAG: Failed to generate embeddings for {url}. Skipping.")
    
    if not all_chunks:
        return "é–¢é€£ã™ã‚‹å¤–éƒ¨æƒ…å ±ã‚’è¦‹ã¤ã‘ã¾ã—ãŸãŒã€å†…å®¹ã‚’èª­ã¿å–ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", urls_to_process

    print(f"--- RAG: Finding relevant chunks from {len(all_chunks)} total chunks... ---")
    query_embedding_list = _get_embeddings([query])
    if not query_embedding_list:
        return "ã‚ãªãŸã®çŠ¶æ³ã‚’åˆ†æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚", urls_with_content
    
    query_embedding = np.array(query_embedding_list[0])
    
    similarities = []
    for i, emb in enumerate(all_embeddings):
        chunk_embedding = np.array(emb)
        dot_product = np.dot(chunk_embedding, query_embedding)
        norm_product = np.linalg.norm(chunk_embedding) * np.linalg.norm(query_embedding)
        similarity = dot_product / norm_product if norm_product != 0 else 0.0
        similarities.append((similarity, all_chunks[i]))
    
    similarities.sort(key=lambda x: x[0], reverse=True)
    relevant_chunks = [chunk for sim, chunk in similarities[:3]]

    if not relevant_chunks:
        return "é–¢é€£æƒ…å ±ã®ä¸­ã‹ã‚‰ã€ã‚ãªãŸã®çŠ¶æ³ã«ç‰¹ã«åˆè‡´ã™ã‚‹éƒ¨åˆ†ã‚’è¦‹ã¤ã‘å‡ºã™ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", urls_with_content

    print("--- RAG: Generating final advice with Gemini... ---")
    context_text = "\n---\n".join(relevant_chunks)

    # â˜…â˜…â˜… ã“ã“ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¨é¢çš„ã«æ›¸ãæ›ãˆã¾ã™ â˜…â˜…â˜…
    if rag_type == 'similar_cases':
        prompt = f"""
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ‚©ã¿ã«å…±æ„Ÿã—ã€ä»–ã®äººã®ã‚±ãƒ¼ã‚¹ã‚’ç´¹ä»‹ã™ã‚‹èãä¸Šæ‰‹ãªå‹äººã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœã€ã¨ã€Œå‚è€ƒæƒ…å ±ï¼ˆä»–ã®äººã®æ‚©ã¿ã‚„ä½“é¨“è«‡ï¼‰ã€ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’åŠ±ã¾ã™ã‚ˆã†ãªå½¢ã§ã€å‚è€ƒæƒ…å ±ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

# æŒ‡ç¤º
- å…¨ä½“ã§200æ–‡å­—ç¨‹åº¦ã®ã€éå¸¸ã«ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªæ–‡ç« ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å®‰å¿ƒã•ã›ã€ä¸€äººã§ã¯ãªã„ã¨æ„Ÿã˜ã•ã›ã‚‹ã‚ˆã†ãªã€æ¸©ã‹ãå…±æ„Ÿçš„ãªãƒˆãƒ¼ãƒ³ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
- ã€Œä¼¼ãŸã‚ˆã†ãªã“ã¨ã§æ‚©ã‚“ã§ã„ã‚‹æ–¹ã‚‚ã„ã‚‹ã‚ˆã†ã§ã™ã€‚ã€ã¨ã„ã£ãŸå‰ç½®ãã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„ã€‚
- æœ€å¾Œã«ã€å‚è€ƒã«ã—ãŸæƒ…å ±æºã®URLã‚’ `[å‚è€ƒæƒ…å ±]` ã¨ã—ã¦ç®‡æ¡æ›¸ãã§å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚

# ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœ
{query}

# å‚è€ƒæƒ…å ± (ä»–ã®äººã®æ‚©ã¿ã‚„ä½“é¨“è«‡)
---
{context_text}
---

# ã‚ãªãŸã®å¿œç­”:
"""
    else: # 'suggestions' or default
        prompt = f"""
ã‚ãªãŸã¯ã€å®¢è¦³çš„ã§ä¿¡é ¼ã§ãã‚‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã™ã‚‹ãƒ—ãƒ­ã®ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœã€ã¨ã€Œå‚è€ƒæƒ…å ±ï¼ˆå°‚é–€æ©Ÿé–¢ã«ã‚ˆã‚‹å…·ä½“çš„ãªå¯¾ç­–ï¼‰ã€ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã®ä¸€æ­©ã‚’è¸ã¿å‡ºã™ãŸã‚ã®ã€å…·ä½“çš„ã§å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

# æŒ‡ç¤º
- å…¨ä½“ã§300æ–‡å­—ç¨‹åº¦ã®ã€ç°¡æ½”ã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ã„æ–‡ç« ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çŠ¶æ³ã‚’æ•´ç†ã—ã€å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç®‡æ¡æ›¸ãã§2ã€œ3ç‚¹ææ¡ˆã™ã‚‹æ§‹æˆã«ã—ã¦ãã ã•ã„ã€‚
- ã€Œã‚ãªãŸã®çŠ¶æ³ã‚’å®¢è¦³çš„ã«è¦‹ã‚‹ã¨ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€ã“ã®ã‚ˆã†ãªã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚ã€ã¨ã„ã£ãŸã€å°‚é–€å®¶ã¨ã—ã¦ã®å†·é™ãªãƒˆãƒ¼ãƒ³ã§å§‹ã‚ã¦ãã ã•ã„ã€‚
- æœ€å¾Œã«ã€å‚è€ƒã«ã—ãŸæƒ…å ±æºã®URLã‚’ `[å‚è€ƒæƒ…å ±]` ã¨ã—ã¦ç®‡æ¡æ›¸ãã§å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚

# ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœ
{query}

# å‚è€ƒæƒ…å ± (å°‚é–€æ©Ÿé–¢ã«ã‚ˆã‚‹å…·ä½“çš„ãªå¯¾ç­–)
---
{context_text}
---

# ã‚ãªãŸã®å¿œç­”:
"""

    pro_model_name = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
    model = GenerativeModel(pro_model_name)
    advice = model.generate_content(prompt, generation_config=GenerationConfig(temperature=0.7)).text
    
    return advice, list(dict.fromkeys(urls_with_content))

def _search_with_vertex_ai_search(project_id: str, location: str, engine_id: str, query: str) -> list[str]:
    if not engine_id:
        print(f"âŒ RAG: Engine ID '{engine_id}' is not configured.")
        return []
    client = discoveryengine.SearchServiceClient()
    serving_config = (
        f"projects/{project_id}/locations/{location}/collections/default_collection/"
        f"engines/{engine_id}/servingConfigs/default_config"
    )
    request = discoveryengine.SearchRequest(serving_config=serving_config, query=query, page_size=5)
    try:
        response = client.search(request)
        urls = [r.document.derived_struct_data.get('link') for r in response.results if r.document.derived_struct_data.get('link')]
        print(f"âœ… RAG: Found URLs from Vertex AI Search: {urls}")
        return urls
    except Exception as e:
        print(f"âŒ RAG: Vertex AI Search failed for engine '{engine_id}': {e}")
        traceback.print_exc()
        return []

def _scrape_text_from_url(url: str) -> str:
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        response = requests.get(url, timeout=10, headers=headers)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        for element in soup(["script", "style", "header", "footer", "nav", "aside"]):
            element.decompose()
        return soup.get_text(separator=' ', strip=True)
    except requests.exceptions.RequestException as e:
        print(f"âŒ RAG: Error fetching URL {url}: {e}")
        return ""

# --- ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç† (å¤‰æ›´ãªã—) ---
def _prefetch_questions_and_save(session_id: str, user_id: str, insights_md: str, current_turn: int, max_turns: int):
    # ... (ã“ã®é–¢æ•°ã®ä¸­èº«ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    print(f"--- Triggered question prefetch for user: {user_id}, session: {session_id}, next_turn: {current_turn + 1} ---")
    if current_turn >= max_turns:
        print("Max turns reached. Skipping question prefetch.")
        return
    try:
        questions = generate_follow_up_questions(insights=insights_md)
        if not questions:
            print(f"âš ï¸ AI failed to generate prefetch questions for session {session_id}.")
            return
        session_doc_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)
        next_turn = current_turn + 1
        questions_collection = session_doc_ref.collection('questions')
        last_question_query = questions_collection.order_by('order', direction=firestore.Query.DESCENDING).limit(1).stream()
        last_order = next(last_question_query, None)
        start_order = last_order.to_dict().get('order', -1) + 1 if last_order else 0
        batch = db_firestore.batch()
        for i, q_data in enumerate(questions):
            if q_text := q_data.get("question_text"):
                q_doc_ref = questions_collection.document()
                batch.set(q_doc_ref, {'text': q_text, 'turn': next_turn, 'order': start_order + i, 'created_at': firestore.SERVER_TIMESTAMP, 'is_prefetched': True})
        batch.commit()
        print(f"âœ… Successfully prefetched questions for turn {next_turn}.")
    except Exception as e:
        print(f"âŒ Failed to prefetch questions for session {session_id}: {e}")
        traceback.print_exc()

def _update_graph_cache(user_id: str):
    # ... (ã“ã®é–¢æ•°ã®ä¸­èº«ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    print(f"--- Triggered graph cache update for user: {user_id} ---")
    try:
        all_insights_text = _get_all_insights_as_text(user_id)
        if not all_insights_text: return
        raw_graph_data = generate_graph_data(all_insights_text)
        nodes = raw_graph_data.get('nodes', [])
        edges = raw_graph_data.get('edges', [])
        sanitized_nodes = [n for n in nodes if isinstance(n, dict) and n.get('id')]
        valid_node_ids = {n['id'] for n in sanitized_nodes}
        sanitized_edges = [e for e in edges if isinstance(e, dict) and e.get('source') in valid_node_ids and e.get('target') in valid_node_ids]
        final_graph_data = {"nodes": sanitized_nodes, "edges": sanitized_edges}
        cache_doc_ref = db_firestore.collection('users').document(user_id).collection('analysis').document('graph_cache')
        cache_doc_ref.set({'data': final_graph_data, 'updated_at': firestore.SERVER_TIMESTAMP})
        print(f"âœ… Successfully updated graph cache for user: {user_id}")
    except Exception as e:
        print(f"âŒ Failed to update graph cache for user {user_id}: {e}")
        traceback.print_exc()

# ===== èªè¨¼ãƒ˜ãƒ«ãƒ‘ãƒ¼ (å¤‰æ›´ãªã—) =====
def _verify_token(request):
    # ... (ã“ã®é–¢æ•°ã®ä¸­èº«ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    auth_header = request.headers.get('Authorization', '')
    if not auth_header.startswith('Bearer '):
        raise auth.InvalidIdTokenError("Authorization token is missing or invalid")
    id_token = auth_header.split('Bearer ')[1]
    return auth.verify_id_token(id_token, clock_skew_seconds=15)

# ===== API Routes (å¤‰æ›´ãƒ»è¿½åŠ ã‚ã‚Š) =====
@app.route('/session/start', methods=['POST'])
def start_session():
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']
        data = request.get_json()
        if not data or 'topic' not in data: return jsonify({'error': 'Topic is required'}), 400
        topic = data['topic']
        questions = generate_initial_questions(topic=topic) # <- ã“ã®è¡Œã‚’ä¿®æ­£
        if not questions: raise Exception("AI failed to generate questions.")
        session_doc_ref = db_firestore.collection('users').document(user_id).collection('sessions').document()
        session_doc_ref.set({'topic': topic, 'status': 'in_progress', 'created_at': firestore.SERVER_TIMESTAMP, 'turn': 1, 'max_turns': 3})
        questions_collection = session_doc_ref.collection('questions')
        question_docs = []
        for i, q_data in enumerate(questions):
            if q_text := q_data.get("question_text"):
                q_doc_ref = questions_collection.document()
                q_doc_ref.set({'text': q_text, 'order': i, 'turn': 1})
                question_docs.append({'question_id': q_doc_ref.id, 'question_text': q_text})
        if not question_docs: raise Exception("All generated questions were empty.")
        return jsonify({'session_id': session_doc_ref.id, 'questions': question_docs}), 200
    except (auth.InvalidIdTokenError, IndexError, ValueError) as e:
        print(f"Auth Error in start_session: {e}")
        return jsonify({'error': 'Invalid or expired token', 'details': str(e)}), 403
    except Exception as e:
        print(f"Error in start_session: {e}")
        traceback.print_exc()
        return jsonify({'error': 'Failed to start session', 'details': str(e)}), 500

@app.route('/session/<string:session_id>/swipe', methods=['POST'])
def record_swipe(session_id):
    # ... (ã“ã®é–¢æ•°ã®ä¸­èº«ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']
        data = request.get_json()
        if not data: return jsonify({'error': 'Request body is missing'}), 400
        question_id = data.get('question_id')
        answer = data.get('answer') 
        hesitation_time = data.get('hesitation_time')
        speed = data.get('speed')
        turn = data.get('turn')
        if not all([question_id, turn is not None]) or not isinstance(answer, bool): return jsonify({'error': 'Missing or invalid type for required fields'}), 400
        session_doc_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)
        session_doc_ref.collection('swipes').add({'question_id': question_id,'answer': answer,'hesitation_time_sec': hesitation_time,'swipe_duration_ms': speed,'turn': turn,'timestamp': firestore.SERVER_TIMESTAMP})
        return jsonify({'status': 'swipe_recorded'}), 200
    except (auth.InvalidIdTokenError, IndexError, ValueError) as e:
        print(f"Auth Error in record_swipe: {e}")
        return jsonify({'error': 'Invalid or expired token', 'details': str(e)}), 403
    except Exception as e:
        print(f"Error recording swipe: {e}")
        traceback.print_exc()
        return jsonify({'error': 'Failed to record swipe', 'details': str(e)}), 500

@app.route('/session/<string:session_id>/summary', methods=['POST'])
def post_summary(session_id):
    # ... (ã“ã®é–¢æ•°ã®ä¸­èº«ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']
        data = request.get_json()
        if not data or 'swipes' not in data: return jsonify({'error': 'Swipes data is required'}), 400
        session_doc_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)
        session_doc = session_doc_ref.get()
        if not session_doc.exists: return jsonify({'error': 'Session not found'}), 404
        session_data = session_doc.to_dict()
        topic = session_data.get('topic', 'ä¸æ˜')
        current_turn = session_data.get('turn', 1)
        max_turns = session_data.get('max_turns', 3)
        swipes_text = "\n".join([f"Q: {s.get('question_text')}\nA: {'ã¯ã„' if s.get('answer') else 'ã„ã„ãˆ'}" for s in data['swipes']])
        summary_data = generate_summary_only(topic=topic, swipes_text=swipes_text) # <- ã“ã®è¡Œã‚’ä¿®æ­£
        insights_md = summary_data.get('insights')
        title = summary_data.get('title')
        if not insights_md or not title: raise Exception("AI failed to generate summary or title.")
        session_doc_ref.collection('analyses').add({'turn': current_turn, 'insights': insights_md, 'created_at': firestore.SERVER_TIMESTAMP})
        update_data = {'status': 'completed', 'updated_at': firestore.SERVER_TIMESTAMP, 'latest_insights': insights_md}
        if current_turn == 1: update_data['title'] = title
        session_doc_ref.update(update_data)
        threading.Thread(target=_update_graph_cache, args=(user_id,)).start()
        threading.Thread(target=_prefetch_questions_and_save, args=(session_id, user_id, insights_md, current_turn, max_turns)).start()
        print("--- Started background threads for graph cache and question prefetch. ---")
        return jsonify({'title': title, 'insights': insights_md, 'turn': current_turn, 'max_turns': max_turns}), 200
    except Exception as e:
        print(f"Error in post_summary: {e}")
        traceback.print_exc()
        return jsonify({'error': 'Failed to generate summary', 'details': str(e)}), 500

@app.route('/session/<string:session_id>/continue', methods=['POST'])
def continue_session(session_id):
    # ... (ã“ã®é–¢æ•°ã®ä¸­èº«ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']
        session_doc_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)
        @firestore.transactional
        def update_turn(transaction, ref):
            snapshot = ref.get(transaction=transaction)
            if not snapshot.exists: raise Exception("Session not found")
            data = snapshot.to_dict()
            if data.get('turn', 1) >= data.get('max_turns', 3): raise Exception("Max turns reached.")
            new_turn = data.get('turn', 1) + 1
            transaction.update(ref, {'status': 'in_progress', 'turn': new_turn, 'updated_at': firestore.SERVER_TIMESTAMP})
            return new_turn
        transaction = db_firestore.transaction()
        new_turn = update_turn(transaction, session_doc_ref)
        questions_collection = session_doc_ref.collection('questions')
        query = questions_collection.where('turn', '==', new_turn).order_by('order')
        question_docs = [{'question_id': doc.id, 'question_text': doc.to_dict().get('text')} for doc in query.stream()]
        if not question_docs:
            print(f"âš ï¸ Prefetched questions not found for turn {new_turn}. Generating and SAVING now (fallback).")
            last_analysis_doc = next(session_doc_ref.collection('analyses').order_by('created_at', direction=firestore.Query.DESCENDING).limit(1).stream(), None)
            if not last_analysis_doc: raise Exception("Cannot generate fallback questions: no analysis found.")
            fallback_questions = generate_follow_up_questions(last_analysis_doc.to_dict().get('insights'))
            if not fallback_questions: raise Exception("AI failed to generate fallback questions.")
            last_question_query = questions_collection.order_by('order', direction=firestore.Query.DESCENDING).limit(1).stream()
            last_order = next(last_question_query, None)
            start_order = last_order.to_dict().get('order', -1) + 1 if last_order else 0
            batch = db_firestore.batch()
            for i, q_data in enumerate(fallback_questions):
                if q_text := q_data.get("question_text"):
                    q_doc_ref = questions_collection.document()
                    batch.set(q_doc_ref, {'text': q_text,'turn': new_turn,'order': start_order + i,'created_at': firestore.SERVER_TIMESTAMP,'is_prefetched': False})
                    question_docs.append({'question_id': q_doc_ref.id,'question_text': q_text})
            batch.commit()
            print(f"âœ… Saved {len(fallback_questions)} fallback questions to Firestore.")
        if not question_docs: raise Exception("Failed to get any questions for the user.")
        return jsonify({'session_id': session_id, 'questions': question_docs, 'turn': new_turn}), 200
    except Exception as e:
        print(f"Error in continue_session: {e}")
        traceback.print_exc()
        return jsonify({'error': 'Failed to continue session', 'details': str(e)}), 500

# --- åˆ†æç³»API ---
def _get_all_insights_as_text(user_id: str) -> str:
    # ... (ã“ã®é–¢æ•°ã®ä¸­èº«ã¯å¤‰æ›´ã‚ã‚Šã¾ã›ã‚“)
    if not db_firestore: return ""
    sessions_ref = db_firestore.collection('users').document(user_id).collection('sessions').order_by('created_at').limit_to_last(20)
    sessions = sessions_ref.get() 
    all_insights = []
    for session in sessions:
        try:
            session_data = session.to_dict()
            if not session_data: continue
            topic = str(session_data.get('topic', ''))
            title = str(session_data.get('title', ''))
            all_insights.append(f"--- ã‚»ãƒƒã‚·ãƒ§ãƒ³: {topic} ({title}) ---\n")
            analyses_ref = session.reference.collection('analyses').order_by('created_at')
            for analysis in analyses_ref.stream():
                analysis_data = analysis.to_dict()
                if analysis_data and isinstance(analysis_data.get('insights'), str):
                    all_insights.append(analysis_data['insights'] + "\n")
        except Exception as inner_e:
            print(f"Skipping potentially corrupted session {session.id} for insight aggregation due to error: {inner_e}")
            continue
    return "".join(all_insights)

@app.route('/analysis/graph', methods=['GET'])
def get_analysis_graph():
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']
        graph_data = _get_graph_from_cache_or_generate(user_id)
        return jsonify(graph_data)
    except (auth.InvalidIdTokenError, IndexError, ValueError) as e:
        print(f"Auth Error in get_analysis_graph: {e}")
        return jsonify({'error': 'Invalid or expired token', 'details': str(e)}), 403
    except Exception as e:
        print(f"Error getting analysis graph: {e}")
        traceback.print_exc()
        return jsonify({'error': 'Failed to get analysis graph', 'details': str(e)}), 500

def _get_graph_from_cache_or_generate(user_id: str):
    cache_doc_ref = db_firestore.collection('users').document(user_id).collection('analysis').document('graph_cache')
    cache_doc = cache_doc_ref.get()
    if cache_doc.exists:
        print(f"âœ… Found graph cache for user {user_id}. Returning cached data.")
        return cache_doc.to_dict().get('data', {"nodes": [], "edges": []})
    
    print(f"âš ï¸ Graph cache not found for user {user_id}. Generating a new one...")
    all_insights_text = _get_all_insights_as_text(user_id)
    if not all_insights_text:
        print("No insights found to generate a graph.")
        return {"nodes": [], "edges": []}
    
    raw_graph_data = generate_graph_data(all_insights_text)
    
    nodes = raw_graph_data.get('nodes', [])
    edges = raw_graph_data.get('edges', [])
    sanitized_nodes = [n for n in nodes if isinstance(n, dict) and n.get('id')]
    valid_node_ids = {n['id'] for n in sanitized_nodes}
    sanitized_edges = [e for e in edges if isinstance(e, dict) and e.get('source') in valid_node_ids and e.get('target') in valid_node_ids]

    final_graph_data = {"nodes": sanitized_nodes, "edges": sanitized_edges}
    cache_doc_ref.set({'data': final_graph_data, 'updated_at': firestore.SERVER_TIMESTAMP})
    print(f"âœ… Successfully generated and cached graph for user: {user_id}")
    return final_graph_data

@app.route('/home/suggestion', methods=['GET'])
def get_home_suggestion():
    """
    ãƒ›ãƒ¼ãƒ ç”»é¢ã«è¡¨ç¤ºã™ã‚‹ãŸã‚ã®ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸå˜ä¸€ã®ææ¡ˆã‚’è¿”ã™ã€‚
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éå»ã®åˆ†æçµæœå…¨ä½“ã‹ã‚‰ã€ç‰¹ã«æ³¨æ„ã‚’å¼•ãã¹ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¢ã—ã¦è¿”ã™ã€‚
    """
    try:
        user = _verify_token(request)
        user_id = user['uid']
        print(f"--- Getting home suggestion for user: {user_id} ---")

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        all_insights_text = _get_all_insights_as_text(user_id)

        if not all_insights_text:
            print("No insights found, no suggestion will be returned.")
            return jsonify({}), 204 # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒãªã‘ã‚Œã°ææ¡ˆãªã—

        # äº‹å‰ã«å®šç¾©ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã¨ç…§åˆã™ã‚‹
        found_keyword = None
        for keyword in PROACTIVE_KEYWORDS:
            # å˜èªã¨ã—ã¦å®Œå…¨ã«ä¸€è‡´ã™ã‚‹å ´åˆã®ã¿ãƒ’ãƒƒãƒˆã•ã›ã‚‹ (ä¾‹: "ä¸å®‰" ã¯ "ä¸å®‰æ„Ÿ" ã«ã¯ãƒ’ãƒƒãƒˆã—ãªã„)
            if re.search(r'\b' + re.escape(keyword) + r'\b', all_insights_text, re.IGNORECASE):
                found_keyword = keyword
                print(f"Found proactive keyword for home suggestion: '{found_keyword}'")
                break # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ææ¡ˆã¨ã—ã¦æ¡ç”¨

        if found_keyword:
            # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã® HomeSuggestion ãƒ¢ãƒ‡ãƒ«ã«åˆã‚ã›ãŸå½¢å¼ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ§‹ç¯‰
            response_data = {
                "title": "AIã‹ã‚‰ã®ææ¡ˆ",
                "subtitle": f"æœ€è¿‘ã€Œ{found_keyword}ã€ã«ã¤ã„ã¦è€ƒãˆã¦ã„ã‚‹ã‚ˆã†ã§ã™ã­ã€‚æ€è€ƒã‚’æ•´ç†ã—ã¾ã›ã‚“ã‹ï¼Ÿ",
                "node_id": found_keyword,
                "node_label": found_keyword
            }
            return jsonify(response_data), 200
        else:
            print("No relevant keywords found in insights for home suggestion.")
            return jsonify({}), 204 # ææ¡ˆã™ã¹ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€Œææ¡ˆãªã—ã€ã§è¿”ã™

    except (auth.InvalidIdTokenError, IndexError, ValueError) as e:
        # èªè¨¼ã‚¨ãƒ©ãƒ¼ã¯ãƒ•ãƒ­ãƒ³ãƒˆå´ã§å†ãƒ­ã‚°ã‚¤ãƒ³ã‚’ä¿ƒã›ã‚‹ã‚ˆã†403ã‚’è¿”ã™
        print(f"Auth Error in get_home_suggestion: {e}")
        return jsonify({'error': 'Invalid or expired token'}), 403
    except Exception as e:
        print(f"âŒ Error in get_home_suggestion: {e}")
        traceback.print_exc()
        return jsonify({"error": "An internal error occurred while generating a suggestion."}), 500

@app.route('/analysis/proactive_suggestion', methods=['GET'])
def get_proactive_suggestion():
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']

        print(f"--- Checking for proactive suggestion for user {user_id} ---")
        
        session_summary = _get_all_insights_as_text(user_id)
        if not session_summary:
            return jsonify(None) # å±¥æ­´ãŒãªã‘ã‚Œã°ä½•ã‚‚è¿”ã•ãªã„

        found_keyword = None
        for keyword in PROACTIVE_KEYWORDS:
            if keyword in session_summary:
                found_keyword = keyword
                print(f"âœ… Found proactive keyword: '{found_keyword}'")
                break
        
        if not found_keyword:
            print("--- No proactive keyword found. ---")
            return jsonify(None) # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ä½•ã‚‚è¿”ã•ãªã„

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€ãã‚Œã«é–¢ã™ã‚‹éå»ã®æ–‡è„ˆã‚’è¦ç´„
        context_summary = _summarize_internal_context(session_summary, found_keyword)

        suggestion_text = (
            f"ã“ã‚Œã¾ã§ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã€ç‰¹ã«ã€Œ{found_keyword}ã€ã«ã¤ã„ã¦è§¦ã‚Œã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ãŒå¤šã„ã‚ˆã†ã§ã™ã€‚\n"
            f"{context_summary}\n"
            "ã‚ˆã‚ã—ã‘ã‚Œã°ã€ã“ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã‚‚ã†å°‘ã—æ·±ãæ˜ã‚Šä¸‹ã’ã¦ã¿ã¾ã›ã‚“ã‹ï¼Ÿ"
        )

        response_data = {
            "initial_summary": suggestion_text,
            "node_label": found_keyword,
            "actions": [
                {"id": "talk_freely", "label": "ã“ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦è©±ã™"},
                {"id": "get_similar_cases", "label": "ä¼¼ãŸæ‚©ã¿ã®è©±ã‚’èã"},
                {"id": "get_suggestions", "label": "å…·ä½“çš„ãªå¯¾ç­–ã‚’è¦‹ã‚‹"}
            ]
        }
        return jsonify(response_data)

    except (auth.InvalidIdTokenError, IndexError, ValueError) as e:
        print(f"Auth Error in get_proactive_suggestion: {e}")
        return jsonify({'error': 'Invalid or expired token', 'details': str(e)}), 403
    except Exception as e:
        print(f"Error in get_proactive_suggestion: {e}")
        traceback.print_exc()
        return jsonify({"error": "An internal error occurred."}), 500


# â˜…â˜…â˜… æ–°è¦è¿½åŠ  â˜…â˜…â˜…
@app.route('/chat/node_tap', methods=['POST'])
def handle_node_tap():
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']
        data = request.get_json()
        if not data or not (node_label := data.get('node_label')):
            return jsonify({'error': 'node_label is required'}), 400

        print(f"--- Node tap received for user {user_id}, node: '{node_label}' ---")

        session_summary = _get_all_insights_as_text(user_id)
        initial_summary = _summarize_internal_context(session_summary, node_label)
        
        response_data = {
            "initial_summary": f"ã€Œ{node_label}ã€ã«ã¤ã„ã¦ã§ã™ã­ã€‚\n{initial_summary}",
            "node_label": node_label, # ãƒ•ãƒ­ãƒ³ãƒˆãŒå¾Œã§ä½¿ã†ãŸã‚ã«ãƒ©ãƒ™ãƒ«ã‚’è¿”ã™
            "actions": [
                {"id": "talk_freely", "label": "è‡ªåˆ†ã®è€ƒãˆã‚’è©±ã™"},
                {"id": "get_similar_cases", "label": "ä¼¼ãŸã‚ˆã†ãªæ‚©ã¿ã®äººã®è©±ã‚’èã"},
                {"id": "get_suggestions", "label": "å…·ä½“çš„ãªå¯¾ç­–ã‚„ãƒ’ãƒ³ãƒˆã‚’è¦‹ã‚‹"}
            ]
        }
        return jsonify(response_data)

    except (auth.InvalidIdTokenError, IndexError, ValueError) as e:
        print(f"Auth Error in handle_node_tap: {e}")
        return jsonify({'error': 'Invalid or expired token', 'details': str(e)}), 403
    except Exception as e:
        print(f"Error in handle_node_tap: {e}")
        traceback.print_exc()
        return jsonify({"error": "An internal error occurred."}), 500

# â˜…â˜…â˜… ã“ã®é–¢æ•°ã‚’ä¿®æ­£ â˜…â˜…â˜…
@app.route('/analysis/chat', methods=['POST'])
def post_chat_message():
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']
        data = request.get_json()
        if not data:
            return jsonify({'error': 'Request body is missing'}), 400

        user_message = data.get('message')
        use_rag = data.get('use_rag', False)
        rag_type = data.get('rag_type', None) # RAGã®ç¨®åˆ¥ã‚’å–å¾—

        if not user_message and not use_rag:
            return jsonify({'error': 'message or use_rag flag is required'}), 400

        session_summary = _get_all_insights_as_text(user_id)
        ai_response = ""
        sources = []

        if not session_summary:
            ai_response = "ã“ã‚“ã«ã¡ã¯ã€‚åˆ†æã§ãã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´ãŒã¾ã ãªã„ã‚ˆã†ã§ã™ã€‚ã¾ãšã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å®Œäº†ã—ã¦ã€ã”è‡ªèº«ã®å†…é¢ã‚’æ¢ã‚‹æ—…ã‚’å§‹ã‚ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"

        elif use_rag:
            print(f"--- RAG advice triggered via chat API flag (type: {rag_type}) ---")
            # RAGã®å‘¼ã³å‡ºã—ã« `rag_type` ã‚’æ¸¡ã™
            ai_response, sources = _generate_rag_based_advice(
                session_summary,
                project_id,
                SIMILAR_CASES_ENGINE_ID,
                SUGGESTIONS_ENGINE_ID,
                rag_type=rag_type
            )
        else:
            ai_response = generate_chat_response(session_summary, data.get('chat_history', []), user_message)

        return jsonify({'answer': ai_response, 'sources': sources})
    except Exception as e:
        print(f"Error in post_chat_message: {e}")
        traceback.print_exc()
        return jsonify({"error": "An internal error occurred."}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=True)