import firebase_admin
from firebase_admin import credentials, firestore, auth
from flask import Flask, request, jsonify
from flask_cors import CORS

import os
import json
import re
import traceback
import threading
import requests
from bs4 import BeautifulSoup
import numpy as np

from tenacity import retry, stop_after_attempt, wait_exponential
import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig
from vertexai.language_models import TextEmbeddingModel
from google.cloud import discoveryengine_v1 as discoveryengine


# --- GCP & Firebase åˆæœŸåŒ– ---
try:
    print("Initializing GCP services using Application Default Credentials...")
    firebase_admin.initialize_app()
    db_firestore = firestore.client()
    
    app_instance = firebase_admin.get_app()
    project_id = app_instance.project_id
    print(f"âœ… Firebase Admin SDK initialized for project: {project_id}")

    vertex_ai_region = os.getenv('GCP_VERTEX_AI_REGION', 'us-central1')
    vertexai.init(project=project_id, location=vertex_ai_region)
    print(f"âœ… Vertex AI initialized for project: {project_id} in {vertex_ai_region}")

    # RAGç”¨è¨­å®š (2ã¤ã®ã‚¨ãƒ³ã‚¸ãƒ³IDã«å¯¾å¿œ)
    SIMILAR_CASES_ENGINE_ID = os.getenv('SIMILAR_CASES_ENGINE_ID')
    SUGGESTIONS_ENGINE_ID = os.getenv('SUGGESTIONS_ENGINE_ID')
    if 'K_SERVICE' in os.environ and (not SIMILAR_CASES_ENGINE_ID or not SUGGESTIONS_ENGINE_ID):
        print("âš ï¸ WARNING: One or both of SIMILAR_CASES_ENGINE_ID and SUGGESTIONS_ENGINE_ID environment variables are not set.")

except Exception as e:
    db_firestore = None
    print(f"âŒ Error during initialization: {e}")
    traceback.print_exc()
    if 'K_SERVICE' in os.environ:
        raise

app = Flask(__name__)
# --- CORSè¨­å®š ---
prod_origin = "https://guchi-app-flutter.web.app"
if 'K_SERVICE' in os.environ:
    origins = [prod_origin]
else:
    origins = [
        prod_origin,
        re.compile(r"http://localhost:.*"),
        re.compile(r"http://127.0.0.1:.*"),
    ]
CORS(app, resources={r"/*": {"origins": origins}})

# ===== JSONã‚¹ã‚­ãƒ¼ãƒå®šç¾© =====
QUESTIONS_SCHEMA = {"type": "object","properties": {"questions": {"type": "array","items": {"type": "object","properties": {"question_text": {"type": "string"}},"required": ["question_text"]}}},"required": ["questions"]}
SUMMARY_SCHEMA = {"type": "object","properties": {"title": {"type": "string", "description": "ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã‚’è¦ç´„ã™ã‚‹15æ–‡å­—ç¨‹åº¦ã®çŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«"},"insights": {"type": "string", "description": "æŒ‡å®šã•ã‚ŒãŸMarkdownå½¢å¼ã§ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æãƒ¬ãƒãƒ¼ãƒˆ"}},"required": ["title", "insights"]}
GRAPH_SCHEMA = {"type": "object","properties": {"nodes": {"type": "array","items": {"type": "object","properties": {"id": {"type": "string"},"type": {"type": "string", "enum": ["emotion", "topic", "keyword", "issue"]},"size": {"type": "integer"}},"required": ["id", "type", "size"]}},"edges": {"type": "array","items": {"type": "object","properties": {"source": {"type": "string"},"target": {"type": "string"},"weight": {"type": "integer"}},"required": ["source", "target", "weight"]}}},"required": ["nodes", "edges"]}

# ===== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ =====
SUMMARY_ONLY_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„Ÿæƒ…ã®å‹•ãã‚’åˆ†æã™ã‚‹ãƒ—ãƒ­ã®è‡¨åºŠå¿ƒç†å£«ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€Œ{topic}ã€ã¨ã„ã†ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦å¯¾è©±ã—ã¦ã„ã¾ã™ã€‚
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ä¼šè©±å±¥æ­´ã‚’åˆ†æã—ã€å¿…ãšæŒ‡ç¤ºé€šã‚Šã®JSONå½¢å¼ã§åˆ†æãƒ¬ãƒãƒ¼ãƒˆã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
# åˆ†æå¯¾è±¡ã®ä¼šè©±å±¥æ­´
{swipes_text}
# å‡ºåŠ›å½¢å¼ (JSON)
å¿…ãšä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
- `title`: ä¼šè©±å…¨ä½“ã‚’è±¡å¾´ã™ã‚‹15æ–‡å­—ç¨‹åº¦ã®çŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«ã€‚
- `insights`: ä»¥ä¸‹ã®Markdownå½¢å¼ã§ **å³å¯†ã«** è¨˜è¿°ã•ã‚ŒãŸåˆ†æãƒ¬ãƒãƒ¼ãƒˆã€‚
```markdown
### âœ¨ å…¨ä½“çš„ãªè¦ç´„
ï¼ˆã“ã“ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç¾åœ¨ã®å¿ƒç†çŠ¶æ…‹ã€ä¸»ãªæ„Ÿæƒ…ã€å†…é¢çš„ãªè‘›è—¤ãªã©ã‚’2ã€œ3æ–‡ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ï¼‰
### ğŸ“ è©³ç´°ãªåˆ†æ
ï¼ˆã“ã“ã«ã€å…·ä½“çš„ãªåˆ†æå†…å®¹ã‚’ç®‡æ¡æ›¸ãã§è¨˜è¿°ã—ã¦ãã ã•ã„ï¼‰
* **æ„Ÿæƒ…ã®çŠ¶æ…‹**: ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ„Ÿã˜ã¦ã„ã‚‹ä¸»è¦ãªæ„Ÿæƒ…ã«ã¤ã„ã¦ã€ãã®æ ¹æ‹ ã¨å…±ã«è¨˜è¿°ã—ã¦ãã ã•ã„ï¼‰
* **æ³¨ç›®ã™ã¹ãç‚¹**: ï¼ˆå›ç­”å†…å®¹ã¨ã€ãŸã‚ã‚‰ã„æ™‚é–“ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹æ„Ÿæƒ…ã®çŸ›ç›¾ã€ç‰¹ã«å°è±¡çš„ãªå›ç­”ãªã©ã€åˆ†æã®éµã¨ãªã£ãŸãƒã‚¤ãƒ³ãƒˆã‚’å…·ä½“çš„ã«æŒ™ã’ã¦ãã ã•ã„ã€‚ä¼šè©±å±¥æ­´ã«ã€Œç‰¹ã«è¿·ã„ãŒè¦‹ã‚‰ã‚Œã¾ã—ãŸã€ã¨è¨˜è¼‰ã®ã‚ã‚‹å›ç­”ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãŸã‚ã‚‰ã„ã‚„è‘›è—¤ã‚’æŠ±ãˆã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰
* **æ ¹æœ¬çš„ãªèª²é¡Œ**: ï¼ˆåˆ†æã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç›´é¢ã—ã¦ã„ã‚‹æ ¹æœ¬çš„ãªèª²é¡Œã‚„æ¬²æ±‚ã«ã¤ã„ã¦è¨˜è¿°ã—ã¦ãã ã•ã„ï¼‰
### ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®ææ¡ˆ
ï¼ˆä»Šå›ã®åˆ†æã‚’è¸ã¾ãˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡å›ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§æ·±æ˜ã‚Šã™ã‚‹ã¨è‰¯ã•ãã†ãªãƒ†ãƒ¼ãƒã‚„ã€æ—¥å¸¸ç”Ÿæ´»ã§æ„è­˜ã—ã¦ã¿ã‚‹ã¨è‰¯ã„ã“ã¨ãªã©ã‚’ã€å…·ä½“çš„ã‹ã¤ãƒã‚¸ãƒ†ã‚£ãƒ–ãªè¨€è‘‰ã§ææ¡ˆã—ã¦ãã ã•ã„ï¼‰
```
"""
GRAPH_ANALYSIS_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã§ã‚ã‚Šã€è‡¨åºŠå¿ƒç†å£«ã§ã‚‚ã‚ã‚Šã¾ã™ã€‚
ã“ã‚Œã‹ã‚‰æ¸¡ã™ãƒ†ã‚­ã‚¹ãƒˆã¯ã€ã‚ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¤‡æ•°å›ã®ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨˜éŒ²ã§ã™ã€‚
ã“ã®è¨˜éŒ²å…¨ä½“ã‚’åˆ†æã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†çŠ¶æ…‹ã®æ ¸ã¨ãªã‚‹è¦ç´ ã‚’æŠ½å‡ºã—ã€ãã‚Œã‚‰ã®é–¢é€£æ€§ã‚’è¡¨ç¾ã™ã‚‹ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
# ã‚°ãƒ©ãƒ•ç”Ÿæˆã®ãƒ«ãƒ¼ãƒ«
1. ãƒãƒ¼ãƒ‰ã®ç¨®é¡: `topic`, `issue`, `emotion`, `keyword`
2. ãƒãƒ¼ãƒ‰ã®éšå±¤: ä¸­å¿ƒã«`topic`ã¨`issue`ã‚’é…ç½®ã—ã€`emotion`ã‚„`keyword`ã¯ãã‚Œã‚‰ã‹ã‚‰æåˆ†ã‹ã‚Œã•ã›ã‚‹ã€‚
3. ãƒãƒ¼ãƒ‰æ•°ã®åˆ¶é™: ç·æ•°ã¯æœ€å¤§ã§ã‚‚15å€‹ç¨‹åº¦ã«å³é¸ã™ã‚‹ã€‚
4. IDã®è¨€èª: `id`ã¯å¿…ãšæ—¥æœ¬èªã®å˜èªã¾ãŸã¯çŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã«ã™ã‚‹ã€‚
# å‡ºåŠ›JSONã®ä»•æ§˜
å‡ºåŠ›ã¯ã€ä»¥ä¸‹ã®ä»•æ§˜ã«å³å¯†ã«å¾“ã£ãŸJSONå½¢å¼ã®ã¿ã¨ã™ã‚‹ã“ã¨ã€‚ { "nodes": [ ... ], "edges": [ ... ] }
# ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²
"""
CHAT_PROMPT_TEMPLATE = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒç†åˆ†æã®å°‚é–€å®¶ã§ã‚ã‚Šã€å…±æ„ŸåŠ›ã¨æ´å¯ŸåŠ›ã«å„ªã‚ŒãŸã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã€Œã‚³ã‚³ãƒ­ã®åˆ†æå®˜ã€ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€è‡ªèº«ã®æ€è€ƒã‚’å¯è¦–åŒ–ã—ãŸã‚°ãƒ©ãƒ•ã‚’è¦‹ãªãŒã‚‰ã€ã‚ãªãŸã¨å¯¾è©±ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚
# ã‚ãªãŸã®å½¹å‰²
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®éå»ã®ä¼šè©±å±¥æ­´ã¨ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€è€ƒã®è¦ç´„ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ï¼‰ã‚’å¸¸ã«å‚ç…§ã—ã€æ–‡è„ˆã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã‚’æ·±ãå‚¾è´ã—ã€ã¾ãšã¯è‚¯å®šçš„ã«å—ã‘æ­¢ã‚ã¦å…±æ„Ÿã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚
- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ã®å†…å®¹ã«åŸºã¥ãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‡ªåˆ†ã§ã‚‚æ°—ã¥ã„ã¦ã„ãªã„å†…é¢ã‚’å„ªã—ãæŒ‡æ‘˜ã—ãŸã‚Šã€æ·±ã„å•ã„ã‚’æŠ•ã’ã‹ã‘ãŸã‚Šã—ã¦ã€è‡ªå·±ç†è§£ã‚’ä¿ƒã—ã¦ãã ã•ã„ã€‚
- æ¯å›ã®è¿”ä¿¡ã‚’è‡ªå·±ç´¹ä»‹ã‹ã‚‰å§‹ã‚ã‚‹ã®ã§ã¯ãªãã€ä¼šè©±ã®æµã‚Œã‚’è‡ªç„¶ã«å¼•ãç¶™ã„ã§ãã ã•ã„ã€‚
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼
{session_summary}
# ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
{chat_history}
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»Šå›ã®ç™ºè¨€
{user_message}
ã‚ãªãŸã®å¿œç­”:
"""

# ===== Gemini ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤ =====
@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
def _call_gemini_with_schema(prompt: str, schema: dict, model_name: str) -> dict:
    model = GenerativeModel(model_name)
    attempt_num = _call_gemini_with_schema.retry.statistics.get('attempt_number', 1)
    print(f"--- Calling Gemini ({model_name}) with schema (Attempt: {attempt_num}) ---")
    try:
        response = model.generate_content(prompt, generation_config=GenerationConfig(response_mime_type="application/json", response_schema=schema))
        # Handle potential markdown code block delimiters
        response_text = response.text.strip()
        if response_text.startswith("```json"):
            response_text = response_text[7:-3].strip()
        elif response_text.startswith("```"):
            response_text = response_text[3:-3].strip()
        return json.loads(response_text)
    except Exception as e:
        print(f"Error on attempt {attempt_num} with model {model_name}: {e}\n--- Gemini Response ---\n{getattr(response, 'text', 'Empty')}\n---")
        traceback.print_exc()
        raise

def generate_initial_questions(topic):
    prompt = f"ã‚ãªãŸã¯ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚ãƒˆãƒ”ãƒƒã‚¯ã€Œ{topic}ã€ã«ã¤ã„ã¦ã€ã€Œã¯ã„ã€ã‹ã€Œã„ã„ãˆã€ã§ç­”ãˆã‚‰ã‚Œã‚‹è³ªå•ã‚’5ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
    flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
    return _call_gemini_with_schema(prompt, QUESTIONS_SCHEMA, model_name=flash_model).get("questions", [])

def generate_follow_up_questions(insights):
    prompt = f"ã‚ãªãŸã¯ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®åˆ†æçµæœã‚’ã•ã‚‰ã«æ·±ã‚ã‚‹ã€ã€Œã¯ã„ã€ã‹ã€Œã„ã„ãˆã€ã§ç­”ãˆã‚‰ã‚Œã‚‹è³ªå•ã‚’5ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n# åˆ†æçµæœ\n{insights}"
    flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
    return _call_gemini_with_schema(prompt, QUESTIONS_SCHEMA, model_name=flash_model).get("questions", [])

def generate_summary_only(topic, swipes_text):
    prompt = SUMMARY_ONLY_PROMPT_TEMPLATE.format(topic=topic, swipes_text=swipes_text)
    flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
    return _call_gemini_with_schema(prompt, SUMMARY_SCHEMA, model_name=flash_model)

def generate_graph_data(all_insights_text):
    prompt = GRAPH_ANALYSIS_PROMPT_TEMPLATE + all_insights_text
    pro_model = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
    return _call_gemini_with_schema(prompt, GRAPH_SCHEMA, model_name=pro_model)

def generate_chat_response(session_summary, chat_history, user_message):
    history_str = "\n".join([f"{msg['author']}: {msg['text']}" for msg in chat_history])
    prompt = CHAT_PROMPT_TEMPLATE.format(session_summary=session_summary, chat_history=history_str, user_message=user_message)
    pro_model = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
    model = GenerativeModel(pro_model)
    return model.generate_content(prompt).text.strip()

@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(3))
def _extract_keywords_for_search(analysis_text: str) -> str:
    """é•·ã„åˆ†æãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹"""
    prompt = f"""
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å¿ƒç†åˆ†æãƒ¬ãƒãƒ¼ãƒˆå…¨ä½“ã‹ã‚‰ã€æœ€ã‚‚é‡è¦ã¨æ€ã‚ã‚Œã‚‹æ¦‚å¿µã‚„èª²é¡Œã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’5ã¤ä»¥å†…ã§æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯Vertex AI Searchã®æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚ä»–ã®æ–‡ã¯å«ã‚ãšã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã®ã¿ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

# åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
{analysis_text}

# å‡ºåŠ›ä¾‹
ä»•äº‹ã®ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼, äººé–“é–¢ä¿‚ã®æ‚©ã¿, è‡ªå·±è‚¯å®šæ„Ÿã®ä½ä¸‹, å°†æ¥ã¸ã®ä¸å®‰

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:
"""
    try:
        flash_model = os.getenv('GEMINI_FLASH_NAME', 'gemini-1.5-flash-preview-05-20')
        model = GenerativeModel(flash_model)
        print("--- Calling Gemini to extract search keywords ---")
        response = model.generate_content(prompt)
        keywords = response.text.strip()
        print(f"âœ… Extracted Keywords: {keywords}")
        return keywords
    except Exception as e:
        print(f"âŒ Failed to extract keywords: {e}")
        return ""

# ===== RAG (Retrieval-Augmented Generation) Helper Functions =====
def _generate_rag_based_advice(query: str, project_id: str, similar_cases_engine_id: str, suggestions_engine_id: str):
    # 1. é•·ã„åˆ†æãƒ¬ãƒãƒ¼ãƒˆ(query)ã‹ã‚‰æ¤œç´¢ç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    search_query = _extract_keywords_for_search(query)
    if not search_query:
        print("âš ï¸ RAG: Could not extract keywords. Using a slice of the original query for search as a fallback.")
        # ã‚¯ã‚¨ãƒªãŒé•·ã™ãã‚‹ã¨APIã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦å…ˆé ­512æ–‡å­—ã‚’ä½¿ã†
        search_query = query[:512]

    all_urls = set()

    # 2a. æŠ½å‡ºã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§é¡ä¼¼ã‚±ãƒ¼ã‚¹ã‚’æ¤œç´¢
    if similar_cases_engine_id:
        print(f"--- RAG: Searching for similar cases in engine '{similar_cases_engine_id}' with query: '{search_query}' ---")
        case_urls = _search_with_vertex_ai_search(project_id, "global", similar_cases_engine_id, search_query)
        all_urls.update(case_urls)

    # 2b. æŠ½å‡ºã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ”¹å–„æ¡ˆã‚’æ¤œç´¢
    if suggestions_engine_id:
        print(f"--- RAG: Searching for suggestions in engine '{suggestions_engine_id}' with query: '{search_query}' ---")
        suggestion_urls = _search_with_vertex_ai_search(project_id, "global", suggestions_engine_id, search_query)
        all_urls.update(suggestion_urls)
    
    if not all_urls:
        print("âš ï¸ RAG: No relevant URLs found from any search engine.")
        return "é–¢é€£ã™ã‚‹å¤–éƒ¨æƒ…å ±ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚ãªãŸã®åˆ†æçµæœã«åŸºã¥ãã¨ã€ã¾ãšã¯ã”è‡ªèº«ã®æ„Ÿæƒ…ã‚’èªè­˜ã—ã€å—ã‘å…¥ã‚Œã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã‚‹ã®ãŒè‰¯ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
    
    urls_to_process = list(all_urls)

    from langchain.text_splitter import RecursiveCharacterTextSplitter
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
    
    all_chunks = []
    print(f"--- RAG: Scraping up to 5 unique URLs: {urls_to_process[:5]} ---")
    for url in urls_to_process[:5]:
        try:
            page_content = _scrape_text_from_url(url)
            if page_content:
                all_chunks.extend(text_splitter.split_text(page_content))
        except Exception as e:
            print(f"âŒ RAG: Failed to scrape or chunk {url}: {e}")

    if not all_chunks:
        print("âš ï¸ RAG: Could not extract any text chunks from URLs.")
        return "é–¢é€£ã™ã‚‹å¤–éƒ¨æƒ…å ±ã‚’è¦‹ã¤ã‘ã¾ã—ãŸãŒã€å†…å®¹ã‚’èª­ã¿å–ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®æ§‹é€ ãŒåŸå› ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"

    print(f"--- RAG: Finding relevant chunks from {len(all_chunks)} total chunks... ---")
    # ãƒãƒ£ãƒ³ã‚¯ã®é–¢é€£æ€§æ¤œç´¢ã«ã¯ã€å…ƒã®è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆå…¨æ–‡(query)ã‚’ä½¿ã£ãŸæ–¹ãŒç²¾åº¦ãŒé«˜ã„ãŸã‚ã€ã“ã“ã§ã¯ 'query' ã‚’ä½¿ç”¨ã™ã‚‹
    relevant_chunks = _find_relevant_chunks(query, all_chunks)
    if not relevant_chunks:
        print("âš ï¸ RAG: No relevant chunks found after vector search.")
        return "é–¢é€£æƒ…å ±ã®ä¸­ã‹ã‚‰ã€ã‚ãªãŸã®çŠ¶æ³ã«ç‰¹ã«åˆè‡´ã™ã‚‹éƒ¨åˆ†ã‚’è¦‹ã¤ã‘å‡ºã™ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    print("--- RAG: Generating final advice with Gemini... ---")
    context_text = "\n---\n".join(relevant_chunks)
    prompt = f"""
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿ƒã®çŠ¶æ…‹ã‚’åˆ†æã—ã€ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ã„ãŸå®¢è¦³çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã™ã‚‹AIã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœã¨ã€é–¢é€£ã™ã‚‹å‚è€ƒæƒ…å ±ï¼ˆé¡ä¼¼ã‚±ãƒ¼ã‚¹ã‚„å…·ä½“çš„ãªæ”¹å–„æ¡ˆãªã©ï¼‰ã‚’èª­ã‚“ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å…·ä½“çš„ã§å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã®ä¸€æ­©ã‚’è¸ã¿å‡ºã›ã‚‹ã‚ˆã†ã«ã€å„ªã—ãã€å…±æ„Ÿçš„ã§ã€è‚¯å®šçš„ãªãƒˆãƒ¼ãƒ³ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚Markdownå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
# ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æçµæœ
{query}
# å‚è€ƒæƒ…å ± (é¡ä¼¼ã‚±ãƒ¼ã‚¹ã‚„æ”¹å–„æ¡ˆã®ãƒ’ãƒ³ãƒˆ)
---
{context_text}
---
# ã‚¢ãƒ‰ãƒã‚¤ã‚¹
"""
    pro_model_name = os.getenv('GEMINI_PRO_NAME', 'gemini-1.5-pro-preview-05-20')
    model = GenerativeModel(pro_model_name)
    return model.generate_content(prompt, generation_config=GenerationConfig(temperature=0.7)).text

def _search_with_vertex_ai_search(project_id: str, location: str, engine_id: str, query: str) -> list[str]:
    if not engine_id:
        print(f"âŒ RAG: Engine ID '{engine_id}' is not configured.")
        return []
    client = discoveryengine.SearchServiceClient()

    # --- â†“â†“â†“ ã“ã“ã‹ã‚‰ãŒä¿®æ­£ç®‡æ‰€ã§ã™ â†“â†“â†“ ---
    # TypeErrorã‚’å›é¿ã™ã‚‹ãŸã‚ã€serving_configã®ãƒ‘ã‚¹ã‚’æ‰‹å‹•ã§çµ„ã¿ç«‹ã¦ã‚‹
    # ã“ã®ãƒ‘ã‚¹å½¢å¼ã¯ã€Enterprise Editionã®ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆ©ç”¨ã™ã‚‹éš›ã«APIãŒè¦æ±‚ã™ã‚‹ã‚‚ã®ã§ã™ã€‚
    serving_config = (
        f"projects/{project_id}/locations/{location}/collections/default_collection/"
        f"engines/{engine_id}/servingConfigs/default_config"
    )
    # --- â†‘â†‘â†‘ ä¿®æ­£ç®‡æ‰€ã“ã“ã¾ã§ â†‘â†‘â†‘

    request = discoveryengine.SearchRequest(serving_config=serving_config, query=query, page_size=5)
    try:
        response = client.search(request)
        urls = [r.document.derived_struct_data.get('link') for r in response.results if r.document.derived_struct_data.get('link')]
        print(f"âœ… RAG: Found URLs from Vertex AI Search: {urls}")
        return urls
    except Exception as e:
        print(f"âŒ RAG: Vertex AI Search failed for engine '{engine_id}': {e}")
        traceback.print_exc()
        return []

def _scrape_text_from_url(url: str) -> str:
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        response = requests.get(url, timeout=10, headers=headers)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        for element in soup(["script", "style", "header", "footer", "nav", "aside"]):
            element.decompose()
        return soup.get_text(separator=' ', strip=True)
    except requests.exceptions.RequestException as e:
        print(f"âŒ RAG: Error fetching URL {url}: {e}")
        return ""

def _find_relevant_chunks(query: str, chunks: list[str], top_k=3) -> list[str]:
    """
    Finds the most relevant text chunks for a given query using text embeddings.
    Handles API limits by batching requests.
    """
    model = TextEmbeddingModel.from_pretrained("text-multilingual-embedding-002")
    # APIã«ã¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ•°(250)ã ã‘ã§ãªãã€åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°(ç´„20000)ã®åˆ¶é™ã‚‚ã‚ã‚‹ã€‚
    # 1ãƒãƒ£ãƒ³ã‚¯ã‚’ç´„1000ãƒˆãƒ¼ã‚¯ãƒ³ã¨è¦‹ç©ã‚‚ã‚Šã€å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã‚’ã¨ã£ã¦ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’15ã«è¨­å®šã™ã‚‹ã€‚
    # (15 chunks * ~1000 tokens/chunk < 20000 tokens)
    BATCH_SIZE = 15

    try:
        all_texts = [query] + chunks
        all_embeddings_responses = []

        print(f"--- RAG: Generating embeddings for {len(all_texts)} texts in batches of {BATCH_SIZE} ---")
        for i in range(0, len(all_texts), BATCH_SIZE):
            batch = all_texts[i:i + BATCH_SIZE]
            all_embeddings_responses.extend(model.get_embeddings(batch))
            # ceiling division to calculate total batches correctly
            print(f"--- RAG: Processed embedding batch {i//BATCH_SIZE + 1}/{-(-len(all_texts) // BATCH_SIZE)} ---")

        query_embedding_response = all_embeddings_responses[0]
        if hasattr(query_embedding_response, 'error'):
            print(f"âŒ RAG: Failed to get embedding for the query: {getattr(query_embedding_response, 'error', 'Unknown error')}")
            return []
        query_embedding = np.array(query_embedding_response.values)

        chunk_embeddings_responses = all_embeddings_responses[1:]
        
        chunk_similarity_pairs = []
        for i, resp in enumerate(chunk_embeddings_responses):
            if not hasattr(resp, 'error'):
                chunk_embedding = np.array(resp.values)
                dot_product = np.dot(chunk_embedding, query_embedding)
                norm_product = np.linalg.norm(chunk_embedding) * np.linalg.norm(query_embedding)
                
                # np.divideã®outå¼•æ•°ã«ã‚¹ã‚«ãƒ©å€¤(0.0)ã‚’æ¸¡ã™ã¨TypeErrorãŒç™ºç”Ÿã™ã‚‹ãŸã‚ã€
                # é€šå¸¸ã®é™¤ç®—ã¨ã‚¼ãƒ­é™¤ç®—ã®ãƒã‚§ãƒƒã‚¯ã«ä¿®æ­£ã—ã¾ã™ã€‚
                if norm_product == 0:
                    similarity = 0.0
                else:
                    similarity = dot_product / norm_product
                
                chunk_similarity_pairs.append({
                    'chunk': chunks[i],
                    'similarity': similarity
                })

        if not chunk_similarity_pairs:
            print("âš ï¸ RAG: No valid chunk embeddings were generated to calculate similarity.")
            return []

        # Sort by similarity and return the text of the top_k chunks
        sorted_pairs = sorted(chunk_similarity_pairs, key=lambda x: x['similarity'], reverse=True)
        return [pair['chunk'] for pair in sorted_pairs[:top_k]]

    except Exception as e:
        print(f"âŒ RAG: An unexpected error occurred while finding relevant chunks: {e}")
        traceback.print_exc()
        return []

# --- ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç† ---
def _prefetch_questions_and_save(session_id: str, user_id: str, insights_md: str, current_turn: int, max_turns: int):
    print(f"--- Triggered question prefetch for user: {user_id}, session: {session_id}, next_turn: {current_turn + 1} ---")
    if current_turn >= max_turns:
        print("Max turns reached. Skipping question prefetch.")
        return
    try:
        questions = generate_follow_up_questions(insights=insights_md)
        if not questions:
            print(f"âš ï¸ AI failed to generate prefetch questions for session {session_id}.")
            return
        session_doc_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)
        next_turn = current_turn + 1
        questions_collection = session_doc_ref.collection('questions')
        last_question_query = questions_collection.order_by('order', direction=firestore.Query.DESCENDING).limit(1).stream()
        last_order = next(last_question_query, None)
        start_order = last_order.to_dict().get('order', -1) + 1 if last_order else 0
        batch = db_firestore.batch()
        for i, q_data in enumerate(questions):
            if q_text := q_data.get("question_text"):
                q_doc_ref = questions_collection.document()
                batch.set(q_doc_ref, {'text': q_text, 'turn': next_turn, 'order': start_order + i, 'created_at': firestore.SERVER_TIMESTAMP, 'is_prefetched': True})
        batch.commit()
        print(f"âœ… Successfully prefetched questions for turn {next_turn}.")
    except Exception as e:
        print(f"âŒ Failed to prefetch questions for session {session_id}: {e}")
        traceback.print_exc()

def _update_graph_cache(user_id: str):
    print(f"--- Triggered graph cache update for user: {user_id} ---")
    try:
        all_insights_text = _get_all_insights_as_text(user_id)
        if not all_insights_text: return
        raw_graph_data = generate_graph_data(all_insights_text)
        nodes = raw_graph_data.get('nodes', [])
        edges = raw_graph_data.get('edges', [])
        sanitized_nodes = [n for n in nodes if isinstance(n, dict) and n.get('id')]
        valid_node_ids = {n['id'] for n in sanitized_nodes}
        sanitized_edges = [e for e in edges if isinstance(e, dict) and e.get('source') in valid_node_ids and e.get('target') in valid_node_ids]
        final_graph_data = {"nodes": sanitized_nodes, "edges": sanitized_edges}
        cache_doc_ref = db_firestore.collection('users').document(user_id).collection('analysis').document('graph_cache')
        cache_doc_ref.set({'data': final_graph_data, 'updated_at': firestore.SERVER_TIMESTAMP})
        print(f"âœ… Successfully updated graph cache for user: {user_id}")
    except Exception as e:
        print(f"âŒ Failed to update graph cache for user {user_id}: {e}")
        traceback.print_exc()

# ===== èªè¨¼ãƒ˜ãƒ«ãƒ‘ãƒ¼ =====
def _verify_token(request):
    auth_header = request.headers.get('Authorization', '')
    if not auth_header.startswith('Bearer '):
        raise auth.InvalidIdTokenError("Authorization token is missing or invalid")
    id_token = auth_header.split('Bearer ')[1]
    return auth.verify_id_token(id_token, clock_skew_seconds=15)

# ===== API Routes =====
@app.route('/session/start', methods=['POST'])
def start_session():
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']
        data = request.get_json()
        if not data or 'topic' not in data: return jsonify({'error': 'Topic is required'}), 400
        topic = data['topic']
        questions = generate_initial_questions(topic)
        if not questions: raise Exception("AI failed to generate questions.")
        session_doc_ref = db_firestore.collection('users').document(user_id).collection('sessions').document()
        session_doc_ref.set({'topic': topic, 'status': 'in_progress', 'created_at': firestore.SERVER_TIMESTAMP, 'turn': 1, 'max_turns': 3})
        questions_collection = session_doc_ref.collection('questions')
        question_docs = []
        for i, q_data in enumerate(questions):
            if q_text := q_data.get("question_text"):
                q_doc_ref = questions_collection.document()
                q_doc_ref.set({'text': q_text, 'order': i, 'turn': 1})
                question_docs.append({'question_id': q_doc_ref.id, 'question_text': q_text})
        if not question_docs: raise Exception("All generated questions were empty.")
        return jsonify({'session_id': session_doc_ref.id, 'questions': question_docs}), 200
    except (auth.InvalidIdTokenError, IndexError, ValueError) as e:
        print(f"Auth Error in start_session: {e}")
        return jsonify({'error': 'Invalid or expired token', 'details': str(e)}), 403
    except Exception as e:
        print(f"Error in start_session: {e}")
        traceback.print_exc()
        return jsonify({'error': 'Failed to start session', 'details': str(e)}), 500

@app.route('/session/<string:session_id>/swipe', methods=['POST'])
def record_swipe(session_id):
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']
        data = request.get_json()
        if not data: return jsonify({'error': 'Request body is missing'}), 400
        question_id = data.get('question_id')
        answer = data.get('answer') 
        hesitation_time = data.get('hesitation_time')
        speed = data.get('speed')
        turn = data.get('turn')
        if not all([question_id, turn is not None]) or not isinstance(answer, bool): return jsonify({'error': 'Missing or invalid type for required fields'}), 400
        session_doc_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)
        session_doc_ref.collection('swipes').add({'question_id': question_id,'answer': answer,'hesitation_time_sec': hesitation_time,'swipe_duration_ms': speed,'turn': turn,'timestamp': firestore.SERVER_TIMESTAMP})
        return jsonify({'status': 'swipe_recorded'}), 200
    except (auth.InvalidIdTokenError, IndexError, ValueError) as e:
        print(f"Auth Error in record_swipe: {e}")
        return jsonify({'error': 'Invalid or expired token', 'details': str(e)}), 403
    except Exception as e:
        print(f"Error recording swipe: {e}")
        traceback.print_exc()
        return jsonify({'error': 'Failed to record swipe', 'details': str(e)}), 500

@app.route('/session/<string:session_id>/summary', methods=['POST'])
def post_summary(session_id):
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']
        data = request.get_json()
        if not data or 'swipes' not in data: return jsonify({'error': 'Swipes data is required'}), 400
        session_doc_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)
        session_doc = session_doc_ref.get()
        if not session_doc.exists: return jsonify({'error': 'Session not found'}), 404
        session_data = session_doc.to_dict()
        topic = session_data.get('topic', 'ä¸æ˜')
        current_turn = session_data.get('turn', 1)
        max_turns = session_data.get('max_turns', 3)
        swipes_text = "\n".join([f"Q: {s.get('question_text')}\nA: {'ã¯ã„' if s.get('answer') else 'ã„ã„ãˆ'}" for s in data['swipes']])
        summary_data = generate_summary_only(topic, swipes_text)
        insights_md = summary_data.get('insights')
        title = summary_data.get('title')
        if not insights_md or not title: raise Exception("AI failed to generate summary or title.")
        session_doc_ref.collection('analyses').add({'turn': current_turn, 'insights': insights_md, 'created_at': firestore.SERVER_TIMESTAMP})
        update_data = {'status': 'completed', 'updated_at': firestore.SERVER_TIMESTAMP, 'latest_insights': insights_md}
        if current_turn == 1: update_data['title'] = title
        session_doc_ref.update(update_data)
        threading.Thread(target=_update_graph_cache, args=(user_id,)).start()
        threading.Thread(target=_prefetch_questions_and_save, args=(session_id, user_id, insights_md, current_turn, max_turns)).start()
        print("--- Started background threads for graph cache and question prefetch. ---")
        return jsonify({'title': title, 'insights': insights_md, 'turn': current_turn, 'max_turns': max_turns}), 200
    except Exception as e:
        print(f"Error in post_summary: {e}")
        traceback.print_exc()
        return jsonify({'error': 'Failed to generate summary', 'details': str(e)}), 500

@app.route('/session/<string:session_id>/continue', methods=['POST'])
def continue_session(session_id):
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']
        session_doc_ref = db_firestore.collection('users').document(user_id).collection('sessions').document(session_id)
        @firestore.transactional
        def update_turn(transaction, ref):
            snapshot = ref.get(transaction=transaction)
            if not snapshot.exists: raise Exception("Session not found")
            data = snapshot.to_dict()
            if data.get('turn', 1) >= data.get('max_turns', 3): raise Exception("Max turns reached.")
            new_turn = data.get('turn', 1) + 1
            transaction.update(ref, {'status': 'in_progress', 'turn': new_turn, 'updated_at': firestore.SERVER_TIMESTAMP})
            return new_turn
        transaction = db_firestore.transaction()
        new_turn = update_turn(transaction, session_doc_ref)
        questions_collection = session_doc_ref.collection('questions')
        query = questions_collection.where('turn', '==', new_turn).order_by('order')
        question_docs = [{'question_id': doc.id, 'question_text': doc.to_dict().get('text')} for doc in query.stream()]
        if not question_docs:
            print(f"âš ï¸ Prefetched questions not found for turn {new_turn}. Generating and SAVING now (fallback).")
            last_analysis_doc = next(session_doc_ref.collection('analyses').order_by('created_at', direction=firestore.Query.DESCENDING).limit(1).stream(), None)
            if not last_analysis_doc: raise Exception("Cannot generate fallback questions: no analysis found.")
            fallback_questions = generate_follow_up_questions(last_analysis_doc.to_dict().get('insights'))
            if not fallback_questions: raise Exception("AI failed to generate fallback questions.")
            last_question_query = questions_collection.order_by('order', direction=firestore.Query.DESCENDING).limit(1).stream()
            last_order = next(last_question_query, None)
            start_order = last_order.to_dict().get('order', -1) + 1 if last_order else 0
            batch = db_firestore.batch()
            for i, q_data in enumerate(fallback_questions):
                if q_text := q_data.get("question_text"):
                    q_doc_ref = questions_collection.document()
                    batch.set(q_doc_ref, {'text': q_text,'turn': new_turn,'order': start_order + i,'created_at': firestore.SERVER_TIMESTAMP,'is_prefetched': False})
                    question_docs.append({'question_id': q_doc_ref.id,'question_text': q_text})
            batch.commit()
            print(f"âœ… Saved {len(fallback_questions)} fallback questions to Firestore.")
        if not question_docs: raise Exception("Failed to get any questions for the user.")
        return jsonify({'session_id': session_id, 'questions': question_docs, 'turn': new_turn}), 200
    except Exception as e:
        print(f"Error in continue_session: {e}")
        traceback.print_exc()
        return jsonify({'error': 'Failed to continue session', 'details': str(e)}), 500

# --- åˆ†æç³»API ---
def _get_all_insights_as_text(user_id: str) -> str:
    if not db_firestore: return ""
    sessions_ref = db_firestore.collection('users').document(user_id).collection('sessions').order_by('created_at').limit_to_last(20)
    
    # Firestoreã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—ã™ã‚‹éš›ã« .stream() ã®ä»£ã‚ã‚Šã« .get() ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    # ã“ã‚Œã«ã‚ˆã‚Šã€å¯¾è±¡ã®å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä¸€åº¦ã«å–å¾—ã™ã‚‹ãŸã‚ã€å¾Œç¶šã®å‡¦ç†ãŒå®‰å®šã—ã¾ã™ã€‚
    sessions = sessions_ref.get() 

    all_insights = []
    for session in sessions:
        try:
            session_data = session.to_dict()
            if not session_data: continue
            topic = str(session_data.get('topic', ''))
            title = str(session_data.get('title', ''))
            all_insights.append(f"--- ã‚»ãƒƒã‚·ãƒ§ãƒ³: {topic} ({title}) ---\n")
            analyses_ref = session.reference.collection('analyses').order_by('created_at')
            for analysis in analyses_ref.stream():
                analysis_data = analysis.to_dict()
                if analysis_data and isinstance(analysis_data.get('insights'), str):
                    all_insights.append(analysis_data['insights'] + "\n")
        except Exception as inner_e:
            print(f"Skipping potentially corrupted session {session.id} for insight aggregation due to error: {inner_e}")
            continue
    return "".join(all_insights)

@app.route('/analysis/graph', methods=['GET'])
def get_analysis_graph():
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']
        graph_data = _get_graph_from_cache_or_generate(user_id)
        return jsonify(graph_data)
    except (auth.InvalidIdTokenError, IndexError, ValueError) as e:
        print(f"Auth Error in get_analysis_graph: {e}")
        return jsonify({'error': 'Invalid or expired token', 'details': str(e)}), 403
    except Exception as e:
        print(f"Error getting analysis graph: {e}")
        traceback.print_exc()
        return jsonify({'error': 'Failed to get analysis graph', 'details': str(e)}), 500

def _get_graph_from_cache_or_generate(user_id: str):
    cache_doc_ref = db_firestore.collection('users').document(user_id).collection('analysis').document('graph_cache')
    cache_doc = cache_doc_ref.get()
    if cache_doc.exists:
        print(f"âœ… Found graph cache for user {user_id}. Returning cached data.")
        return cache_doc.to_dict().get('data', {"nodes": [], "edges": []})
    
    print(f"âš ï¸ Graph cache not found for user {user_id}. Generating a new one...")
    all_insights_text = _get_all_insights_as_text(user_id)
    if not all_insights_text:
        print("No insights found to generate a graph.")
        return {"nodes": [], "edges": []}
    
    raw_graph_data = generate_graph_data(all_insights_text)
    
    nodes = raw_graph_data.get('nodes', [])
    edges = raw_graph_data.get('edges', [])
    sanitized_nodes = [n for n in nodes if isinstance(n, dict) and n.get('id')]
    valid_node_ids = {n['id'] for n in sanitized_nodes}
    sanitized_edges = [e for e in edges if isinstance(e, dict) and e.get('source') in valid_node_ids and e.get('target') in valid_node_ids]

    final_graph_data = {"nodes": sanitized_nodes, "edges": sanitized_edges}
    cache_doc_ref.set({'data': final_graph_data, 'updated_at': firestore.SERVER_TIMESTAMP})
    print(f"âœ… Successfully generated and cached graph for user: {user_id}")
    return final_graph_data

@app.route('/analysis/chat', methods=['POST'])
def post_chat_message():
    try:
        decoded_token = _verify_token(request)
        user_id = decoded_token['uid']
        data = request.get_json()
        if not data or not (user_message := data.get('message')): return jsonify({'error': 'message is required'}), 400
        
        session_summary = _get_all_insights_as_text(user_id)
        if not session_summary:
            ai_response = "ã“ã‚“ã«ã¡ã¯ã€‚åˆ†æã§ãã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´ãŒã¾ã ãªã„ã‚ˆã†ã§ã™ã€‚ã¾ãšã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å®Œäº†ã—ã¦ã€ã”è‡ªèº«ã®å†…é¢ã‚’æ¢ã‚‹æ—…ã‚’å§‹ã‚ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
        
        elif 'RAGã‚’ä½¿ã£ã¦å…·ä½“çš„ãªæ”¹å–„æ¡ˆã‚’' in user_message:
            print("--- RAG advice triggered via chat ---")
            ai_response = _generate_rag_based_advice(
                session_summary,
                project_id,
                SIMILAR_CASES_ENGINE_ID,
                SUGGESTIONS_ENGINE_ID
            )
        else:
            ai_response = generate_chat_response(session_summary, data.get('chat_history', []), user_message)
        return jsonify({'response': ai_response})
    except Exception as e:
        print(f"Error in post_chat_message: {e}")
        traceback.print_exc()
        return jsonify({"error": "An internal error occurred."}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=True)